<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://lposti.github.io/MLPages/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lposti.github.io/MLPages/" rel="alternate" type="text/html" /><updated>2022-11-05T05:32:04-05:00</updated><id>https://lposti.github.io/MLPages/feed.xml</id><title type="html">An Astronomerâ€™s perspective on Deep Learning</title><subtitle>Notes and notebooks of an Astronomer diving deep into the wonders of machine learning</subtitle><entry><title type="html">Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results</title><link href="https://lposti.github.io/MLPages/gaussian_processes/2022/11/02/gp_rotcurves.html" rel="alternate" type="text/html" title="Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results" /><published>2022-11-02T00:00:00-05:00</published><updated>2022-11-02T00:00:00-05:00</updated><id>https://lposti.github.io/MLPages/gaussian_processes/2022/11/02/gp_rotcurves</id><author><name>Lorenzo Posti</name></author><category term="gaussian_processes" /><summary type="html"><![CDATA[Correlations between velocity measurements in disk galaxy rotation curves are usually neglected when fitting dynamical models. This notebook, which accompanies the paper **link_to_paper**, shows how data correlations can be taken into account in rotation curve decompositions using Gaussian Processes.]]></summary></entry><entry><title type="html">Gaussian Processes: modelling correlated noise in a dataset</title><link href="https://lposti.github.io/MLPages/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes.html" rel="alternate" type="text/html" title="Gaussian Processes: modelling correlated noise in a dataset" /><published>2022-10-17T00:00:00-05:00</published><updated>2022-10-17T00:00:00-05:00</updated><id>https://lposti.github.io/MLPages/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes</id><author><name>Lorenzo Posti</name></author><category term="gaussian_processes" /><category term="bayesian" /><category term="jupyter" /><summary type="html"><![CDATA[Independent datapoints are most often just a convenient idealisation which can even hamper your model inference at times and bias your results. Learn how to embrace the reality of correlated noise in the data and marginalize the parameter posteriors with Gaussian Processes.]]></summary></entry><entry><title type="html">Variational Autoencoder: learning an underlying distribution and generating new data</title><link href="https://lposti.github.io/MLPages/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves.html" rel="alternate" type="text/html" title="Variational Autoencoder: learning an underlying distribution and generating new data" /><published>2022-10-07T00:00:00-05:00</published><updated>2022-10-07T00:00:00-05:00</updated><id>https://lposti.github.io/MLPages/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves</id><author><name>Lorenzo Posti</name></author><category term="neural_network" /><category term="autoencoder" /><category term="variational autoencoder" /><category term="basics" /><category term="jupyter" /><summary type="html"><![CDATA[Constructing an autoencoder that learns the underlying distribution of the input data, generated from a multi-dimensional smooth function `f=f(x_1,x_2,x_3,x_4)`. This can be used to generate new data, sampling from the learned distribution]]></summary></entry><entry><title type="html">Autoencoder represents a multi-dimensional smooth function</title><link href="https://lposti.github.io/MLPages/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves.html" rel="alternate" type="text/html" title="Autoencoder represents a multi-dimensional smooth function" /><published>2022-06-10T00:00:00-05:00</published><updated>2022-06-10T00:00:00-05:00</updated><id>https://lposti.github.io/MLPages/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves</id><author><name>Lorenzo Posti</name></author><category term="neural_network" /><category term="autoencoder" /><category term="basics" /><category term="jupyter" /><summary type="html"><![CDATA[Setting up a simple `Autoencoder` neural network to reproduce a dataset obtained by sampling a multi-dimensional smooth function `f=f(x_1,x_2,x_3,x_4)`. As an example I'm using a disc+halo rotation curve model where both components are described by 2-parameters circular velocities]]></summary></entry><entry><title type="html">Ground-up construction of a simple neural network</title><link href="https://lposti.github.io/MLPages/neural_network/basics/jupyter/2022/03/22/NN-from-scratch.html" rel="alternate" type="text/html" title="Ground-up construction of a simple neural network" /><published>2022-03-22T00:00:00-05:00</published><updated>2022-03-22T00:00:00-05:00</updated><id>https://lposti.github.io/MLPages/neural_network/basics/jupyter/2022/03/22/NN-from-scratch</id><author><name>Lorenzo Posti</name></author><category term="neural_network" /><category term="basics" /><category term="jupyter" /><summary type="html"><![CDATA[Constructing a simple multi-layered neural network (NN) from scratch using pure `python` and a bit of `pytorch`. This is mostly my personal re-writing of the fantastic lesson 1 of the `fast.ai` course Part 2]]></summary></entry></feed>