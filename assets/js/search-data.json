{
  
    
        "post0": {
            "title": "Variational Autoencoder: learning an underlying distribution and generating new data",
            "content": "Variational AutoEncoder (VAE): an algorithm to work with distributions . This notebook deals with generating an Autoencoder model to learn the underlying distribution of the data. To do this we have to modify the autoencoder such that the encoder does not learn a compressed representation of the input data, but rather it will learn the parameters of the distribution of the data in the latent (compressed) space. . So the idea is to start from an observed sample of the distribution of the data $P({ bf X})$ and to pass this to the encoder which will reduce its dimensionality, i.e. $P({ bf X}) mapsto P&#39;({ bf X}_{ rm c})$ where ${ bf X} in mathrm{R}^m$ and ${ bf X}_{ rm c} in mathrm{R}^n$ with $n&lt;m$. In other words, in a VAE the encoder step does not represent the input data ${ bf X}$ with a code ${ bf X}_{ rm c}$, but rather the initial data distribution $P({ bf X})$ with a compressed distribution $P&#39;({ bf X}_{ rm c})$, which we usually need to approximate in some analytic form, e.g. a multi-variate normal $P&#39;({ bf X}_{ rm c}) sim mathcal{N}( mu, Sigma)$. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random import corner %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . # halo concentration--mass relation def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . . Let&#39;s start again by generating the distribution of physical parameters and calculating the rotation curve of a galaxy with those parameters. This part is taken from the blog post on Autoencoders so I just refer to that for details. . nsamp = 2000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . cm=curveMod(ms,rd,mh,cc) . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . This plot shows a random realization of nsamp rotation curves from our physical model. These curves are the dataset the we are going to use to train our Variational Autoencoder. Let&#39;s start by normalizing the data and defining the training and validation sets. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/torch/csrc/utils/tensor_new.cpp:204.) &#34;&#34;&#34; . The Variational Autoencoder . We now build the class, deriving from nn.Module, for our Variational AutoEncoder (VAE). In particular, we define the layers in the __init__ method and we define an encoder and a decoder method, just as we did for the Autoencoder. However, this class is quite a lot richer than the one we used for and Autoencoder and we will go through each method below. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Sequential(nn.Linear(in_features=ninp, out_features=32), nn.ReLU()) self.encodeLayer2 = nn.Sequential(nn.Linear(in_features=32, out_features=16), nn.ReLU()) self.encodeOut = nn.Linear(in_features=16, out_features=8) self.decodeLayer1 = nn.Sequential(nn.Linear(in_features=4, out_features=16), nn.ReLU()) self.decodeLayer2 = nn.Sequential(nn.Linear(in_features=16, out_features=32), nn.ReLU()) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar = torch.split(self.encodeOut(self.encodeLayer2(self.encodeLayer1(x))),4,dim=1) return mean, logvar def decoder(self, encoded): return self.decodeOut(self.decodeLayer2(self.decodeLayer1(encoded))) def reparametrize(self, mean, logvar): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) return eps * torch.exp(logvar * 0.5) + mean # exp(0.5logvar) = std # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed # https://arxiv.org/pdf/1312.6114.pdf # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes?noredirect=1&amp;lq=1 def _ELBO(self, x, decoded, mean, logvar): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim = 1) # KLdiv = -0.5 * (torch.sum(1 + logvar, dim=1) - torch.sum(mean**2, dim=1) - torch.sum(logvar.exp(), dim=1)) return (KLdiv - logpx_z).mean() def forward(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, logvar) return decoded def getELBO_loss(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, logvar) . Ok, so there&#39;s a lot to break down here! . First of all, we notice that the overall structure of the encoder/decoder network is relatively similar to the autoencoder we saw before, with the important difference that the encoder now returns 8 parameters instead of 4. These are the parameters of the multi-variate normal distribution with which we represent the 4-dimensional latent space, so mean and variance for each of the four physical properties that generate the rotation curves. Thus, the encoder step does not output a code, but means and variances for each physical property. | the decoder step is instead totally similar to a simple autoencoder and in fact it requires a code as an input. In order to generate a code from the means and variances that come out of the encoder phase without breaking the backprop flow of the algorithm, Kingma &amp; Welling (2013) proposed to use a reparametrization trick, which consists of throwing a new sample from a standard normal and then shifting this to have the same mean and variance as given by the encoder. | the forward method of this class follows these steps: the encoder gives means and variances of the latent space, the reparametrization trick is used to generate a code, which is finally decoded by the decoder. | the appropriate loss function for a variational autoencoder is the Evidence Lower BOund (ELBO). In fact, minimising the -ELBO means maximising a lower bound on the evidence or likelihood of the model. The evidence, or reconstruction loss, is logpx_z which is just an MSE loss on the data and the decoded output of the autoencoder. This term encourages the reconstruction of the dataset and tends to prefer separated encodings for each element of the dataset. The other term, KLdiv, is the Kullback-Leibler divergence of the proposed distribution in the latent space with the likelihood. This term has the opposite effect of promoting overlapping encodings for separate observations. For this reason, maximising ELBO guarantees to achieve a nice compromise between representing the original data and the ability to generalize by generating realistic new data. | . With these changes our neural network is now capable of constructing an approximation for the distribution of the four physical parameters in the latent space. We can now run the usual optimization algorithm and start training this model. . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=1e-2) for epoch in range(2000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%100==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.46e+04 valid L:1.76e+04 100 train L:1.29e+03 valid L:3.67e+02 200 train L:5.40e+02 valid L:1.73e+02 300 train L:4.72e+02 valid L:1.39e+02 400 train L:1.42e+02 valid L:6.75e+01 500 train L:1.19e+02 valid L:5.85e+01 600 train L:9.75e+01 valid L:5.35e+01 700 train L:1.05e+02 valid L:5.57e+01 800 train L:7.98e+01 valid L:4.68e+01 900 train L:1.91e+02 valid L:7.53e+01 1000 train L:7.49e+01 valid L:4.26e+01 1100 train L:8.18e+01 valid L:4.40e+01 1200 train L:6.40e+01 valid L:3.92e+01 1300 train L:6.89e+01 valid L:3.86e+01 1400 train L:6.15e+01 valid L:3.69e+01 1500 train L:6.42e+01 valid L:3.66e+01 1600 train L:5.76e+01 valid L:3.55e+01 1700 train L:5.99e+01 valid L:3.60e+01 1800 train L:1.28e+02 valid L:5.11e+01 1900 train L:4.93e+01 valid L:3.26e+01 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The plot above shows the distribution of rotation curves that the VAE has learned. We can see that there is a quite large variety of rotation curve shapes that can be represented by this model. Notice that the region in the radius-velicity space covered by the VAE is indeed quite similar to that of the training set (see plot above). . This shows that the VAE has indeed learned an effective distribution in the latent space which generates the rotation curve dataset we started from. . Exploring the latent space distribution . We can now have a look at what the VAE has learned about the latent space. To do so, we can take the means and variances derived by the encoder on the training set and we can use them to generate samples on the latent space. Basically for each $x_i$ in the training set we get a $ mu_i$ and a $ sigma^2_i$ and we draw 100 samples from $ mathcal{N}( mu_i, sigma^2_i)$. . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . We thus have an array of training_set_size x N_samples = 1600 x 100 = 160000 samples generated from the distribution inferred by the VAE on the latent space. Let&#39;s now plot them (I&#39;m using corner to do so) . rr = ((-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . Here in black we plotted the resulting samples of the latent space as described above, while in orange we overplot just the means $ mu_i$ for each element in the training set. It turns out that the distribution of the means (in orange) is virtually identical to that derived from sampling each multi-variate Gaussian in the latent space (in black). . This implies that the variances $ sigma^2_i$ that are the output of the encoder are generally quite small and that the VAE effectively reconstructs the distribution of the latent space by superimposing many thin Gaussians, all with slightly different mean and small variance. In fact, one could interpret the orange distribution as a superposition of Dirac deltas centered at each mean derived by the encoder on the training set, i.e. $ sum_i delta_i(x- mu_i)$. . Let&#39;s now plot together the physical parameters that we used to generate each rotation curve in the training set, together with the means derived by the encoder step on each element of that set. This allows us to explore whether there are any correlations between the original 4 physical parameters and the 4 dimensions of the latent space constructed by the VAE. . To do this we can stack together the tensor of physical parameters and that of the means. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] mdtrain, mhtrain = mdshuff[:int(nsamp*(1.0-fval))], mhshuff[:int(nsamp*(1.0-fval))] rdtrain, cctrain = rdshuff[:int(nsamp*(1.0-fval))], ccshuff[:int(nsamp*(1.0-fval))] # physical parameters corresponding to each element of the training set partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) # stacking the tensor of phsyical parameters with that of the means derived by the encoder dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr2 = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr2, smooth=0.75, smooth1d=0.75, labels=ll); . Ok, now this corner plot has a lot of information...let&#39;s breakdown the most important ones: . the first 4 blocks of this corner plot are relative to the physical parameters (ms,mh,rd,cc) and show the marginal distribution of each of these parameters and how they are correlated with each other. | the last 4 blocks, instead, are relative to the 4 latent parameters (L1,L2,L3,L4), and again show the marginal distribution of each and their mutual correlations - this is equivalent to the corner plot just above (in that case this was plotted in orange colour) | the 4x4 sub-block on the lower-left corner is possibly the most interesting one as it is the &quot;mixed&quot; block that highlights the relation between the physical and latent parameters. Each of these 16 panels show the correlation of one of the physical parameters with one of the latent ones. We can see that there are several significant correlations (e.g. ms-L1, mh-L2, rd-L3), meaning that these two sets are not independent. | . Generating new realistic data . Now that we have a working approximation of the distribution in the latent space it is easy to use the VAE to generate new rotation curves. We will showcase now a quite simplistic way to do it, which is by assuming that we can represent the latent space with a 4-dimensional Gaussian, even though we have seen in the plots above that the actual distribution is more complex. . We consider the means $ mu_i$ that the encoder derives for the full training set and we take the mean and standard deviation of these. We use these two parameters to define a normal distribution . size=500 dd = torch.distributions.normal.Normal(msm.mean(dim=0), msm.std(dim=0)) new_code = dd.sample(torch.Size([size])) . Let&#39;s make a comparison by plotting the marginalised distributions of the 4 latent parameters with that of the normal distribution that we are assuming . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(rr[0][0],rr[0][1],20), np.linspace(rr[1][0], rr[1][1],20), np.linspace(rr[2][0],rr[2][1],20), np.linspace(rr[3][0], rr[3][1],20)] for i in range(4): ax[i].hist(msm[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;$ mathcal{N}$&#39;+&#39;-approx&#39;); if i==0: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) ax[i].set_xlabel(&#39;L%1d&#39;%(i+1)) . Quite a big difference! . However, this is not an issue since we are just using a normal distribution since it is convenient to sample and for us it is just a means to generate plausible code to be interpreted by the decoder. As a matter of fact, by doing this we are able to generate quite a new variety of possible galaxy rotation curves. . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(vae.decoder(new_code),xmean,xstd)[:100]: ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With a bit on work on the reparametrization trick, we can relax the assumption that the distribution in the latent space is of a multi-variate, but uncorrelated, normal. In practice, instead of having just 4 means and 4 variances as output of the encoder step, we also add 6 covariances, so that we can define the full non-zero covariance matrix of the multi-variate normal in the latent space. . Unfortunately this require quite a bit more math on the reparametrization trick, which is not anymore just $ epsilon* sigma+ mu$, but where the full covariance matrix is used. This calculation requires, among other things, to derive the square root of a matrix, for which we employ the SVD decomposition: if $A = U ,{ rm diag}(s) ,V^{ rm T}$, where $A, U, V in mathbb{R}^{n, n}$, $s in mathbb{R}^n$, and diag$(s)$ is a diagonal matrix with the elements of $s$ as diagonal, then $ sqrt{A} = U ,{ rm diag} left( sqrt{s} right) ,V^{ rm T}$. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=14) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar, covs = torch.split(self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))), [4, 4, 6], dim=1) return mean, logvar, covs def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def reparametrize(self, mean, m_cov): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) # return eps * var.sqrt() + mean # find matrix square root with SVD decomposition # https://math.stackexchange.com/questions/3820169/a-is-a-symmetric-positive-definite-matrix-it-has-square-root-using-svd?noredirect=1&amp;lq=1 U,S,V = torch.svd(m_cov) # A = U diag(S) V.T dS = torch.stack([torch.diag(S[i,:]) for i in range(S.shape[0])]) # sqrt(A) = U diag(sqrt(S)) V.T cov_sqrt = torch.einsum(&#39;bij,bkj-&gt;bik&#39;,torch.einsum(&#39;bij,bjk-&gt;bik&#39;,U,dS.sqrt()),V) return torch.einsum(&#39;bij,bi-&gt;bj&#39;, cov_sqrt, eps) + mean def _ELBO(self, x, decoded, mean, m_cov, var): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * (torch.log(m_cov.det()) + 4 - torch.sum(mean**2 + var, dim = 1)) # return (KLdiv - logpx_z).mean() return torch.mean((KLdiv - logpx_z)[~(KLdiv - logpx_z).isnan()]) # torch.nanmean def _get_m_cov(self, logvar, covs): # covariance matrix m_cov = torch.zeros(logvar.shape[0], 4, 4) m_cov[:,[0,1,2,3],[0,1,2,3]] = logvar.exp() m_cov[:,[0,0,0,1,1,2],[1,2,3,2,3,3]] = covs m_cov[:,[1,2,3,2,3,3],[0,0,0,1,1,2]] = covs # var = torch.einsum(&#39;bii-&gt;bi&#39;, m_cov) return m_cov, logvar.exp() def forward(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, m_cov, var) return decoded def getELBO_loss(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, m_cov, var) . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=0.4e-2) for epoch in range(1000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.26e+04 valid L:2.21e+04 50 train L:6.80e+03 valid L:1.42e+03 100 train L:1.47e+03 valid L:4.22e+02 150 train L:9.56e+02 valid L:2.77e+02 200 train L:7.22e+02 valid L:1.98e+02 250 train L:5.99e+02 valid L:2.26e+02 300 train L:4.56e+02 valid L:1.53e+02 350 train L:4.18e+02 valid L:1.85e+02 400 train L:3.63e+02 valid L:1.66e+02 450 train L:3.53e+02 valid L:1.57e+02 500 train L:3.33e+02 valid L:1.39e+02 550 train L:4.32e+02 valid L:1.75e+02 600 train L:3.07e+02 valid L:1.39e+02 650 train L:3.10e+02 valid L:1.10e+02 700 train L:2.77e+02 valid L:1.09e+02 750 train L:9.70e+02 valid L:3.07e+02 800 train L:6.58e+02 valid L:3.33e+02 850 train L:4.97e+02 valid L:2.41e+02 900 train L:4.53e+02 valid L:1.82e+02 950 train L:6.56e+02 valid L:3.57e+02 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . rr = ((-25,30),(-40,15),(-30,10),(-20,6)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-25,30),(-40,15),(-30,10),(-20,6)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr, smooth=0.75, smooth1d=0.75, labels=ll); .",
            "url": "https://lposti.github.io/MLPages/neural%20network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/13/variational_autoencoder_rotcurves.html",
            "relUrl": "/neural%20network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/13/variational_autoencoder_rotcurves.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding how basic linear NNs handle non-linearities",
            "content": "How simple neurons handle non-linearities . The Universal approximation Theorem . While studying deep learning, coming from a mathematical physics background, I struggled a lot in understanding how a neural network (NN) can fit non-linear functions. Most of the visual explanations or tutorials I could find on NNs did not give me a satisfactory explanation as to how a composition of linear functions - i.e. an NN - can handle non-linear behaviour. So I looked for more formal derivations, as I knew that the mathematical foundations of deep learning had to be sound. . Eventually I came across one of the most important papers in this field: Cybenko (1989, doi:10.1007/BF02551274). Reading this paper opened my eyes and gave me a completely different perspective on the problem. I realized that the key to the universal approximation theorem is that the composition of a linear function and a sigmoidal (so-called activation) function yields a series of functions which is dense in the space of continuous functions. . In other words, any continuous function can be written as a finite sum of terms given by the composition of a linear and a sigmoidal function, i.e. $$ sum_{i=0}^N alpha_i , sigma({ bf w}_i cdot{ bf x} + b_i), $$ with $ sigma: mathrm{R} to mathrm{R}$ being a sigmoidal activation function, ${ bf x} in mathrm{R}^n$ and ${ bf w}_i in mathrm{R}^n$, $ alpha_i, ,b_i in mathrm{R}$ $ forall i$. Cybenko (1989) showed that the set of functions above spans the whole space of continuous functions in $ mathrm{R}^n$, effectively making this set kind of a basis for $ mathrm{R}^n$, except that the functions are not linearly independent. . Elements of this set of function as in the equation above are usually called units or neurons. . Where does the non-linear behaviour come from . Since a neuron is a composition of a linear function with an activation function, the key to approximate non-linear functions is in the sigmoidal activation function. Formally a sigmoidal is a function $ sigma: mathrm{R} to mathrm{R}$ such that $ lim_{x to+ infty} sigma(x)=1$ and $ lim_{x to- infty} sigma(x)=0$. The Heaviside function is an example of one of the simplest sigmoidal functions; however that is not continuous near $x=0$, thus in practice smooth approximations of it are often used. A popular one is: $$ sigma(x)= frac{1}{1+e^{-x}} $$ . But how can a linear combination of lines composed with a step function approximate any non-linear behaviour? I knew from Cybenko&#39;s results that this had to be the case, so I set out and tried to understand and see this better. . Part 1: How many neurons are needed to approximate a given non-linearity? . I asked myself how many neurons (i.e. elements of Cybenko&#39;s quasi-basis) would I need to approximate a simple second-order non-linearity, i.e. the function $x mapsto x^2$. . import torch import numpy as np import matplotlib.pyplot as plt import torch.nn as nn device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; device = torch.device(device) %matplotlib inline %config Completer.use_jedi = False . . Let&#39;s generate the data of a nice parabolic curve with some small random noise and let&#39;s plot them . size = 500 x = torch.linspace(-5, 5, size) y = x.pow(2) + 0.5 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131837ba8&gt;] . In order to build some intuition of how we may approximate this data with a linear combination of neurons let&#39;s experiment a bit and overplot a few simple combinations of neurons by hand. . First, we need to define our sigmoidal activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) . fig,ax = plt.subplots(figsize=(15,4), ncols=3) def commons(ax): ax.plot(x,y,&#39;k.&#39;, zorder=0) ax.legend(loc=&#39;upper center&#39;) ax.set_ylim((None, 30)) ax[0].plot(x,20*sig(x), lw=3, label=r&quot;$ rm 20 sigma(x)$&quot;) ax[0].plot(x,20*sig(-x), lw=3, label=r&quot;$ rm 20 sigma(-x)$&quot;) ax[0].plot(x,20*(sig(x)+sig(-x)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x)+ sigma(-x)]$&quot;) commons(ax[0]) ax[1].plot(x,20*sig(x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*sig(-x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*(sig(x-3)+sig(-x-3)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x-3)+ sigma(-x-3)]$&quot;) commons(ax[1]) ax[2].plot(x,25*(sig(1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax[2].plot(x,25*(sig(-1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax[2].plot(x,25*(sig(1.2*x-4)+sig(-1.2*x-4)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax[2]) . These examples help in building up some intuition on how we can use these sigmoidal building blocks to approximate relatively simple non linear behaviours. In this specific case of a convex second-order non-linearity, the sum of two scaled, shifted and mirrored step functions seems to yield a decent representation of the data close to the origin. . Dependence on the shape of the activation function . Naturally, this is strongly dependent on the particular shape of the sigmoidal function that we chose, i.e. $ sigma: x mapsto (1+e^{-x})^{-1}$. If we had chosen a Heaviside function instead, the sum of the two neurons above would not have yielded a similarly good approximation of the data. . fig,ax=plt.subplots() ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(-1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)+np.heaviside(-1.2*x-4,0.5)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax) . . NN fitting of $x^2$ function . Now that we have some intuition on the linear combinations of neurons let&#39;s try to answer the question posed at the beginning of Part 1, that is how many neurons are needed to approximate $x^2$. . To answer this we will build a series of simple torch models made up of just one (hidden) layer of neurons, i.e. . nn.Sequential(nn.Linear(1, N_units), nn.Sigmoid(), nn.Linear(N_units, 1)) . We start by defining a learning rate and a number of epochs; then we loop through the 5 numbers of neurons explored, [1,2,4,8,16], we set up the nn.Sequential model and we start the full training loop on the data. We put all of this into a convenient class with a run method and a plots method, which we use to visualize the output. . class ModelsTestingActivations(): def __init__(self, activation_fn=nn.Sigmoid(), loss_fn=nn.MSELoss(reduction=&#39;sum&#39;), units=[1,2,4,8,16], learning_rate=3e-2, num_epochs=1000): self.activ_fn, self.loss_fn = activation_fn, loss_fn self.units, self.lr, self.num_epochs = units, learning_rate, num_epochs # outputs self.models, self.preds = [], [] def make_model(self, u): return nn.Sequential(nn.Linear(in_features=1, out_features=u, bias=True), self.activ_fn, nn.Linear(in_features=u, out_features=1) ) def plots(self, residuals=True, xextrap=10): if not hasattr(self, &#39;x&#39;): print (&#39;Have you run the model yet?&#39;) return fig,ax = plt.subplots(figsize=(18,3.2), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$y$&#39;) ax[i].plot(self.x,self.y,&#39;k.&#39;) ax[i].plot(self.x,self.preds[i],&#39;r.&#39;) ax[i].plot(np.linspace(-xextrap,xextrap), self.models[i](torch.linspace(-xextrap,xextrap,50).unsqueeze(1)).detach(), &#39;b--&#39;) ax[i].text(0.05,0.05,r&quot;N=%d&quot; % self.units[i], transform=ax[i].transAxes, fontsize=14) # residuals if not residuals: return fig,ax = plt.subplots(figsize=(18,1.6), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$ Delta y$&#39;) ax[i].plot(self.x,(self.y-self.preds[i]).abs(), &#39;k-&#39;, lw=0.5) ax[i].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (self.y-self.preds[i]).abs().mean(), transform=ax[i].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;) def run(self, x, y): self.x, self.y = x, y for i,u in enumerate(self.units): # define model model = self.make_model(u) self.models.append(model) # define optimizer optimizer = torch.optim.Adam(model.parameters(), lr=self.lr) # fitting loop for epoch in range(self.num_epochs): # reinitialize gradient of the model weights optimizer.zero_grad() # prediction &amp; loss y_pred = model(self.x.unsqueeze(-1)) loss = self.loss_fn(y_pred, self.y.unsqueeze(-1)) # backpropagation loss.backward() # weight update optimizer.step() self.preds.append(y_pred.squeeze().detach()) . . It is interesting to see how the results of this series of models change depending on which activation function is used, thus we can make our custom class to have as input the shape of the activation function. . We start with the classic sigmoidal . mods_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_sigmoid.run(x, y) . mods_sigmoid.plots() . These plots show how the NN model (red) compares to the actual data used for training (black) as a function of the number (N) of neurons used in the linear layer. Moving from the panels left to right the number of neurons increases from $N=1$ to $N=6$. The dashed blue line is the prediction of the NN model, which we also extrapolated in the range $x in[-10,10]$ outside of the domain of the data, that are confined within $[-5,5]$. The rows of panels below show the residuals $ Delta y=|y-y_{ rm NN}|$, i.e. abs(black-red), while $ langle Delta y rangle$ is the mean of the residuals. . There are a few interesting things that we can notice: . when using only one neuron the NN is able only to capture the mean of the data $ langle y rangle approx 8.613$ | with N=2 neurons, the linear combinations of the two activations is already able to represent the convex 2nd-order non-linearity of the data, reducing the mean residual by an order of magnitude with respect to the model just predicting the mean (i.e. that with N=1). | obviously, increasing N results in a better approximation to the training data, for a fixed learning rate and number of training epochs, up to a residual 4x better with N=6 than with N=2. | the extrapolations of the NN models outside of the domain of the data do not follow the $x^2$ curve at all, showing that the NN models have successfully learned to reproduce the data, but have not learned completely the behaviour of the underlying curve. This exemplifies that NN models are often poor predictors outside of the domain of the training data | . We can have a closer look at the parameters obtained by the NN model with 2 neurons and see how do they compare to the simple fit by eye that we did above. To do this we can grab the named_parameters of the model with $N=2$ and print them out . for name, param in mods_sigmoid.models[1].named_parameters(): if param.requires_grad: print (name, param.data) . 0.weight tensor([[-1.4160], [ 1.4624]]) 0.bias tensor([-4.9017, -4.9822]) 2.weight tensor([[23.6837, 22.9339]]) 2.bias tensor([0.9818]) . Let&#39;s print out explicitly the function found with $N=2$ neurons . def string_func_mod(x): return (&quot;%1.0f sig(%1.1f*x %1.1f) + %1.0f sig(%1.1f*x %1.1f) + %1.1f&quot; % (x[2], x[0], x[1], x[5], x[3], x[4], x[6])) print(string_func_mod(mod2_sigmoid)) . . 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 . Really similar to the parameters we obained by eye! (results may vary a bit due to randomness) . Fit with different activation functions . We can also repeat this exercise with different activation functions, e.g. with a softsign . mods_softsign = ModelsTestingActivations(activation_fn=nn.Softsign(), units=[1,2,3,4,6], learning_rate=9e-2, num_epochs=1000) mods_softsign.run(x, y) . mods_softsign.plots() . or with a ReLU (however, ReLU is not sigmoidal in the sense of Cybenko, so strictly speaking their universal approximation theorem does not apply. It has been shown that the hypothesis on $ sigma$ can be relaxed to it being non-constant, bounded and piecewise continuous, see e.g. Hornik 1991, Leshno et al. 1993) . mods_relu = ModelsTestingActivations(activation_fn=nn.ReLU(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_relu.run(x, y) . mods_relu.plots() . While the accuracy of the NN with ReLU, as measured by $ langle Delta y rangle$, is not significantly better than with Sigmoid for the sake of this experiment, the extrapolation out-of-domain is much better. This is because the NN model with ReLU tends to linear outside of the data domain, while the NN model with Sigmoid is approximately constant outside of the range of the training data. . Part 2: How does this compares with a Bayesian likelihood estimator . It would be interesting to fully reconstruct the likelihood distribution of the parameters of the NN model. If the number of parameters is not huge - that is if we are working with a limited number of neurons in a single layer - then an exploration of the multi-dimensional likelihood distribution is still feasible. Moreover, if we are able to map the full likelihood of the model parameters we can also see where the best model found by the NN sits in the space of the likelihood. . To do this we can use a Monte Carlo Markov Chain (MCMC) analysis. This is actually what I would normally do when facing an optimization problem in physics, as often one can have a pretty good guess on a suitable functional form to use for the fitting function and, more importantly, this method naturally allows to study the uncertainties on the model found. . We&#39;re using the library emcee (paper) to run the MCMC analysis and corner (paper) to plot the posterior distribution. . import emcee import corner # the functional definition of the NN model def lmod(x, pars): &quot;&quot;&quot; A linear combination of nu sigmoidals composed with linear functions &quot;&quot;&quot; nu = int((len(pars)-1)/3) # number of neurons, with 2 biases res = pars[-1] for i in range(nu): res += sig(pars[0+i*3]*x+pars[1+i*3])*pars[2+i*3] return res # log-likelihood def lnlike(pars, x, y): &quot;&quot;&quot; This is equivalent to MSELoss(reduction=&#39;sum&#39;) &quot;&quot;&quot; y_lmod = lmod(x,pars) return -((y-y_lmod).pow(2).sum() / len(y)).item() # log-prior on the parameters def lnprior(pars): &quot;&quot;&quot; A multi-dimensional Gaussian prior with null mean, fixed std and null correlation &quot;&quot;&quot; std = 30 lp = 0. for p in pars: lp += -0.5*(p)**2/std**2-np.log(np.sqrt(2*np.pi)*std) return lp # log-probability = log-likelihood + log-prior def lnprob(pars, x, y): lk, lp = lnlike(pars, x, y), lnprior(pars) if not np.isfinite(lp) or not np.isfinite(lk): return -np.inf return lp + lk . . Let&#39;s run the MCMC analysis for a $N=2$ NN (see here for a bit more context on MCMCs with emcee) . nunits = 2 dim = 3*nunits+1 . %%time nu, nw, nstep = 2, 4*dim, 10000 # initial conditions of each chain pos = [[0]*dim + 1e-4*np.random.randn(dim) for j in range(nw)] # launch the MCMC sampler = emcee.EnsembleSampler(nw, dim, lnprob, args=(x.squeeze(), y.squeeze())) sampler.run_mcmc(pos, nstep); # collate the chains of each walker and remove the first 500 steps - the burn-in phase samples = sampler.chain[:,500:,:].reshape((-1, dim)) . /Users/lposti/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp &#34;&#34;&#34;Entry point for launching an IPython kernel. . CPU times: user 1min 23s, sys: 988 ms, total: 1min 24s Wall time: 1min 38s . Finally, let&#39;s plot the posterior probability distribution of the 7 parameters of a NN model with $N=2$ neurons and let&#39;s also mark the location of the model we obtained above with nn.Sequential . corner.corner(samples, bins=30, smooth=1.5, smooth1d=1.5, truths=mod2_sigmoid); . Find the maximum probability model, i.e. the model with highest log-prob . idmax = np.unravel_index(sampler.lnprobability.argmax(), sampler.lnprobability.shape) # maximum probability max_prob = sampler.chain[idmax[0],idmax[1],:] . print (&#39;NN model:&#39;) print (string_func_mod(mod2_sigmoid), &#39; LOSS:%1.2f&#39; % lnlike(mod2_sigmoid, x, y)) print () print (&#39;MCMC model&#39;) print (string_func_mod(max_prob), &#39; LOSS:%1.2f&#39; % lnlike(max_prob, x, y)) . NN model: 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 LOSS:-0.73 MCMC model 28 sig(-1.2*x -4.3) + 31 sig(1.0*x -3.9) + -0.2 LOSS:-0.31 . and we can also plot them side by side in comparison to the data . fig,ax = plt.subplots(figsize=(9,4.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) ax.plot(x,y,&#39;ko&#39;) ax.set_ylim((-1,32)) commons(ax[0]) ax[0].set_title(&quot;NN best model&quot;, fontsize=16) ax[0].plot(x, lmod(x, mod2_sigmoid), &#39;r.&#39;) ax[0].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), mod2_sigmoid), &#39;b--&#39;) commons(ax[1]) ax[1].set_title(&quot;MCMC max prob&quot;, fontsize=16) ax[1].plot(x, lmod(x, max_prob), &#39;r.&#39;) ax[1].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), max_prob), &#39;b--&#39;) fig,ax = plt.subplots(figsize=(9,2.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$ Delta y$&#39;) ax.set_ylim((-0.1,3)) commons(ax[0]) ax[0].plot(x, (y-lmod(x, mod2_sigmoid)).abs(), &#39;k-&#39;, lw=0.5) ax[0].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, mod2_sigmoid)).abs().mean(), transform=ax[0].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); commons(ax[1]) ax[1].plot(x, (y-lmod(x, max_prob)).abs(), &#39;k-&#39;, lw=0.5) ax[1].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, max_prob)).abs().mean(), transform=ax[1].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); . Part 3: Dealing with more complex non-linearities . Let&#39;s repeat the analysis of Part 1 but for a more complex non-linear function, for instance a sin function with an oscillating non-linear behaviour. Let&#39;s now ask ourselves how many neurons would you need to fit this function. . As before, let&#39;s start by generating some data . size = 1000 x = torch.linspace(-10, 10, size) y = torch.sin(x) + 0.2 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131f3c4a8&gt;] . Now we can use the same ModelsTestingActivations class that we wrote above, just passing the new xs and ys to the run method. Let&#39;s use a Sigmoid activation function and let&#39;s have a look at the performance of the NN models for increasing number of neurons $N$ . mods_sin_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=4e-2, num_epochs=2000) mods_sin_sigmoid.run(x, y) . mods_sin_sigmoid.plots(xextrap=20) . From these plots we can clearly notice a few important things: . the number of neurons limits the number turnovers (i.e. changes of sign of the derivative) of the function that can be fitted | an NN model with $N$ neurons is generally limited to approximate decently only function that change their increasing/decreasing tendency $N$ times | in this particular example, the sin function turnsover 6 times in the interval $[-10,10]$, thus an NN with at least $N=6$ neurons is needed to capture all the times the data turnover | also in this case, the extrapolation of the NN models outside of the domain of the data yield poor predictions. This means that the NN has learned to reproduce the data, but has not learned the underlying functional behaivour | .",
            "url": "https://lposti.github.io/MLPages/neural%20network/basics/bayesian/mcmc/jupyter/2022/10/13/neurons_non-linearities.html",
            "relUrl": "/neural%20network/basics/bayesian/mcmc/jupyter/2022/10/13/neurons_non-linearities.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Autoencoder represents a multi-dimensional smooth function",
            "content": "Autoencoder: an algorithm to learn a representation of the dataset . In this piece I&#39;m interested in a neural network that is not designed to predict an outcome given a dataset, but rather in an algorithm that is capable of learning an underlying - more compressed - representation of the dataset at hand. This can be used for a number of excitng applications such as data compression, latent representation or recovering the true data distribution. An autoencoder, which is a special encoder/decoder network, is an efficient algorithm that can achieve this. It is divided in two parts: . encoder: the algorithm learns a simple (lower dimensional) representation of the data (code) | decoder: starting from an instance of such representation, code, the algorithm develops it returning the data in the orignal (higher dimensional) form. | . The Autoencoder is then a map $ x longmapsto Psi longmapsto x$, where the first part is the encoder and the second is the decoder; thus it is effectively a map of $x$ onto itself. This implies 1) that we can use the data itself $x$ in the loss function and 2) that the algorithm is learning a representation of the dataset itself. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . A smooth multi-dimensional function: rotation curve model . To test an autoencoder network we&#39;ll use a simple rotation curve model that is the superposition of two massive components, a disc and an halo. Both components have circular velocity curves that depend on two parameters, a mass and a physical scale, such that the total model has 4 parameters, 2 for each component. . This is a nice case to test how an autoencoder learns an underlying lower dimensional representation of a dataset, since each rotation curve that we will supply to it is actually derived sampling a smooth function of just 4 parameters. . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . Sampling uniformly in disc mass, between $10^9$ and $10^{12}$ solar masses, and generating random samples of disc size, halo mass, and halo concentration following the above scaling relations. . nsamp = 1000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . Above we have generated our latent representation of the dataset, that is each galaxy model is represented by a quadruple (ms,rd,mh,cc). Below we construct a class to generate the rotation curve of the corresponding galaxy model. . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . We can now initialize this class with the samples (ms,rd,mh,cc) above, which will store inside the class the rotation curves of all the models - defined on the same radial scale (np.logscale(-1, np.log10(50), 50)) by default. . cm=curveMod(ms,rd,mh,cc) . Let&#39;s plot all the rotation curves together: . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The Autoencoder network . We can now build our Autoncoder class deriving from nn.Module. In this example, each rotation curve is a collection of 50 values (see the radial grid in curveMod) and we want to compress it down to a code of just 4 numbers. Notice that we start from the simplified case in which we assume to already know that the ideal latent representation has 4 parameters. . The encoder network has 3 layers, going from the initial $n=50$ to $32$, then to $16$, and finally to $4$. The decoder is symmetric, going from $n=4$ to $16$, then to $32$, and finally to $50$. . class AutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=4) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) def encoder(self, x): return self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))) def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def forward(self, x): encoded = self.encoder(x) decoded = self.decoder(encoded) return decoded . AutoEncoder(len(cm.rad)) . AutoEncoder( (encodeLayer1): Linear(in_features=50, out_features=32, bias=True) (encodeLayer2): Linear(in_features=32, out_features=16, bias=True) (encodeOut): Linear(in_features=16, out_features=4, bias=True) (decodeLayer1): Linear(in_features=4, out_features=16, bias=True) (decodeLayer2): Linear(in_features=16, out_features=32, bias=True) (decodeOut): Linear(in_features=32, out_features=50, bias=True) ) . Data normalization and training/validation split . We now shuffle, normalize, and split the rotation curve dataset into training and validation with a 20% validation split. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . Training loop . We now initialize the training loop, defining a simple MSE loss function and a standard Adam optimizer, and we start training . ae = AutoEncoder(len(cm.rad)) # Adam and MSE Loss loss_func = nn.MSELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.Adam(ae.parameters(), lr=0.01) for epoch in range(1001): ymod = ae.forward(xtrain) loss = loss_func(xtrain, ymod) loss.backward() optimizer.step() optimizer.zero_grad() if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % loss_func(xvalid, ae.forward(xvalid))) . 0 train L:1.04e+00 valid L:9.36e-01 50 train L:2.77e-02 valid L:2.33e-02 100 train L:5.27e-03 valid L:5.16e-03 150 train L:3.13e-03 valid L:3.33e-03 200 train L:2.59e-03 valid L:2.80e-03 250 train L:2.26e-03 valid L:2.12e-03 300 train L:1.59e-03 valid L:1.63e-03 350 train L:1.04e-03 valid L:1.14e-03 400 train L:9.29e-04 valid L:9.83e-04 450 train L:7.87e-04 valid L:8.51e-04 500 train L:9.03e-04 valid L:1.10e-03 550 train L:6.68e-04 valid L:7.73e-04 600 train L:6.68e-04 valid L:7.54e-04 650 train L:7.83e-04 valid L:7.54e-04 700 train L:6.77e-04 valid L:7.19e-04 750 train L:1.35e-03 valid L:1.92e-03 800 train L:4.56e-04 valid L:5.63e-04 850 train L:4.23e-04 valid L:5.46e-04 900 train L:8.74e-04 valid L:1.07e-03 950 train L:3.84e-04 valid L:4.79e-04 1000 train L:4.45e-04 valid L:4.95e-04 . After 1000 epochs of trainig we get down to a low and stable MSE on the validation set. We can now compare the actual rotation curves in the validation set with those decoded by the model, finding an impressively good match! . fig,ax = plt.subplots(figsize=(12,4), ncols=2) for v in datascale(xvalid,xmean,xstd): ax[0].plot(cm.rad, v) for v in datascale(ae.forward(xvalid),xmean,xstd): ax[1].plot(cm.rad, v.detach().numpy()) ax[0].set_xlabel(&#39;radius&#39;); ax[1].set_xlabel(&#39;radius&#39;) ax[0].set_ylabel(&#39;velocity&#39;); . How does the code correlate with the original 4 physical parameters? . Finally we can explore a bit the properties of the 4 values encoded by the autoencoder and try to understand how do they relate to the original 4 physical parameters. Initially we generated each rotation curve starting from a 4-ple in (ms, mh, rd, cc), where these numbers are generated from some well known scaling relations. We plot the distribution of the initial 4 parameters here: . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp = [ms, mh, rd, cc] for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(np.log10(pp[j]), np.log10(pp[i]), s=2) if j==i: ax[i,j].hist(np.log10(pp[i]), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[1,0].set_ylabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[2,0].set_ylabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); ax[3,0].set_ylabel(&#39;log10(cc)&#39;); . Now we can ask ourselves if similar correlations are observed in the 4 parameters coded by the autoencoder. In principle these are some other 4 numbers that fully specify a single rotation curve model, but that do not necessarily have anything to do with the original (ms, mh, rd, cc). . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(pp_ae[:,j], pp_ae[:,i], s=2) if j==i: ax[i,j].hist(pp_ae[:,i].detach().numpy(), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() . We can see that the coded parameters are indeed strongly correlated among themselves, however it is difficult to draw parallelisms with the behaviour we see in the original 4 parameters. An important difference to notice in these plots is that here we do not use a logarithmic scale for the 4 coded parameters, since they also take negative values unlike (ms, mh, rd, cc). . Let&#39;s now have a look at how the 4 coded parameters are correlated with the original ones. This is interesting since while the 4-dimensional latent space found by the autoencoder is not necessarily the original space of (ms, mh, rd, cc), the 4 new parameters might be well correlated with the 4 original physical quantites. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] ith = int(nsamp*(1.0-fval)) mdtrain, mhtrain = mdshuff[:ith], mhshuff[:ith] rdtrain, cctrain = rdshuff[:ith], ccshuff[:ith] mdvalid, mhvalid = mdshuff[ith:], mhshuff[ith:] rdvalid, ccvalid = rdshuff[ith:], ccshuff[ith:] partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) parvalid = (np.vstack([mdvalid, mhvalid, rdvalid, ccvalid]).T) . . Plotting the mutual correalations in the training set we do see that the 4 coded parameters are not at all randomly related to the original physical quantities. . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;=i: ax[i,j].scatter(np.log10(partrain[:,j]), pp_ae[:,i], s=2) if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); . Generating new data with the autoencoder . To finish, let&#39;s have a look at how we can use the autoencoder to generate new fake data that resembles our original dataset. We do so by random sampling from the distribution of coded values that we obtained during training. In this way we generate a new plausible code which we then decode to construct a new rotation curve. . We start by generating new code from the distribution obtained during training - to do this we use numpy.random.choice . size=500 new_pp_ae = [] for i in range(4): new_pp_ae.append(tensor(np.random.choice(pp_ae[:,i].numpy(), size))) new_code = torch.stack(new_pp_ae).T . Let&#39;s plot the original and new distributions of coded parameters: . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(-10,10,50), np.linspace(-1,20,50), np.linspace(-2.5,2.5,50), np.linspace(-5,5,50)] for i in range(4): ax[i].hist(pp_ae[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;new code&#39;); if i==1: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) . Since they look very much alike we can now decode the new code that we just generated and we are ready to plot the new rotation curves . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(ae.decoder(new_code),xmean,xstd): ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With this method we have effectively generated new rotation curves that are realistic and are not part of the training dataset. This illustrates the power of autoencoders, however to do this even better we can adapt our autoencoder to learn the underlying distribution in the code space - this is what Variatioal Autoencoders (VAEs) are for! .",
            "url": "https://lposti.github.io/MLPages/neural%20network/autoencoder/basics/jupyter/2022/10/13/autoencoder_rotcurves.html",
            "relUrl": "/neural%20network/autoencoder/basics/jupyter/2022/10/13/autoencoder_rotcurves.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Ground-up construction of a simple neural network",
            "content": "Linear layers and activation functions from scratch . When approaching the study of a new subject I find it extremely useful to get my hands dirty and play around with the stuff I&#39;m learning, in order to cement the knowledge that I&#39;m passively acquiring reading or listening to a lecture. In the case of deep learning, before starting to use massively the superb python libraries available, e.g. pytorch or fast.ai, I think it&#39;s critical to build a simple NN from scratch. . The bits required are just linear operations, e.g. matrix multiplications, functional composition and the chain rule to get the derivatives during back-propagation. All of this sounds not terrible at all, so we just need a bit of organization to glue all the pieces together. . We take inspiration from the pytorch library and we start by building an abstract Module class. . import numpy as np from torch import tensor from torch import nn import torch, math import random %config Completer.use_jedi = False rng = np.random.default_rng() . . class Module(): &quot;&quot;&quot; abstract class: on call it saves the input and output, and it returns the output &quot;&quot;&quot; def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . When called, Module stores the input and the output items and just returns the output which is defined by the method forward, which needs to be overridden by the derived class. Another method, backward, will have to return the derivative of the function, thus implementing the necessary step for back-propagation. . Let&#39;s now use the class Module to implement a sigmoid activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) class Sigmoid(Module): def forward(self, inp): return sig(inp) def bwd(self, out, inp): inp.g = sig(inp) * (1-sig(inp)) * out.g . Here the class Sigmoid inherits from Module and we just need to specify the forward method, which is just the value of the sigmoid function, and the bwd method, which is what is called by backward. We use bwd to implement the derivative of the sigmoid $$ sigma&#39;(x) = sigma(x) left[1- sigma(x) right], $$ which we store in the .g attribute, that stands for gradient, of the input. This storing the gradient of the class in the .g attribute of the input combined with the last multiplication by out.g that we do in the bwd method is basically the chain rule. The gradient in each layer of an NN is, according to the chain rule, the derivative of the layer times the derivative of the input. Once computed, we store this in the gradient of inp, which is exactly the same variable as out of the previous layer, thus we can reference its gradient with out.g when climbing back the hierarchy of layers. . Similarly, a linear layer $W{ bf x} + b$, where $w$ is a matrix, ${ bf x}$ is a vector and $b$ is a scalar, can be written as: . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) . As before, forward implements the linear layer (@ is the matrix multiplication operator in pytorch) and bwd implements the gradient. The derivative of a matrix multiplication $W{ bf x}$ is just a matrix multiplication by the transpose of the matrix, $W^T$. Since the linear layer has the weights $w$ and bias $b$ parameters that we want to learn, then we need to calculate the gradient of the output of the layer with respect to the weights and the bias. This is what is implemented in self.w.g and self.b.g. . Finally we can define the loss as a class derived from Module as: . class Mse(Module): def forward (self, inp, target): return (inp.squeeze(-1) - target).pow(2).mean() def bwd(self, out, inp, target): inp.g = 2*(inp.squeeze(-1)-target).unsqueeze(-1) / target.shape[0] . This is a mean squared error loss function, $L({ bf y},{ bf y}_{ rm target}) = sum_i (y_i-y_{i, rm target})^2$, where the forward and bwd methods have the same meaning as above. Notice that here the bwd method just stores the inp.g attribute and does not have a multiplication by out.g, because this is the final layer of our NN. . Finally we can bundle everything together in a Model class which takes as input a list of layers and implements a forward method, where maps the input into each layer sequentially, and a backward method, where it goes through the gradient of each layer in reversed order. . class Model(): def __init__(self, layers): self.layers = layers self.loss = Mse() def __call__(self, x, target): return self.forward(x, target) def forward(self, x, target): for l in self.layers: x = l(x) return self.loss(x, target) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Let&#39;s now take some fake data and let&#39;s randomly initialize the weights and biases (unsing standard Xavier initialization so that the output of the layers are still a null mean and unit variance) . n,m = 200,1 x = torch.randn(n,m) y = x.pow(2) nh = 100 # standard xavier init w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . We can now define a model as a sequence of linear and activation layers and we can make a forward pass to calculate the loss... . model = Model([Lin(w1,b1), Sigmoid(), Lin(w2,b2)]) loss = model(x, y) . ...and also a backward pass to calculate the gradients . model.backward() . The architecture above is basically equivalent to an nn.Sequential model . nn.Sequential(nn.Linear(m,nh), nn.Sigmoid(), nn.Linear(nh,1)) . Sequential( (0): Linear(in_features=1, out_features=100, bias=True) (1): Sigmoid() (2): Linear(in_features=100, out_features=1, bias=True) ) .",
            "url": "https://lposti.github.io/MLPages/neural%20network/basics/jupyter/2022/10/13/NN_from_scratch.html",
            "relUrl": "/neural%20network/basics/jupyter/2022/10/13/NN_from_scratch.html",
            "date": " • Oct 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lposti.github.io/MLPages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lposti.github.io/MLPages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Lorenzo Posti, a postdoctoral researcher in Astrophysics at the Observatory of Strasbourg (France). You can find my updated *list of publications* on [ADS](https://ui.adsabs.harvard.edu/user/libraries/fCiUp3W_T7qXYNboezCKAg) and some info on my personal [website](http://astro.u-strasbg.fr/~posti/). . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lposti.github.io/MLPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lposti.github.io/MLPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}