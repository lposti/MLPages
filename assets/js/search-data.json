{
  
    
        "post0": {
            "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
            "content": "Attribution . This code is freely distributed and it can be used and copied at will. However, if you use parts of the code below you are requested to properly cite the paper link_to_paper . Introduction . Galaxy rotation curves are usually modelled by assuming that each datapoint in the curve is independent from the others. However, this is naturally just a first order approximation, since observational effects such due to geometrical projection and resolution, as well as physical effects such as non-circular motions, can make the velocities measured in two adjacent annnuli significantly correlated. . In this notebook I use the rotation curve of NGC 2403 as a test case to show how to include Gaussian Processes (GPs) in rotation curve decomposition models, in order to account for underlying data correlations. More details can be found in the accompanying paper link_to_paper. . import numpy as np import matplotlib import matplotlib.pylab as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import jax import jax.numpy as jnp import jaxopt from functools import partial from tinygp import GaussianProcess, kernels import numpyro import arviz import corner jax.config.update(&quot;jax_enable_x64&quot;, True) import warnings warnings.filterwarnings(&#39;ignore&#39;) from matplotlib import rc rc(&#39;text&#39;, usetex=True) %config Completer.use_jedi = False %matplotlib inline # rng = np.random.default_rng() . . Data . Here I introduce the data for the galaxy NGC 2403 and the functions needed to work with Eq. (1) in the paper. . Note that the code below works also for any other galaxy whose data are formatted in the same way, e.g. it works with no modifications needed for all galaxies in the SPARC catalog (Lelli et al. 2016). . Definitions of functions for curve decomposition . I start with some definitions to specify $V_{ rm DM}(R)$, the contribution of DM to the circular velocity in Eq. (1). I assume NFW profiles for the DM halos, which are specified by two parameters: halo mass $M_{ rm h}$ and concentration $c$. $M_{ rm h}$ is the virial mass defined with a critical overdensity of $ Delta_{ rm c}=200$. . G = 4.301e-9 # gravitational constant, in Mpc km^2 s^-2 Msun^-1 H = 70. # Hubble&#39;s constant, in km s^-1 Mpc^-1 Dc= 200. # critical overdensity . Note that below I use jax, and not numpy, to define these functions. This is needed in order to do model inference with numpyro. . def jax_fc(x): return jnp.log(1+x)-x/(1+x) # definitions of virial velocity and virial radius def jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def jax_Rvir(Mh): rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G)) return 1e3 * ((Mh / rho_hat)**(1./3.)) # V_DM(R) for an NFW halo def jax_vhalo(params, R): Mh, cc = 10**params[&#39;log_mh&#39;], 10**params[&#39;log_c&#39;] rv = jax_Rvir(Mh) return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) . Finally, below I implement the model rotation curve in Eq. (1). Two things are worth pointing out: . I use a linear interpolation for the baryonic part of the curve, since I assume that $V_{ rm gas}$ and $V_ star$ are measured at fixed radii, so that params[&#39;r&#39;], params[&#39;vg&#39;] etc. are expected to be arrays of the same size. | I decomposed $V_ star$ in the bulge and disk components, with two different mass-to-light ratios. While my test case NGC 2403 has no bulge, it is good to include it here for the sake of generality. The SPARC catalog indeed includes stellar circular velocities decomposed into bulge and disk. | . def jax_vmod(params, R): return jnp.sqrt(jax_vhalo(params, R)**2 + # DM jnp.interp(R, params[&#39;r&#39;], params[&#39;vg&#39;]**2 + # gas 10**params[&#39;log_mld&#39;]*params[&#39;vd&#39;]**2+10**params[&#39;log_mlb&#39;]*params[&#39;vb&#39;]**2 # stars )) . Rotation curve data for NGC 2403 . I take the rotation curve data of NGC 2403 from the SPARC catalog at face value. I include a copy of the file here for convenience. . r, vobs, e_vobs, vg, vd, vb, _, _ = np.genfromtxt(&#39;data/NGC2403_rotmod.dat&#39;, unpack=True) . Let&#39;s plot the data below together with the best-fit model obtained by Posti et al. (2019) for reference. . params = { # parameters of the best-fit in Posti et al. (2019) &#39;log_mh&#39; : 11.4, &#39;log_c&#39; : 1.14, &#39;log_mld&#39;: -0.377, &#39;log_mlb&#39;: -99., # this galaxy has no bulge # data arrrays &#39;r&#39; : r, &#39;vg&#39; : vg, &#39;vd&#39; : vd, &#39;vb&#39; : vb, } fig,ax = plt.subplots(figsize=(8,5)) ax.errorbar(r, vobs, yerr=e_vobs, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=r&#39;$ rm data$&#39;) ax.plot(r, vg, &#39;:&#39;, c=&#39;tab:cyan&#39;, label=r&#39;$ rm gas$&#39;) ax.plot(r, np.sqrt(10**params[&#39;log_mld&#39;]*vd**2+10**params[&#39;log_mlb&#39;]*vb**2), &#39;--&#39;, c=&#39;tab:olive&#39;, label=r&#39;$ rm stars$&#39;) ax.plot(r, jax_vhalo(params, r), &#39;-.&#39;, c=&#39;tab:purple&#39;, label=r&#39;$ rm DM$&#39;) ax.plot(r, jax_vmod(params, r), c=&#39;grey&#39;, lw=2, label=r&#39;$ rm fit$&#39;) ax.set_xlabel(r&quot;$ rm radius/kpc$&quot;, fontsize=18) ax.set_ylabel(r&quot;$ rm velocity/km ,s^{-1}$&quot;, fontsize=18) ax.set_title(r&quot;$ rm NGC 2403$&quot;, fontsize=20); ax.legend(loc=&#39;lower right&#39;, fontsize=14); ax.tick_params(labelsize=14); . Models with or without GPs . Let&#39;s now get to the modelling side of things. I set up two models here. The first one is analogous to Posti et al. (2019), as well as many other works in this context (e.g. Katz et al. 2017, Li et al. 2020, Mancera-Pina et al. 2022, di Teodoro et al. 2022), it has a $ chi^2$ likelihood and it implicitly assumes that the rotation curve datapoints are independent. The second one generalizes this model by using GPs to take into account data correlations. I recommend the recent review by Aigrain &amp; Foreman-Mackey (2022), in particular their first section, as an introduction to GPs. . I use the library tinygp to set up my GP regression problem and I use numpyro to sample the posterior distribution. In particular, I use and Hamiltonian Monte Carlo sampler called No U-Turn Sampler (NUTS). . Gaussian Processes . Let&#39;s generate GPs with an Exp-Squared kernel with two parameters, an amplitude $A_k$ and a scale $s_k$, i.e. $$ k(R_i, R_j) = A_k exp left[- frac{1}{2} left( frac{|R_i-R_j|}{s_k} right)^2 right] $$ This kernel is said to be stationary because it depends only on the distance between two points $|R_i-R_j|$. . def build_gp(params, x, yerr): kernel = 10**params[&#39;log_amp&#39;]*kernels.ExpSquared(10**params[&#39;log_scl&#39;], distance=kernels.distance.L1Distance()) return GaussianProcess(kernel, x, diag=yerr**2, mean=partial(jax_vmod, params) ) . numpyro models . I now set up the model&#39;s posterior distribution to be sampled by numpyro, thus it is a function containing pyro primitives. . The model starts by defining the priors on the physical parameters ($ theta_V$ in the paper). I use: . uninformative prior on $ log ,M_{ rm h}$ | Gaussian on $ log ,c$, with mean and width following the $c-M_{ rm h}$ relation found in cosmological simulations (Dutton &amp; Maccio&#39; 2014) | Gaussian on $ log ,(M/L)_{ rm D}$, centred on $(M/L)_{ rm D}=0.5$ and with standard deviation of 0.2 dex (compatible with stellar population synthesis models, e.g. Lelli et al. 2016) | Gaussian on $ log ,(M/L)_{ rm B}$, centred on $(M/L)_{ rm B}=0.7$ and with standard deviation of 0.2 dex (again, see Lelli et al. 2016). Note that this is not used in the case of NGC 2403 | . After the definition of the priors the function branches out: one branch with GPs and one without. I borrowed this structure from the transit example of Fig. 3 in Aigrain &amp; Foreman-Mackey (2022). . The branch with GP also implement additional priors for the two parameters of the kernel ($ theta_k$ in the paper), i.e. the amplitude and scale. . def model(t, y_err, y, params, use_gp=False): # priors params[&quot;log_mh&quot;]=numpyro.sample(&quot;log_mh&quot;,numpyro.distributions.Uniform(8.0, 14.0)) params[&quot;log_c&quot;] =numpyro.sample(&#39;log_c&#39;,numpyro.distributions.Normal(0.905-0.101*(params[&#39;log_mh&#39;]*0.7-12),0.11)) params[&quot;log_mld&quot;]=numpyro.sample(&#39;log_mld&#39;,numpyro.distributions.Normal(-0.3, 0.2)) params[&quot;log_mlb&quot;]=numpyro.sample(&#39;log_mlb&#39;,numpyro.distributions.Normal(-0.15, 0.2)) if use_gp: ################### # branch WITH GPs # ################### # define kernel parameters params[&quot;log_amp&quot;] = numpyro.sample(&quot;log_amp&quot;, numpyro.distributions.Uniform(-4.0, 5.0)) params[&quot;log_scl&quot;] = numpyro.sample(&quot;log_scl&quot;, numpyro.distributions.Uniform(-2.0, 3.0)) # generate the GP gp = build_gp(params, t, y_err) # sample the posterior numpyro.sample(&quot;y&quot;, gp.numpyro_dist(), obs=y) # calculate the predicted V_rot (i.e. the mean function) of the model mu = gp.mean_function(params[&quot;r_grid&quot;]) numpyro.deterministic(&quot;mu&quot;, mu) else: ###################### # branch WITHOUT GPs # ###################### # sample the posterior numpyro.sample(&quot;y&quot;, numpyro.distributions.Normal(jax_vmod(params, t), y_err), obs=y) # calculate properties of the model numpyro.deterministic(&quot;mu&quot;, jax_vmod(params, params[&quot;r_grid&quot;])) . Running the model without GP, i.e. assuming independent datapoints . I start by sampling the posterior of the model akin to that of Posti et al. (2019), i.e. assuming that the points in the curve are independent. . I&#39;m using arviz to analyse the posterior sampled by NUTS. In particular, keep an eye on r_hat which is the Gelman-Rubin statistics: for our purposes, we can use ${ rm r_{hat}} simeq 1$ as an indicator that the marginalized posterior on a particular parameter is well determined. . grid_size = 1000 r_grid = jnp.linspace(r.min(), r.max(), grid_size) # radial grid on which to predict V_rot(R) params = {&quot;vg&quot; : vg, &quot;vd&quot; : vd, &quot;vb&quot; : vb, &quot;r&quot; : r, &quot;r_grid&quot;: r_grid} . num_warmup=1000 num_samples=3000 num_chains=3 accept_prob = 0.9 sampler_wn = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=accept_prob, ), num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains, progress_bar=True, ) %time sampler_wn.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params) inf_data_wn = arviz.from_numpyro(sampler_wn) arviz.summary(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;]) . sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 277.33it/s, 15 steps of size 2.45e-01. acc. prob=0.92] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 281.80it/s, 15 steps of size 2.07e-01. acc. prob=0.95] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:13&lt;00:00, 288.30it/s, 15 steps of size 1.95e-01. acc. prob=0.95] . CPU times: user 43.9 s, sys: 807 ms, total: 44.7 s Wall time: 45.7 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.325 | 0.014 | 11.298 | 11.351 | 0.000 | 0.000 | 3623.0 | 4045.0 | 1.0 | . log_c 1.234 | 0.019 | 1.198 | 1.268 | 0.000 | 0.000 | 3382.0 | 3801.0 | 1.0 | . log_mld -0.542 | 0.045 | -0.632 | -0.463 | 0.001 | 0.001 | 3267.0 | 3282.0 | 1.0 | . Let&#39;s plot the chains to make sure each parameter is converged . azLabeller = arviz.labels.MapLabeller(var_name_map={&quot;log_mh&quot; : r&quot;$ log ,M_{ rm h}$&quot;, &quot;log_c&quot; : r&quot;$ log ,c$&quot;, &quot;log_mld&quot;: r&quot;$ log ,M/L$&quot;, &quot;log_amp&quot;: r&quot;$A_k$&quot;, &quot;log_scl&quot;: r&quot;$s_k$&quot;}) arviz.plot_trace(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], figsize=(12,9), labeller = azLabeller); . Running the model with GP, i.e. taking into account data correlations . Now, let&#39;s have a look at what happens when GPs come into play. I run exactly the same procedure as before to sample the model&#39;s posterior, but this time I select the branch with use_gp=True. Since now I have two more free parameters, the scale and amplitude of the kernel, I also initialize these two in init_strategy. . sampler = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=accept_prob, ), num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains, progress_bar=True, ) %time sampler.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params, use_gp=True) inf_data = arviz.from_numpyro(sampler) arviz.summary(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;]) . sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:29&lt;00:00, 136.37it/s, 15 steps of size 3.46e-01. acc. prob=0.96] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:27&lt;00:00, 146.69it/s, 7 steps of size 3.67e-01. acc. prob=0.96] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:25&lt;00:00, 159.04it/s, 7 steps of size 4.01e-01. acc. prob=0.94] . CPU times: user 2min 27s, sys: 2.85 s, total: 2min 30s Wall time: 1min 38s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.491 | 0.076 | 11.353 | 11.642 | 0.001 | 0.001 | 5466.0 | 4897.0 | 1.0 | . log_c 1.079 | 0.065 | 0.956 | 1.201 | 0.001 | 0.001 | 5602.0 | 4905.0 | 1.0 | . log_mld -0.326 | 0.075 | -0.470 | -0.191 | 0.001 | 0.001 | 5531.0 | 4435.0 | 1.0 | . log_amp 1.104 | 0.270 | 0.624 | 1.602 | 0.005 | 0.004 | 3793.0 | 3277.0 | 1.0 | . log_scl -0.018 | 0.087 | -0.188 | 0.131 | 0.001 | 0.001 | 3788.0 | 3652.0 | 1.0 | . And let&#39;s plot again the chains to make sure that every parameter has converged . arviz.plot_trace(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;], figsize=(12,15), labeller = azLabeller); . Everything looks good for both models. . Comparing the predictions of the two models . Finally, I can compare the posterior distributions of the two models. I use corner to plot 1-D and 2-D projections of the posteriors. . ranges = [(11.2, 11.8), (0.8, 1.35), (-0.8,-0.0)] # NGC 2403 fig = corner.corner(inf_data_wn, bins=40, range=ranges, color=&quot;C0&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], smooth=1.0, smooth1d=1.0 ) fig = corner.corner(inf_data, bins=40, range=ranges, color=&quot;C3&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], smooth=1.0, smooth1d=1.0, labels=[&quot;$ log ,M_h$&quot;, &quot;$ log ,c$&quot;, r&quot;$ log ,M/L$&quot;], label_kwargs={&quot;fontsize&quot;:20}, fig=fig) # make legend ax = fig.axes[1] key_nn = matplotlib.lines.Line2D([], [], color=&#39;C0&#39;, linestyle=&#39;-&#39;, label=r&#39;$ rm Model , ,{ bf without , ,GPs}: , ,independent , ,data$&#39;) key_gp = matplotlib.lines.Line2D([], [], color=&#39;C3&#39;, linestyle=&#39;-&#39;, label=r&#39;$ rm Model , ,{ bf with , ,GPs}: , ,correlated , ,data$&#39;) ax.legend(loc=&#39;upper left&#39;, handles=[key_nn, key_gp], fontsize=16); . Then I can compare the predicted curve decompositions of the two models. Here I get the predictions of the model, excluding the warmup phase during sampling. . final_shape = (num_chains*(num_samples-num_warmup),grid_size) # shape of the predictions array after removing warmup pred_wn = sampler_wn.get_samples(group_by_chain=True)[&#39;mu&#39;][:,num_warmup:,:].reshape(final_shape) pred = sampler.get_samples(group_by_chain=True)[&#39;mu&#39;][:,num_warmup:,:].reshape(final_shape) . fig,ax = plt.subplots(figsize=(7,8), nrows=2, gridspec_kw={&#39;hspace&#39;:0}) def commons(ax, i, lmh, lc, lmld, lmlb, leg=True): # rotation curve ax[i].errorbar(r, vobs, yerr=e_vobs, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=r&#39;$ rm data$&#39;, markersize=4) ax[i].plot(r, vg, &#39;:&#39;, c=&#39;tab:cyan&#39;, label=r&#39;$ rm gas$&#39;) ax[i].plot(r, np.sqrt(10**lmld*vd**2+10**lmlb*vb**2), &#39;--&#39;, c=&#39;tab:olive&#39;, label=r&#39;$ rm stars$&#39;) ax[i].plot(r, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r), &#39;-.&#39;, c=&#39;tab:purple&#39;, label=r&#39;$ rm DM$&#39;) if leg: ax[i].legend(loc=&#39;lower right&#39;, fontsize=14) ax[i].set_xlabel(r&quot;$ rm radius/kpc$&quot;, fontsize=18) ax[i].set_ylabel(r&quot;$ rm velocity/km ,s^{-1}$&quot;, fontsize=18) ax[i].tick_params(direction=&#39;inout&#39;, top=True, labelsize=14) ax[0].fill_between(r_grid, np.percentile(pred_wn,2.1,axis=0), np.percentile(pred_wn,97.9,axis=0), facecolor=&#39;C0&#39;, alpha=0.3) ax[1].fill_between(r_grid, np.percentile(pred,2.1,axis=0), np.percentile(pred,97.9,axis=0), facecolor=&#39;C3&#39;, alpha=0.3) ax[0].plot(r_grid, np.median(pred_wn, axis=0), &#39;C0&#39;) ax[1].plot(r_grid, np.median(pred, axis=0), &#39;C3&#39;) commons(ax, 0, 11.325, 1.234, -0.542, -99.) commons(ax, 1, 11.491, 1.079, -0.326, -99., leg=False) . Correlation matrix . Given that the parameters of the kernel $A_k$ and $s_k$ are well constrained by the model, we can have a look at the correlation matrix of the model with GPs. . %%time def get_kmat(params, x, yerr,): gp = build_gp(params, x, yerr) xm1, xm2 = jnp.meshgrid(x,x) zm = np.zeros_like(xm1.flatten()) for i in range(len(xm1.flatten())): zm[i]=gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i]) return zm.reshape((len(x), len(x))) zm = get_kmat({&quot;log_amp&quot;:1.104, &quot;log_scl&quot;:-0.018, &#39;log_mh&#39;:11.325, &#39;log_c&#39;:1.234, &#39;log_mld&#39;:-0.542, &#39;log_mlb&#39;:-99., &#39;vg&#39;:vg, &#39;vd&#39;:vd, &#39;vb&#39;:vb, &#39;r&#39;:r}, r, e_vobs) . CPU times: user 1min 35s, sys: 2.17 s, total: 1min 37s Wall time: 2min . def plt_mat(ax, zm): im=ax.matshow(zm) ax.set_xlabel(&quot;$i$&quot;, fontsize=20) ax.set_ylabel(&quot;$j$&quot;, fontsize=20) ax.tick_params(labelsize=14) ax.xaxis.set_label_position(&#39;top&#39;) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.05) cb = plt.colorbar(im, cax=cax) cb.set_label(r&quot;$k(R_i, R_j)$&quot;, fontsize=22) cb.ax.tick_params(labelsize=16) . fig,ax = plt.subplots() plt_mat(ax, zm) . The peculiar 2-block shape of this matrix is due to the combined H$ alpha$-HI nature of the rotation curve. The optical H$ alpha$ curve samples the inner regions of the galaxy, up to $ sim 5$ kpc, with a finer spatial sampling of 0.1 kpc with respect to the HI rotation curve, which instead has a spacing of 0.5 kpc and is from 5 kpc outwards. .",
            "url": "https://lposti.github.io/MLPages/gaussian_processes/2022/11/02/gp_rotcurves.html",
            "relUrl": "/gaussian_processes/2022/11/02/gp_rotcurves.html",
            "date": " • Nov 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Gaussian Processes: modelling correlated noise in a dataset",
            "content": "How I got interested in Gaussian Processes . A few weeks ago I saw a paper passing on the arXiv by Aigrain &amp; Foreman-Mackey which is a review on Gaussian Processes (GPs) from an astrophysical perspective - it is actually targeted for astronomical time-series. I had heard a lot of things about GPs before, but I never really had the time nor will to sit down an actually understand what they are about. That changed when I glanced over the review by Aigrain &amp; Foreman-Mackey, as I quickly realised a couple of things that caught my interest. . In particular, the thing that struck me the most is their Figure 3. This appears at the end of the first Section of the review, which is mostly dedicated to two motivating examples from astronomical time-series analysis (a very well-crafted first Section I must say!). This shows the fit of a mock exoplanet transit lightcurve by two models: the both share the same physics, which is also used to generate the mock dataset, but one takes into account the correlated noise in the data, while the other doesn&#39;t and therefore works under the assumption that the datapoints are all uncorrelated and independent. . GPs are used to realistically represent the correlation in the time-series data typically observed for exoplanet transits and are an ingredient used to generate the mock dataset. So the final outcome of this controlled experiment is that only the model that accounts for correlated data, modelled with a GP, is able to recover reasonably well the true physical parameters, while the other model without a GP infers a severely biased result. . It is interesting to notice that in this example the GP introduces another layer of uncertainty in the model (i.e. how are the data correlated) over which we have to marginalise in order to arrive at the final prediction for the physical parameters. This means that the simple model that treats the data as independent is very confidently inferring a biased result (with relatively high accuracy), as opposed to the model with a GP which is instead less accurate, but unbiased. . Gaussian Processes (GPs) in rotation curve modelling . In this notebook I&#39;m going to explore how we can use GPs in modelling rotation curves of galaxies. Often - pretty much always - circular velocity measurements in galaxies are treated as independent and parametrized rotation curve models are fitted to such datasets without worrying too much if that is a reasonable assumption. Given that most HI rotation curves are derived with a tilted-ring model, I am unconfortable with assuming that each datapoint in a rotation curve is actually independent from all the others, since adjacent rings can easily enter the line-of-sight of a given radius. . For this reason I am going to generate a mock rotation curve dataset where the points are actually correlated. This will be done using a GP. Then I will be fitting this dataset with two models: one assuming that the data are independent, and another taking into account the correlated noise. . import numpy as np import matplotlib import matplotlib.pylab as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import jax import jax.numpy as jnp import jaxopt from functools import partial from tinygp import GaussianProcess, kernels import numpyro import arviz import corner jax.config.update(&quot;jax_enable_x64&quot;, True) import warnings warnings.filterwarnings(&#39;ignore&#39;) %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . Definitions and rotation curve sampling . Define the mathematical functions for the galaxy rotation curves . G, H, Dc = 4.301e-9, 70, 200. # accessory functions def jax_fc(x): return jnp.log(1+x)-x/(1+x) def jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def jax_Rvir(Mh): rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G)) return 1e3 * ((Mh / rho_hat)**(1./3.)) # actual rotation curve def jax_vhalo(params, R): Mh, cc = 10**params[&#39;log_mh&#39;], 10**params[&#39;log_c&#39;] rv = jax_Rvir(Mh) return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) . Lat&#39;s now plot a randomly sampled rotation curve with a typical error on each datapoint of 8 km/s. As a first step, we will assume that each datapoint is independent and thus we will sample from a Gaussian noise distribution for each measurement. . size = 40 lmh, lc, verr = 12, 0.95, 10.0 # generating independent datapoints with Gaussian errors x = np.linspace(0.1, 50, size) y = jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x) + rng.normal(loc=0, scale=verr, size=size) fig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) # rotation curve ax[0].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0].errorbar(x, y, yerr=verr, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=&#39;data&#39;) ax[0].legend(loc=&#39;lower right&#39;) ax[0].set_ylabel(&#39;velocity&#39;); # residuals ax[1].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1].errorbar(x, y-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=verr, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5) ax[1].set_xlabel(&#39;radius&#39;) ax[1].set_ylabel(&#39;residuals&#39;) ax[1].set_ylim(-60,60); . In the simple case plotted above we have generated the points with the implicit assumption that each datapoint was independent from all the others. This is why mathematically we simply added a Gaussian noise term to the median curve when defining y. In the residuals plot, the fact that each datapoint is independent becomes apparent since there is no clear trend in the residuals as a function of radius. . However, in practice this is rarely the case with astronomical observations, since typically instrumental characteristics of the telescope and physical processes make the measurement of a single datapoint to have a non-negligible dependence on some other datapoints. Most of the times when modelling astrophysical data we do not know precisely if and which measurements are correlated with which others, so it is in our best interest to employ a modelling technique that allows for correlated datapoints, instead of assuming they are independent. This is where GPs come into play. . Generating rotation curve data with correlated noise using GPs . Let&#39;s now use GPs to generate a new set of datapoints, but this time they will be correlated to one another. To specify this correlation we need to define a kernel or a covariance function which, in the simplest case that we are using here, is a function only of the physical distance of each point (absolute or L1 distance). . Kernels that depend only of the distance of points are called stationary. A very common kernel function used in GPs is the so-called radial basis function (RBF) or exponential-squared, since $k(x_i, x_j) propto exp left(d_{ij}^2/2s right)$, where $d_{ij}=|x_i-x_j|$ is the distance of the two datapoints, while $s in mathbb{R}$ is a scale parameter. . Build the GP with tinygp . We define the GP as follows using the library tinygp. . def build_gp(params, x, yerr): kernel = 10**params[&#39;log_amp&#39;]*kernels.ExpSquared(10**params[&#39;log_scl&#39;], distance=kernels.distance.L1Distance()) return GaussianProcess(kernel, x, diag=yerr**2, mean=partial(jax_vhalo, params) ) . This GP has 2 adjustable parameters: an amplitude log_amp and a scale log_scl (both defined in log). We build the GP by passing it the kernel function, the set of datapoints (just x, not the velocity measurements), the measured uncertainty of the measurements, and the mean function that needs to be added to the noise generated by the GP. . What the library is doing is just building a full covariance matrix on the dataset x using the kernel function provided. The value that we pass on the diag argument will be considered as an additional variance to be added to the covariance matrix. . Let&#39;s now initialize a GP with some amplitude and scale parameters and let&#39;s sample random datapoints from its covariance matrix. . params = {&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc, &#39;log_amp&#39;:jnp.log10(300.0), &#39;log_scl&#39;:jnp.log10(5.0)} # initialize the GP and sample from it gp = build_gp(params, x, verr) # vm = gp.sample(jax.random.PRNGKey(11)) vm = gp.sample(jax.random.PRNGKey(33)) e_vm = np.sqrt(gp.variance) . Here gp.sample gets a random realization of y-measurements on the x-array. We can define their standard errorbars by just taking the variances (i.e. diagonal of the covariance matrix). . The covariance matrix . Just to help out with visualising the GP, let&#39;s plot the covariance matrix for this problem. . def plt_mat(ax, params, x, yerr): gp = build_gp(params, x, yerr) xm1, xm2 = jnp.meshgrid(x,x) zm = np.zeros_like(xm1.flatten()) for i in range(len(xm1.flatten())): zm[i]=(gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i])) im=ax.matshow(zm.reshape((len(x), len(x))), extent=(x.min(), x.max(), x.max(), x.min())) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.05) plt.colorbar(im, cax=cax) . fig,ax = plt.subplots() plt_mat(ax, params, x, verr) . Here each point is coloured according to the covariance $k(x_i,x_j)$. It is highest along the diagonal, where $k(x_i,x_i)=300$ km$^2$/s$^2$, implying a standard uncertainty on each data point of $ sigma= sqrt{300+10^2}=20$ km/s, where the term $10^2$ comes from adding the measured uncertainties to the covariance matrix (the diag argument in GaussianProcess). Given the kernel function and the scale of $5$ that we used in this example, we can see that each datapoint has a significant correlation with all the points closer than ~10. . Sampling the rotation curve with correlated noise . Finally, let&#39;s plot the sampled rotation curve and its residuals and let&#39;s compare them with the uncorrelated case above. . fig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) # rotation curve ax[0].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0].errorbar(x, vm, yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5, label=&#39;data&#39;) ax[0].legend(loc=&#39;lower right&#39;) ax[0].set_ylabel(&#39;velocity&#39;); # residuals ax[1].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1].errorbar(x, vm-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5) ax[1].set_xlabel(&#39;radius&#39;) ax[1].set_ylabel(&#39;residuals&#39;) ax[1].set_ylim(-60,60); . We do see quite a lot of structure in the residuals plot this time! This is in stark contrast to the picture we had when generating datapoints independently. This time each measurements feels the influence of the other measurements closer than ~10 in radius, thus the rotation curve starts having significant trends above and below the mean. . When fitting the rotation curve these trend can be misinterpreted as signal, instead of just correlated noise, and this can potentially bias our inference on the curve parameters quite significantly. We see below an example of this. . Fitting the rotation curve with or without Gaussian Processes . Let&#39;s now consider the rotation curve generated with the GP above, i.e. the blue set of points, and let&#39;s build a model to fit it. The model is the same jax_vhalo function that we used to generate the data, which has 2 free parameters: a mass log_mh and a concentration log_c. . We run the fit in a Bayesian framework and in particular with an MCMC sampler using a standard $ chi^2$ log-likelihood on the observed datapoints. We impose a uniform prior on log_mh and normal prior on log_c, whose mean follows the well-known mass-concentration relation of dark matter halos in $ Lambda$CDM (Dutton &amp; Maccio&#39; 2014). . We use the library numpyro to define the model and to run the MCMC sampling. In particular, numpyro uses a state-of-the-art Hamiltonian No U-Turn Sampler (NUTS) to derive the posterior of the parameters. . We run the fit two times: the first time, we treat the datapoints as independent and we have a &quot;standard&quot; Bayesian inference on the parameters; the second time, we allow the data to be correlated and we model their correlation with a GP with an exp-squared kernel that has two additional free parameters, an amplitude log_amp and a scale log_scl. We impose an uniformative uniform prior on the two parameters of the kernel. . Model without GP . r_grid = jnp.linspace(0.1, 50.0, 1000) def model(t, y_err, y, use_gp=False): # priors log_mh = numpyro.sample(&#39;log_mh&#39;, numpyro.distributions.Uniform(8.0, 14.0)) log_c = numpyro.sample(&#39;log_c&#39;, numpyro.distributions.Normal(0.905-0.101*(log_mh-12.0), 0.15)) # parameters of the underlying physical model params = {&quot;log_mh&quot;: log_mh, &quot;log_c&quot; : log_c} if use_gp: # branch WITH GPs # # define kernel parameters params[&quot;log_amp&quot;] = numpyro.sample(&quot;log_amp&quot;, numpyro.distributions.Uniform(-4.0, 5.0)) params[&quot;log_scl&quot;] = numpyro.sample(&quot;log_scl&quot;, numpyro.distributions.Uniform(-2.0, 3.0)) # generate the GP gp = build_gp(params, t, y_err) # sample the posterior numpyro.sample(&quot;y&quot;, gp.numpyro_dist(), obs=y) # calculate properties of the model mu = gp.mean_function(r_grid) numpyro.deterministic(&quot;mu&quot;, mu) numpyro.deterministic(&quot;gp&quot;, gp.condition(y, r_grid, include_mean=False).gp.loc) else: # branch WITHOUT GPs # # sample the posterior numpyro.sample(&quot;y&quot;, numpyro.distributions.Normal(jax_vhalo(params, t), y_err), obs=y) # calculate properties of the model numpyro.deterministic(&quot;mu&quot;, jax_vhalo(params, r_grid)) . Sampling the posterior with numpyro . The model function above has the numpyro primitives like numpyro.sample that are used by the NUTS MCMC sampler to construct the posterior. Below we run the model the first time selecting the use_gp=False branch, i.e. assuming that the data are independent. . We launch 2 chains for 3000 steps (of which 1/3 of warmup) of the NUTS sampler, starting from a specific value of the parameters that is not too far from the truth (to convieniently speed up convergence). We then use the arviz package to evaluate some statistics on the posterior samples. . sampler_wn = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=0.9, init_strategy=numpyro.infer.init_to_value(values={&#39;log_mh&#39;:11.0, &#39;log_c&#39;:1.0}), ), num_warmup=1000, num_samples=3000, num_chains=2, progress_bar=True, ) %time sampler_wn.run(jax.random.PRNGKey(11), x, e_vm, vm) inf_data_wn = arviz.from_numpyro(sampler_wn) arviz.summary(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;]) . sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11&lt;00:00, 357.51it/s, 3 steps of size 8.43e-01. acc. prob=0.91] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11&lt;00:00, 357.45it/s, 3 steps of size 7.96e-01. acc. prob=0.93] . CPU times: user 18.3 s, sys: 1.07 s, total: 19.4 s Wall time: 23.4 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.897 | 0.060 | 11.786 | 12.009 | 0.001 | 0.001 | 4581.0 | 3535.0 | 1.0 | . log_c 1.087 | 0.058 | 0.983 | 1.200 | 0.001 | 0.001 | 4758.0 | 3911.0 | 1.0 | . Explore the MCMC samples with arviz . The posterior has been successfully sampled and we can now have a look at the marginalized distributions of the two physical parameters, mass and concentration. . arviz.plot_density(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], hdi_prob=0.99, colors=&#39;k&#39;, shade=0.1); . Model with GP . Let&#39;s now repeat the fitting procedure, but this time for the use_gp=True branch of model, i.e. allowing for the data to be correlated. . sampler = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=0.9, init_strategy=numpyro.infer.init_to_value(values={&#39;log_mh&#39;:11.0, &#39;log_c&#39;:1.0, &#39;log_amp&#39;:1.0, &#39;log_scl&#39;:0.5 }), ), num_warmup=1000, num_samples=3000, num_chains=2, progress_bar=True, ) %time sampler.run(jax.random.PRNGKey(11), x, e_vm, vm, use_gp=True) inf_data = arviz.from_numpyro(sampler) arviz.summary(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;]) . sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:21&lt;00:00, 183.03it/s, 7 steps of size 2.41e-01. acc. prob=0.97] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 273.18it/s, 31 steps of size 1.56e-01. acc. prob=0.97] . CPU times: user 1min, sys: 2.38 s, total: 1min 2s Wall time: 44.9 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.974 | 0.261 | 11.525 | 12.476 | 0.006 | 0.004 | 2440.0 | 1578.0 | 1.0 | . log_c 0.989 | 0.142 | 0.716 | 1.258 | 0.003 | 0.002 | 2553.0 | 2809.0 | 1.0 | . log_amp 2.723 | 0.647 | 1.929 | 3.712 | 0.039 | 0.027 | 942.0 | 580.0 | 1.0 | . log_scl 0.840 | 0.233 | 0.517 | 1.143 | 0.008 | 0.008 | 1543.0 | 1419.0 | 1.0 | . In this case, we have two additional free parameters that are the amplitude and scale of the GP kernel. These can be considered as nuisance parameters in the present case, since we are only interested in the distributions of the two physical parameters marginalized over everything else. . The chain statistics summarized above look great, but let&#39;s inspect the plot of the autocorrelation time to be extra sure that all the chains are converged and well-behaved. . arviz.plot_autocorr(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], max_lag=200); . From this plot we can see that the autocorrelation of both parameters for all chains tends to die out for sufficiently large lags. This confirms that the MCMC samples that we have derived are actually independent and can be reliably used to infer the posterior. . Let&#39;s now compare the marginalized distributions of log_mh and log_c between the two modeling runs. . arviz.plot_density([inf_data_wn, inf_data], var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], data_labels=[&quot;independent data&quot;, &quot;correlated data&quot;], hdi_prob=0.99, colors=[&#39;k&#39;,&#39;C0&#39;], shade=0.1); . We clearly see that if we working under the assumption of uncorrelated data the resulting posteriors are thinner and somewhat biased. On the other hand, by allowing the data to be correlated in the fit, as modelled by a GP, the resulting posterior are significantly wider and more uncertain, but are significantly less biased. . Comparing the 2 models with a corner plot . Let&#39;s have a look at the corner plot of the marginalised posterior distribution in the mass-concentration space. . ranges = [(11.4, 12.6), (0.5, 1.4)] # PRNG 33 fig = corner.corner(inf_data_wn, bins=40, range=ranges, color=&quot;k&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], smooth=1.0, smooth1d=1.0 ) fig = corner.corner(inf_data, bins=40, range=ranges, color=&quot;C0&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], smooth=1.0, smooth1d=1.0, labels=[&quot;$ log ,M_h$&quot;, &quot;$ log ,c$&quot;], truths=[params[&#39;log_mh&#39;], params[&#39;log_c&#39;]], truth_color=&#39;tab:pink&#39;, fig=fig) # make legend ax = fig.axes[1] key_tr = matplotlib.lines.Line2D([], [], color=&#39;tab:pink&#39;, linestyle=&#39;-&#39;, marker=&#39;s&#39;, label=&#39;truth&#39;) key_nn = matplotlib.lines.Line2D([], [], color=&#39;k&#39;, linestyle=&#39;-&#39;, label=&#39;independent data&#39;) key_gp = matplotlib.lines.Line2D([], [], color=&#39;C0&#39;, linestyle=&#39;-&#39;, label=&#39;correlated data&#39;) ax.legend(loc=&#39;upper right&#39;, handles=[key_tr, key_nn, key_gp]); . This figure shows more clearly how the first run of the fit, assuming independent data, infers a clearly biased (lower) mass and (higher) concentration. On the other hand, when including a GP in the model to account for correlations in the data the posterior on $M_h$ and $c$ becomes perfectly compatible with the true value within 1-$ sigma$. . Predicted rotation curves of the 2 models . Finally, let&#39;s plot the predicted rotation curves of the two models in comparison with the data. . pred_wn = sampler_wn.get_samples(group_by_chain=True)[&#39;mu&#39;][:,1000:,:].reshape((4000,1000)) pred = sampler.get_samples(group_by_chain=True)[&#39;mu&#39;][:,1000:,:].reshape((4000,1000)) pred_cd = (sampler.get_samples(group_by_chain=True)[&#39;mu&#39;]+ sampler.get_samples(group_by_chain=True)[&#39;gp&#39;])[:,1000:,:].reshape((4000,1000)) # get random subset inds = np.random.randint(0, 2000, 20) . fig,ax = plt.subplots(figsize=(16,6), ncols=3, nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) def commons(ax, i): # rotation curve ax[0,i].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0,i].errorbar(x, vm, yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5, label=&#39;data&#39;) ax[0,i].legend(loc=&#39;lower right&#39;) ax[0,i].set_ylabel(&#39;velocity&#39;); # residuals ax[1,i].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1,i].errorbar(x, vm-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5) ax[1,i].set_xlabel(&#39;radius&#39;) ax[1,i].set_ylabel(&#39;residuals&#39;) ax[1,i].set_ylim(-60,60); commons(ax,0) commons(ax,1) commons(ax,2) ax[0,0].plot(r_grid, pred_wn[inds].T, &#39;k&#39;, alpha=0.1) ax[0,1].plot(r_grid, pred[inds].T, &#39;C0&#39;, alpha=0.2) ax[0,2].plot(r_grid, pred_cd[inds].T, &#39;C2&#39;, alpha=0.1) ax[1,0].plot(r_grid, (pred_wn[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;k&#39;, alpha=0.1) ax[1,1].plot(r_grid, (pred[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;C0&#39;, alpha=0.2) ax[1,2].plot(r_grid, (pred_cd[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;C2&#39;, alpha=0.1); ax[0,0].set_title(&quot;Model assuming independent data&quot;); ax[0,1].set_title(&quot;Model with GP: correlated data&quot;); ax[0,2].set_title(&quot;Model with GP &quot;+r&quot;$ rm bf conditioned$&quot;+&quot; to data&quot;); . In the left-hand panels we compare 20 random samples of the predicted rotation curve for the first iteration of the model, i.e. the one treating each datapoint as independent. We see that the model rotation curves (in black) tend to overshoot the mean at small radii and tend to fall below it at large radii - this becomes particularly clear in the residuals plot. The reason for this discrepancy is that this model finds a biased result, predicting a higher concentration and a lower mass than the true values. . The middle panels similarly compare samples of the predictions of the second model, i.e. the one that uses GPs to model correlations among successive datapoints. We see that while the prediction of the rotation curve (in blue) becomes much less accurate, it is now unbiased. In fact, the true mean rotation curve that we used to generate the dataset is very well encompassed by the random samples of this model. . The right panels demonstrate why such a large variety of rotation curve shapes in the blue model is consistent with the dataset. In fact, each blue curve is itself a GP and when conditioning) it to the measurements we obtain the green curves in the right panels, which are all consistent with the data (reduced $ chi^2$ less than unity). .",
            "url": "https://lposti.github.io/MLPages/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes.html",
            "relUrl": "/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes.html",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Variational Autoencoder: learning an underlying distribution and generating new data",
            "content": "Variational AutoEncoder (VAE): an algorithm to work with distributions . This notebook deals with generating an Autoencoder model to learn the underlying distribution of the data. To do this we have to modify the autoencoder such that the encoder does not learn a compressed representation of the input data, but rather it will learn the parameters of the distribution of the data in the latent (compressed) space. . So the idea is to start from an observed sample of the distribution of the data $P({ bf X})$ and to pass this to the encoder which will reduce its dimensionality, i.e. $P({ bf X}) mapsto P&#39;({ bf X}_{ rm c})$ where ${ bf X} in mathrm{R}^m$ and ${ bf X}_{ rm c} in mathrm{R}^n$ with $n&lt;m$. In other words, in a VAE the encoder step does not represent the input data ${ bf X}$ with a code ${ bf X}_{ rm c}$, but rather the initial data distribution $P({ bf X})$ with a compressed distribution $P&#39;({ bf X}_{ rm c})$, which we usually need to approximate in some analytic form, e.g. a multi-variate normal $P&#39;({ bf X}_{ rm c}) sim mathcal{N}( mu, Sigma)$. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random import corner %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . # halo concentration--mass relation def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . . Let&#39;s start again by generating the distribution of physical parameters and calculating the rotation curve of a galaxy with those parameters. This part is taken from the blog post on Autoencoders so I just refer to that for details. . nsamp = 2000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . cm=curveMod(ms,rd,mh,cc) . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . This plot shows a random realization of nsamp rotation curves from our physical model. These curves are the dataset the we are going to use to train our Variational Autoencoder. Let&#39;s start by normalizing the data and defining the training and validation sets. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/torch/csrc/utils/tensor_new.cpp:204.) &#34;&#34;&#34; . The Variational Autoencoder . We now build the class, deriving from nn.Module, for our Variational AutoEncoder (VAE). In particular, we define the layers in the __init__ method and we define an encoder and a decoder method, just as we did for the Autoencoder. However, this class is quite a lot richer than the one we used for and Autoencoder and we will go through each method below. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Sequential(nn.Linear(in_features=ninp, out_features=32), nn.ReLU()) self.encodeLayer2 = nn.Sequential(nn.Linear(in_features=32, out_features=16), nn.ReLU()) self.encodeOut = nn.Linear(in_features=16, out_features=8) self.decodeLayer1 = nn.Sequential(nn.Linear(in_features=4, out_features=16), nn.ReLU()) self.decodeLayer2 = nn.Sequential(nn.Linear(in_features=16, out_features=32), nn.ReLU()) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar = torch.split(self.encodeOut(self.encodeLayer2(self.encodeLayer1(x))),4,dim=1) return mean, logvar def decoder(self, encoded): return self.decodeOut(self.decodeLayer2(self.decodeLayer1(encoded))) def reparametrize(self, mean, logvar): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) return eps * torch.exp(logvar * 0.5) + mean # exp(0.5logvar) = std # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed # https://arxiv.org/pdf/1312.6114.pdf # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes?noredirect=1&amp;lq=1 def _ELBO(self, x, decoded, mean, logvar): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim = 1) return (KLdiv - logpx_z).mean() def forward(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, logvar) return decoded def getELBO_loss(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, logvar) . Ok, so there&#39;s a lot to break down here! . First of all, we notice that the overall structure of the encoder/decoder network is relatively similar to the autoencoder we saw before, with the important difference that the encoder now returns 8 parameters instead of 4. These are the parameters of the multi-variate normal distribution with which we represent the 4-dimensional latent space, so mean and variance for each of the four physical properties that generate the rotation curves. Thus, the encoder step does not output a code, but means and variances for each physical property. | the decoder step is instead totally similar to a simple autoencoder and in fact it requires a code as an input. In order to generate a code from the means and variances that come out of the encoder phase without breaking the backprop flow of the algorithm, Kingma &amp; Welling (2013) proposed to use a reparametrization trick, which consists of throwing a new sample from a standard normal and then shifting this to have the same mean and variance as given by the encoder. | the forward method of this class follows these steps: the encoder gives means and variances of the latent space, the reparametrization trick is used to generate a code, which is finally decoded by the decoder. | the appropriate loss function for a variational autoencoder is the Evidence Lower BOund (ELBO). In fact, minimising the -ELBO means maximising a lower bound on the evidence or likelihood of the model. The evidence, or reconstruction loss, is logpx_z which is just an MSE loss on the data and the decoded output of the autoencoder. This term encourages the reconstruction of the dataset and tends to prefer separated encodings for each element of the dataset. The other term, KLdiv, is the Kullback-Leibler divergence of the proposed distribution in the latent space with the likelihood. This term has the opposite effect of promoting overlapping encodings for separate observations. For this reason, maximising ELBO guarantees to achieve a nice compromise between representing the original data and the ability to generalize by generating realistic new data. | . With these changes our neural network is now capable of constructing an approximation for the distribution of the four physical parameters in the latent space. We can now run the usual optimization algorithm and start training this model. . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=1e-2) for epoch in range(2000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%100==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.46e+04 valid L:1.76e+04 100 train L:1.29e+03 valid L:3.67e+02 200 train L:5.40e+02 valid L:1.73e+02 300 train L:4.72e+02 valid L:1.39e+02 400 train L:1.42e+02 valid L:6.75e+01 500 train L:1.19e+02 valid L:5.85e+01 600 train L:9.75e+01 valid L:5.35e+01 700 train L:1.05e+02 valid L:5.57e+01 800 train L:7.98e+01 valid L:4.68e+01 900 train L:1.91e+02 valid L:7.53e+01 1000 train L:7.49e+01 valid L:4.26e+01 1100 train L:8.18e+01 valid L:4.40e+01 1200 train L:6.40e+01 valid L:3.92e+01 1300 train L:6.89e+01 valid L:3.86e+01 1400 train L:6.15e+01 valid L:3.69e+01 1500 train L:6.42e+01 valid L:3.66e+01 1600 train L:5.76e+01 valid L:3.55e+01 1700 train L:5.99e+01 valid L:3.60e+01 1800 train L:1.28e+02 valid L:5.11e+01 1900 train L:4.93e+01 valid L:3.26e+01 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The plot above shows the distribution of rotation curves that the VAE has learned. We can see that there is a quite large variety of rotation curve shapes that can be represented by this model. Notice that the region in the radius-velicity space covered by the VAE is indeed quite similar to that of the training set (see plot above). . This shows that the VAE has indeed learned an effective distribution in the latent space which generates the rotation curve dataset we started from. . Exploring the latent space distribution . We can now have a look at what the VAE has learned about the latent space. To do so, we can take the means and variances derived by the encoder on the training set and we can use them to generate samples on the latent space. Basically for each $x_i$ in the training set we get a $ mu_i$ and a $ sigma^2_i$ and we draw 100 samples from $ mathcal{N}( mu_i, sigma^2_i)$. . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . We thus have an array of training_set_size x N_samples = 1600 x 100 = 160000 samples generated from the distribution inferred by the VAE on the latent space. Let&#39;s now plot them (I&#39;m using corner to do so) . rr = ((-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . Here in black we plotted the resulting samples of the latent space as described above, while in orange we overplot just the means $ mu_i$ for each element in the training set. It turns out that the distribution of the means (in orange) is virtually identical to that derived from sampling each multi-variate Gaussian in the latent space (in black). . This implies that the variances $ sigma^2_i$ that are the output of the encoder are generally quite small and that the VAE effectively reconstructs the distribution of the latent space by superimposing many thin Gaussians, all with slightly different mean and small variance. In fact, one could interpret the orange distribution as a superposition of Dirac deltas centered at each mean derived by the encoder on the training set, i.e. $ sum_i delta_i(x- mu_i)$. . Let&#39;s now plot together the physical parameters that we used to generate each rotation curve in the training set, together with the means derived by the encoder step on each element of that set. This allows us to explore whether there are any correlations between the original 4 physical parameters and the 4 dimensions of the latent space constructed by the VAE. . To do this we can stack together the tensor of physical parameters and that of the means. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] mdtrain, mhtrain = mdshuff[:int(nsamp*(1.0-fval))], mhshuff[:int(nsamp*(1.0-fval))] rdtrain, cctrain = rdshuff[:int(nsamp*(1.0-fval))], ccshuff[:int(nsamp*(1.0-fval))] # physical parameters corresponding to each element of the training set partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) # stacking the tensor of phsyical parameters with that of the means derived by the encoder dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr2 = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr2, smooth=0.75, smooth1d=0.75, labels=ll); . Ok, now this corner plot has a lot of information...let&#39;s breakdown the most important ones: . the first 4 blocks of this corner plot are relative to the physical parameters (ms,mh,rd,cc) and show the marginal distribution of each of these parameters and how they are correlated with each other. | the last 4 blocks, instead, are relative to the 4 latent parameters (L1,L2,L3,L4), and again show the marginal distribution of each and their mutual correlations - this is equivalent to the corner plot just above (in that case this was plotted in orange colour) | the 4x4 sub-block on the lower-left corner is possibly the most interesting one as it is the &quot;mixed&quot; block that highlights the relation between the physical and latent parameters. Each of these 16 panels show the correlation of one of the physical parameters with one of the latent ones. We can see that there are several significant correlations (e.g. ms-L1, mh-L2, rd-L3), meaning that these two sets are not independent. | . Generating new realistic data . Now that we have a working approximation of the distribution in the latent space it is easy to use the VAE to generate new rotation curves. We will showcase now a quite simplistic way to do it, which is by assuming that we can represent the latent space with a 4-dimensional Gaussian, even though we have seen in the plots above that the actual distribution is more complex. . We consider the means $ mu_i$ that the encoder derives for the full training set and we take the mean and standard deviation of these. We use these two parameters to define a normal distribution . size=500 dd = torch.distributions.normal.Normal(msm.mean(dim=0), msm.std(dim=0)) new_code = dd.sample(torch.Size([size])) . Let&#39;s make a comparison by plotting the marginalised distributions of the 4 latent parameters with that of the normal distribution that we are assuming . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(rr[0][0],rr[0][1],20), np.linspace(rr[1][0], rr[1][1],20), np.linspace(rr[2][0],rr[2][1],20), np.linspace(rr[3][0], rr[3][1],20)] for i in range(4): ax[i].hist(msm[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;$ mathcal{N}$&#39;+&#39;-approx&#39;); if i==0: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) ax[i].set_xlabel(&#39;L%1d&#39;%(i+1)) . Quite a big difference! . However, this is not an issue since we are just using a normal distribution since it is convenient to sample and for us it is just a means to generate plausible code to be interpreted by the decoder. As a matter of fact, by doing this we are able to generate quite a new variety of possible galaxy rotation curves. . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(vae.decoder(new_code),xmean,xstd)[:100]: ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With a bit on work on the reparametrization trick, we can relax the assumption that the distribution in the latent space is of a multi-variate, but uncorrelated, normal. In practice, instead of having just 4 means and 4 variances as output of the encoder step, we also add 6 covariances, so that we can define the full non-zero covariance matrix of the multi-variate normal in the latent space. . Unfortunately this require quite a bit more math on the reparametrization trick, which is not anymore just $ epsilon* sigma+ mu$, but where the full covariance matrix is used. This calculation requires, among other things, to derive the square root of a matrix, for which we employ the SVD decomposition: if $A = U ,{ rm diag}(s) ,V^{ rm T}$, where $A, U, V in mathbb{R}^{n, n}$, $s in mathbb{R}^n$, and diag$(s)$ is a diagonal matrix with the elements of $s$ as diagonal, then $ sqrt{A} = U ,{ rm diag} left( sqrt{s} right) ,V^{ rm T}$. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=14) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar, covs = torch.split(self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))), [4, 4, 6], dim=1) return mean, logvar, covs def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def reparametrize(self, mean, m_cov): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) # return eps * var.sqrt() + mean # find matrix square root with SVD decomposition # https://math.stackexchange.com/questions/3820169/a-is-a-symmetric-positive-definite-matrix-it-has-square-root-using-svd?noredirect=1&amp;lq=1 U,S,V = torch.svd(m_cov) # A = U diag(S) V.T dS = torch.stack([torch.diag(S[i,:]) for i in range(S.shape[0])]) # sqrt(A) = U diag(sqrt(S)) V.T cov_sqrt = torch.einsum(&#39;bij,bkj-&gt;bik&#39;,torch.einsum(&#39;bij,bjk-&gt;bik&#39;,U,dS.sqrt()),V) return torch.einsum(&#39;bij,bi-&gt;bj&#39;, cov_sqrt, eps) + mean def _ELBO(self, x, decoded, mean, m_cov, var): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * (torch.log(m_cov.det()) + 4 - torch.sum(mean**2 + var, dim = 1)) return torch.mean((KLdiv - logpx_z)[~(KLdiv - logpx_z).isnan()]) # torch.nanmean def _get_m_cov(self, logvar, covs): # covariance matrix m_cov = torch.zeros(logvar.shape[0], 4, 4) m_cov[:,[0,1,2,3],[0,1,2,3]] = logvar.exp() m_cov[:,[0,0,0,1,1,2],[1,2,3,2,3,3]] = covs m_cov[:,[1,2,3,2,3,3],[0,0,0,1,1,2]] = covs # var = torch.einsum(&#39;bii-&gt;bi&#39;, m_cov) return m_cov, logvar.exp() def forward(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, m_cov, var) return decoded def getELBO_loss(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, m_cov, var) . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=0.4e-2) for epoch in range(1000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.26e+04 valid L:2.21e+04 50 train L:6.80e+03 valid L:1.42e+03 100 train L:1.47e+03 valid L:4.22e+02 150 train L:9.56e+02 valid L:2.77e+02 200 train L:7.22e+02 valid L:1.98e+02 250 train L:5.99e+02 valid L:2.26e+02 300 train L:4.56e+02 valid L:1.53e+02 350 train L:4.18e+02 valid L:1.85e+02 400 train L:3.63e+02 valid L:1.66e+02 450 train L:3.53e+02 valid L:1.57e+02 500 train L:3.33e+02 valid L:1.39e+02 550 train L:4.32e+02 valid L:1.75e+02 600 train L:3.07e+02 valid L:1.39e+02 650 train L:3.10e+02 valid L:1.10e+02 700 train L:2.77e+02 valid L:1.09e+02 750 train L:9.70e+02 valid L:3.07e+02 800 train L:6.58e+02 valid L:3.33e+02 850 train L:4.97e+02 valid L:2.41e+02 900 train L:4.53e+02 valid L:1.82e+02 950 train L:6.56e+02 valid L:3.57e+02 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . rr = ((-25,30),(-40,15),(-30,10),(-20,6)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-25,30),(-40,15),(-30,10),(-20,6)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr, smooth=0.75, smooth1d=0.75, labels=ll); .",
            "url": "https://lposti.github.io/MLPages/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves.html",
            "relUrl": "/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Autoencoder represents a multi-dimensional smooth function",
            "content": "Autoencoder: an algorithm to learn a representation of the dataset . In this piece I&#39;m interested in a neural network that is not designed to predict an outcome given a dataset, but rather in an algorithm that is capable of learning an underlying - more compressed - representation of the dataset at hand. This can be used for a number of excitng applications such as data compression, latent representation or recovering the true data distribution. An autoencoder, which is a special encoder/decoder network, is an efficient algorithm that can achieve this. It is divided in two parts: . encoder: the algorithm learns a simple (lower dimensional) representation of the data (code) | decoder: starting from an instance of such representation, code, the algorithm develops it returning the data in the orignal (higher dimensional) form. | . The Autoencoder is then a map $ x longmapsto Psi longmapsto x$, where the first part is the encoder and the second is the decoder; thus it is effectively a map of $x$ onto itself. This implies 1) that we can use the data itself $x$ in the loss function and 2) that the algorithm is learning a representation of the dataset itself. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . A smooth multi-dimensional function: rotation curve model . To test an autoencoder network we&#39;ll use a simple rotation curve model that is the superposition of two massive components, a disc and an halo. Both components have circular velocity curves that depend on two parameters, a mass and a physical scale, such that the total model has 4 parameters, 2 for each component. . This is a nice case to test how an autoencoder learns an underlying lower dimensional representation of a dataset, since each rotation curve that we will supply to it is actually derived sampling a smooth function of just 4 parameters. . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . Sampling uniformly in disc mass, between $10^9$ and $10^{12}$ solar masses, and generating random samples of disc size, halo mass, and halo concentration following the above scaling relations. . nsamp = 1000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . Above we have generated our latent representation of the dataset, that is each galaxy model is represented by a quadruple (ms,rd,mh,cc). Below we construct a class to generate the rotation curve of the corresponding galaxy model. . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . We can now initialize this class with the samples (ms,rd,mh,cc) above, which will store inside the class the rotation curves of all the models - defined on the same radial scale (np.logscale(-1, np.log10(50), 50)) by default. . cm=curveMod(ms,rd,mh,cc) . Let&#39;s plot all the rotation curves together: . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The Autoencoder network . We can now build our Autoncoder class deriving from nn.Module. In this example, each rotation curve is a collection of 50 values (see the radial grid in curveMod) and we want to compress it down to a code of just 4 numbers. Notice that we start from the simplified case in which we assume to already know that the ideal latent representation has 4 parameters. . The encoder network has 3 layers, going from the initial $n=50$ to $32$, then to $16$, and finally to $4$. The decoder is symmetric, going from $n=4$ to $16$, then to $32$, and finally to $50$. . class AutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=4) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) def encoder(self, x): return self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))) def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def forward(self, x): encoded = self.encoder(x) decoded = self.decoder(encoded) return decoded . AutoEncoder(len(cm.rad)) . AutoEncoder( (encodeLayer1): Linear(in_features=50, out_features=32, bias=True) (encodeLayer2): Linear(in_features=32, out_features=16, bias=True) (encodeOut): Linear(in_features=16, out_features=4, bias=True) (decodeLayer1): Linear(in_features=4, out_features=16, bias=True) (decodeLayer2): Linear(in_features=16, out_features=32, bias=True) (decodeOut): Linear(in_features=32, out_features=50, bias=True) ) . Data normalization and training/validation split . We now shuffle, normalize, and split the rotation curve dataset into training and validation with a 20% validation split. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . Training loop . We now initialize the training loop, defining a simple MSE loss function and a standard Adam optimizer, and we start training . ae = AutoEncoder(len(cm.rad)) # Adam and MSE Loss loss_func = nn.MSELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.Adam(ae.parameters(), lr=0.01) for epoch in range(1001): ymod = ae.forward(xtrain) loss = loss_func(xtrain, ymod) loss.backward() optimizer.step() optimizer.zero_grad() if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % loss_func(xvalid, ae.forward(xvalid))) . 0 train L:1.04e+00 valid L:9.36e-01 50 train L:2.77e-02 valid L:2.33e-02 100 train L:5.27e-03 valid L:5.16e-03 150 train L:3.13e-03 valid L:3.33e-03 200 train L:2.59e-03 valid L:2.80e-03 250 train L:2.26e-03 valid L:2.12e-03 300 train L:1.59e-03 valid L:1.63e-03 350 train L:1.04e-03 valid L:1.14e-03 400 train L:9.29e-04 valid L:9.83e-04 450 train L:7.87e-04 valid L:8.51e-04 500 train L:9.03e-04 valid L:1.10e-03 550 train L:6.68e-04 valid L:7.73e-04 600 train L:6.68e-04 valid L:7.54e-04 650 train L:7.83e-04 valid L:7.54e-04 700 train L:6.77e-04 valid L:7.19e-04 750 train L:1.35e-03 valid L:1.92e-03 800 train L:4.56e-04 valid L:5.63e-04 850 train L:4.23e-04 valid L:5.46e-04 900 train L:8.74e-04 valid L:1.07e-03 950 train L:3.84e-04 valid L:4.79e-04 1000 train L:4.45e-04 valid L:4.95e-04 . After 1000 epochs of trainig we get down to a low and stable MSE on the validation set. We can now compare the actual rotation curves in the validation set with those decoded by the model, finding an impressively good match! . fig,ax = plt.subplots(figsize=(12,4), ncols=2) for v in datascale(xvalid,xmean,xstd): ax[0].plot(cm.rad, v) for v in datascale(ae.forward(xvalid),xmean,xstd): ax[1].plot(cm.rad, v.detach().numpy()) ax[0].set_xlabel(&#39;radius&#39;); ax[1].set_xlabel(&#39;radius&#39;) ax[0].set_ylabel(&#39;velocity&#39;); . How does the code correlate with the original 4 physical parameters? . Finally we can explore a bit the properties of the 4 values encoded by the autoencoder and try to understand how do they relate to the original 4 physical parameters. Initially we generated each rotation curve starting from a 4-ple in (ms, mh, rd, cc), where these numbers are generated from some well known scaling relations. We plot the distribution of the initial 4 parameters here: . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp = [ms, mh, rd, cc] for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(np.log10(pp[j]), np.log10(pp[i]), s=2) if j==i: ax[i,j].hist(np.log10(pp[i]), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[1,0].set_ylabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[2,0].set_ylabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); ax[3,0].set_ylabel(&#39;log10(cc)&#39;); . Now we can ask ourselves if similar correlations are observed in the 4 parameters coded by the autoencoder. In principle these are some other 4 numbers that fully specify a single rotation curve model, but that do not necessarily have anything to do with the original (ms, mh, rd, cc). . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(pp_ae[:,j], pp_ae[:,i], s=2) if j==i: ax[i,j].hist(pp_ae[:,i].detach().numpy(), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() . We can see that the coded parameters are indeed strongly correlated among themselves, however it is difficult to draw parallelisms with the behaviour we see in the original 4 parameters. An important difference to notice in these plots is that here we do not use a logarithmic scale for the 4 coded parameters, since they also take negative values unlike (ms, mh, rd, cc). . Let&#39;s now have a look at how the 4 coded parameters are correlated with the original ones. This is interesting since while the 4-dimensional latent space found by the autoencoder is not necessarily the original space of (ms, mh, rd, cc), the 4 new parameters might be well correlated with the 4 original physical quantites. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] ith = int(nsamp*(1.0-fval)) mdtrain, mhtrain = mdshuff[:ith], mhshuff[:ith] rdtrain, cctrain = rdshuff[:ith], ccshuff[:ith] mdvalid, mhvalid = mdshuff[ith:], mhshuff[ith:] rdvalid, ccvalid = rdshuff[ith:], ccshuff[ith:] partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) parvalid = (np.vstack([mdvalid, mhvalid, rdvalid, ccvalid]).T) . . Plotting the mutual correalations in the training set we do see that the 4 coded parameters are not at all randomly related to the original physical quantities. . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;=i: ax[i,j].scatter(np.log10(partrain[:,j]), pp_ae[:,i], s=2) if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); . Generating new data with the autoencoder . To finish, let&#39;s have a look at how we can use the autoencoder to generate new fake data that resembles our original dataset. We do so by random sampling from the distribution of coded values that we obtained during training. In this way we generate a new plausible code which we then decode to construct a new rotation curve. . We start by generating new code from the distribution obtained during training - to do this we use numpy.random.choice . size=500 new_pp_ae = [] for i in range(4): new_pp_ae.append(tensor(np.random.choice(pp_ae[:,i].numpy(), size))) new_code = torch.stack(new_pp_ae).T . Let&#39;s plot the original and new distributions of coded parameters: . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(-10,10,50), np.linspace(-1,20,50), np.linspace(-2.5,2.5,50), np.linspace(-5,5,50)] for i in range(4): ax[i].hist(pp_ae[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;new code&#39;); if i==1: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) . Since they look very much alike we can now decode the new code that we just generated and we are ready to plot the new rotation curves . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(ae.decoder(new_code),xmean,xstd): ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With this method we have effectively generated new rotation curves that are realistic and are not part of the training dataset. This illustrates the power of autoencoders, however to do this even better we can adapt our autoencoder to learn the underlying distribution in the code space - this is what Variatioal Autoencoders (VAEs) are for! .",
            "url": "https://lposti.github.io/MLPages/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves.html",
            "relUrl": "/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Ground-up construction of a simple neural network",
            "content": "Linear layers and activation functions from scratch . When approaching the study of a new subject I find it extremely useful to get my hands dirty and play around with the stuff I&#39;m learning, in order to cement the knowledge that I&#39;m passively acquiring reading or listening to a lecture. In the case of deep learning, before starting to use massively the superb python libraries available, e.g. pytorch or fast.ai, I think it&#39;s critical to build a simple NN from scratch. . The bits required are just linear operations, e.g. matrix multiplications, functional composition and the chain rule to get the derivatives during back-propagation. All of this sounds not terrible at all, so we just need a bit of organization to glue all the pieces together. . We take inspiration from the pytorch library and we start by building an abstract Module class. . import numpy as np from torch import tensor from torch import nn import torch, math import random %config Completer.use_jedi = False rng = np.random.default_rng() . . class Module(): &quot;&quot;&quot; abstract class: on call it saves the input and output, and it returns the output &quot;&quot;&quot; def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . When called, Module stores the input and the output items and just returns the output which is defined by the method forward, which needs to be overridden by the derived class. Another method, backward, will have to return the derivative of the function, thus implementing the necessary step for back-propagation. . Let&#39;s now use the class Module to implement a sigmoid activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) class Sigmoid(Module): def forward(self, inp): return sig(inp) def bwd(self, out, inp): inp.g = sig(inp) * (1-sig(inp)) * out.g . Here the class Sigmoid inherits from Module and we just need to specify the forward method, which is just the value of the sigmoid function, and the bwd method, which is what is called by backward. We use bwd to implement the derivative of the sigmoid $$ sigma&#39;(x) = sigma(x) left[1- sigma(x) right], $$ which we store in the .g attribute, that stands for gradient, of the input. This storing the gradient of the class in the .g attribute of the input combined with the last multiplication by out.g that we do in the bwd method is basically the chain rule. The gradient in each layer of an NN is, according to the chain rule, the derivative of the layer times the derivative of the input. Once computed, we store this in the gradient of inp, which is exactly the same variable as out of the previous layer, thus we can reference its gradient with out.g when climbing back the hierarchy of layers. . Similarly, a linear layer $W{ bf x} + b$, where $w$ is a matrix, ${ bf x}$ is a vector and $b$ is a scalar, can be written as: . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) . As before, forward implements the linear layer (@ is the matrix multiplication operator in pytorch) and bwd implements the gradient. The derivative of a matrix multiplication $W{ bf x}$ is just a matrix multiplication by the transpose of the matrix, $W^T$. Since the linear layer has the weights $w$ and bias $b$ parameters that we want to learn, then we need to calculate the gradient of the output of the layer with respect to the weights and the bias. This is what is implemented in self.w.g and self.b.g. . Finally we can define the loss as a class derived from Module as: . class Mse(Module): def forward (self, inp, target): return (inp.squeeze(-1) - target).pow(2).mean() def bwd(self, out, inp, target): inp.g = 2*(inp.squeeze(-1)-target).unsqueeze(-1) / target.shape[0] . This is a mean squared error loss function, $L({ bf y},{ bf y}_{ rm target}) = sum_i (y_i-y_{i, rm target})^2$, where the forward and bwd methods have the same meaning as above. Notice that here the bwd method just stores the inp.g attribute and does not have a multiplication by out.g, because this is the final layer of our NN. . Finally we can bundle everything together in a Model class which takes as input a list of layers and implements a forward method, where maps the input into each layer sequentially, and a backward method, where it goes through the gradient of each layer in reversed order. . class Model(): def __init__(self, layers): self.layers = layers self.loss = Mse() def __call__(self, x, target): return self.forward(x, target) def forward(self, x, target): for l in self.layers: x = l(x) return self.loss(x, target) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Let&#39;s now take some fake data and let&#39;s randomly initialize the weights and biases (unsing standard Xavier initialization so that the output of the layers are still a null mean and unit variance) . n,m = 200,1 x = torch.randn(n,m) y = x.pow(2) nh = 100 # standard xavier init w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . We can now define a model as a sequence of linear and activation layers and we can make a forward pass to calculate the loss... . model = Model([Lin(w1,b1), Sigmoid(), Lin(w2,b2)]) loss = model(x, y) . ...and also a backward pass to calculate the gradients . model.backward() . The architecture above is basically equivalent to an nn.Sequential model . nn.Sequential(nn.Linear(m,nh), nn.Sigmoid(), nn.Linear(nh,1)) . Sequential( (0): Linear(in_features=1, out_features=100, bias=True) (1): Sigmoid() (2): Linear(in_features=100, out_features=1, bias=True) ) .",
            "url": "https://lposti.github.io/MLPages/neural_network/basics/jupyter/2022/03/22/NN-from-scratch.html",
            "relUrl": "/neural_network/basics/jupyter/2022/03/22/NN-from-scratch.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding how basic linear NNs handle non-linearities",
            "content": "How simple neurons handle non-linearities . The Universal approximation Theorem . While studying deep learning, coming from a mathematical physics background, I struggled a lot in understanding how a neural network (NN) can fit non-linear functions. Most of the visual explanations or tutorials I could find on NNs did not give me a satisfactory explanation as to how a composition of linear functions - i.e. an NN - can handle non-linear behaviour. So I looked for more formal derivations, as I knew that the mathematical foundations of deep learning had to be sound. . Eventually I came across one of the most important papers in this field: Cybenko (1989, doi:10.1007/BF02551274). Reading this paper opened my eyes and gave me a completely different perspective on the problem. I realized that the key to the universal approximation theorem is that the composition of a linear function and a sigmoidal (so-called activation) function yields a series of functions which is dense in the space of continuous functions. . In other words, any continuous function can be written as a finite sum of terms given by the composition of a linear and a sigmoidal function, i.e. $$ sum_{i=0}^N alpha_i , sigma({ bf w}_i cdot{ bf x} + b_i), $$ with $ sigma: mathrm{R} to mathrm{R}$ being a sigmoidal activation function, ${ bf x} in mathrm{R}^n$ and ${ bf w}_i in mathrm{R}^n$, $ alpha_i, ,b_i in mathrm{R}$ $ forall i$. Cybenko (1989) showed that the set of functions above spans the whole space of continuous functions in $ mathrm{R}^n$, effectively making this set kind of a basis for $ mathrm{R}^n$, except that the functions are not linearly independent. . Elements of this set of function as in the equation above are usually called units or neurons. . Where does the non-linear behaviour come from . Since a neuron is a composition of a linear function with an activation function, the key to approximate non-linear functions is in the sigmoidal activation function. Formally a sigmoidal is a function $ sigma: mathrm{R} to mathrm{R}$ such that $ lim_{x to+ infty} sigma(x)=1$ and $ lim_{x to- infty} sigma(x)=0$. The Heaviside function is an example of one of the simplest sigmoidal functions; however that is not continuous near $x=0$, thus in practice smooth approximations of it are often used. A popular one is: $$ sigma(x)= frac{1}{1+e^{-x}} $$ . But how can a linear combination of lines composed with a step function approximate any non-linear behaviour? I knew from Cybenko&#39;s results that this had to be the case, so I set out and tried to understand and see this better. . Part 1: How many neurons are needed to approximate a given non-linearity? . I asked myself how many neurons (i.e. elements of Cybenko&#39;s quasi-basis) would I need to approximate a simple second-order non-linearity, i.e. the function $x mapsto x^2$. . import torch import numpy as np import matplotlib.pyplot as plt import torch.nn as nn device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; device = torch.device(device) %matplotlib inline %config Completer.use_jedi = False . . Let&#39;s generate the data of a nice parabolic curve with some small random noise and let&#39;s plot them . size = 500 x = torch.linspace(-5, 5, size) y = x.pow(2) + 0.5 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131837ba8&gt;] . In order to build some intuition of how we may approximate this data with a linear combination of neurons let&#39;s experiment a bit and overplot a few simple combinations of neurons by hand. . First, we need to define our sigmoidal activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) . fig,ax = plt.subplots(figsize=(15,4), ncols=3) def commons(ax): ax.plot(x,y,&#39;k.&#39;, zorder=0) ax.legend(loc=&#39;upper center&#39;) ax.set_ylim((None, 30)) ax[0].plot(x,20*sig(x), lw=3, label=r&quot;$ rm 20 sigma(x)$&quot;) ax[0].plot(x,20*sig(-x), lw=3, label=r&quot;$ rm 20 sigma(-x)$&quot;) ax[0].plot(x,20*(sig(x)+sig(-x)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x)+ sigma(-x)]$&quot;) commons(ax[0]) ax[1].plot(x,20*sig(x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*sig(-x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*(sig(x-3)+sig(-x-3)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x-3)+ sigma(-x-3)]$&quot;) commons(ax[1]) ax[2].plot(x,25*(sig(1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax[2].plot(x,25*(sig(-1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax[2].plot(x,25*(sig(1.2*x-4)+sig(-1.2*x-4)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax[2]) . These examples help in building up some intuition on how we can use these sigmoidal building blocks to approximate relatively simple non linear behaviours. In this specific case of a convex second-order non-linearity, the sum of two scaled, shifted and mirrored step functions seems to yield a decent representation of the data close to the origin. . Dependence on the shape of the activation function . Naturally, this is strongly dependent on the particular shape of the sigmoidal function that we chose, i.e. $ sigma: x mapsto (1+e^{-x})^{-1}$. If we had chosen a Heaviside function instead, the sum of the two neurons above would not have yielded a similarly good approximation of the data. . fig,ax=plt.subplots() ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(-1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)+np.heaviside(-1.2*x-4,0.5)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax) . . NN fitting of $x^2$ function . Now that we have some intuition on the linear combinations of neurons let&#39;s try to answer the question posed at the beginning of Part 1, that is how many neurons are needed to approximate $x^2$. . To answer this we will build a series of simple torch models made up of just one (hidden) layer of neurons, i.e. . nn.Sequential(nn.Linear(1, N_units), nn.Sigmoid(), nn.Linear(N_units, 1)) . We start by defining a learning rate and a number of epochs; then we loop through the 5 numbers of neurons explored, [1,2,4,8,16], we set up the nn.Sequential model and we start the full training loop on the data. We put all of this into a convenient class with a run method and a plots method, which we use to visualize the output. . class ModelsTestingActivations(): def __init__(self, activation_fn=nn.Sigmoid(), loss_fn=nn.MSELoss(reduction=&#39;sum&#39;), units=[1,2,4,8,16], learning_rate=3e-2, num_epochs=1000): self.activ_fn, self.loss_fn = activation_fn, loss_fn self.units, self.lr, self.num_epochs = units, learning_rate, num_epochs # outputs self.models, self.preds = [], [] def make_model(self, u): return nn.Sequential(nn.Linear(in_features=1, out_features=u, bias=True), self.activ_fn, nn.Linear(in_features=u, out_features=1) ) def plots(self, residuals=True, xextrap=10): if not hasattr(self, &#39;x&#39;): print (&#39;Have you run the model yet?&#39;) return fig,ax = plt.subplots(figsize=(18,3.2), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$y$&#39;) ax[i].plot(self.x,self.y,&#39;k.&#39;) ax[i].plot(self.x,self.preds[i],&#39;r.&#39;) ax[i].plot(np.linspace(-xextrap,xextrap), self.models[i](torch.linspace(-xextrap,xextrap,50).unsqueeze(1)).detach(), &#39;b--&#39;) ax[i].text(0.05,0.05,r&quot;N=%d&quot; % self.units[i], transform=ax[i].transAxes, fontsize=14) # residuals if not residuals: return fig,ax = plt.subplots(figsize=(18,1.6), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$ Delta y$&#39;) ax[i].plot(self.x,(self.y-self.preds[i]).abs(), &#39;k-&#39;, lw=0.5) ax[i].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (self.y-self.preds[i]).abs().mean(), transform=ax[i].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;) def run(self, x, y): self.x, self.y = x, y for i,u in enumerate(self.units): # define model model = self.make_model(u) self.models.append(model) # define optimizer optimizer = torch.optim.Adam(model.parameters(), lr=self.lr) # fitting loop for epoch in range(self.num_epochs): # reinitialize gradient of the model weights optimizer.zero_grad() # prediction &amp; loss y_pred = model(self.x.unsqueeze(-1)) loss = self.loss_fn(y_pred, self.y.unsqueeze(-1)) # backpropagation loss.backward() # weight update optimizer.step() self.preds.append(y_pred.squeeze().detach()) . . It is interesting to see how the results of this series of models change depending on which activation function is used, thus we can make our custom class to have as input the shape of the activation function. . We start with the classic sigmoidal . mods_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_sigmoid.run(x, y) . mods_sigmoid.plots() . These plots show how the NN model (red) compares to the actual data used for training (black) as a function of the number (N) of neurons used in the linear layer. Moving from the panels left to right the number of neurons increases from $N=1$ to $N=6$. The dashed blue line is the prediction of the NN model, which we also extrapolated in the range $x in[-10,10]$ outside of the domain of the data, that are confined within $[-5,5]$. The rows of panels below show the residuals $ Delta y=|y-y_{ rm NN}|$, i.e. abs(black-red), while $ langle Delta y rangle$ is the mean of the residuals. . There are a few interesting things that we can notice: . when using only one neuron the NN is able only to capture the mean of the data $ langle y rangle approx 8.613$ | with N=2 neurons, the linear combinations of the two activations is already able to represent the convex 2nd-order non-linearity of the data, reducing the mean residual by an order of magnitude with respect to the model just predicting the mean (i.e. that with N=1). | obviously, increasing N results in a better approximation to the training data, for a fixed learning rate and number of training epochs, up to a residual 4x better with N=6 than with N=2. | the extrapolations of the NN models outside of the domain of the data do not follow the $x^2$ curve at all, showing that the NN models have successfully learned to reproduce the data, but have not learned completely the behaviour of the underlying curve. This exemplifies that NN models are often poor predictors outside of the domain of the training data | . We can have a closer look at the parameters obtained by the NN model with 2 neurons and see how do they compare to the simple fit by eye that we did above. To do this we can grab the named_parameters of the model with $N=2$ and print them out . for name, param in mods_sigmoid.models[1].named_parameters(): if param.requires_grad: print (name, param.data) . 0.weight tensor([[-1.4160], [ 1.4624]]) 0.bias tensor([-4.9017, -4.9822]) 2.weight tensor([[23.6837, 22.9339]]) 2.bias tensor([0.9818]) . Let&#39;s print out explicitly the function found with $N=2$ neurons . def string_func_mod(x): return (&quot;%1.0f sig(%1.1f*x %1.1f) + %1.0f sig(%1.1f*x %1.1f) + %1.1f&quot; % (x[2], x[0], x[1], x[5], x[3], x[4], x[6])) print(string_func_mod(mod2_sigmoid)) . . 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 . Really similar to the parameters we obained by eye! (results may vary a bit due to randomness) . Fit with different activation functions . We can also repeat this exercise with different activation functions, e.g. with a softsign . mods_softsign = ModelsTestingActivations(activation_fn=nn.Softsign(), units=[1,2,3,4,6], learning_rate=9e-2, num_epochs=1000) mods_softsign.run(x, y) . mods_softsign.plots() . or with a ReLU (however, ReLU is not sigmoidal in the sense of Cybenko, so strictly speaking their universal approximation theorem does not apply. It has been shown that the hypothesis on $ sigma$ can be relaxed to it being non-constant, bounded and piecewise continuous, see e.g. Hornik 1991, Leshno et al. 1993) . mods_relu = ModelsTestingActivations(activation_fn=nn.ReLU(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_relu.run(x, y) . mods_relu.plots() . While the accuracy of the NN with ReLU, as measured by $ langle Delta y rangle$, is not significantly better than with Sigmoid for the sake of this experiment, the extrapolation out-of-domain is much better. This is because the NN model with ReLU tends to linear outside of the data domain, while the NN model with Sigmoid is approximately constant outside of the range of the training data. . Part 2: How does this compares with a Bayesian likelihood estimator . It would be interesting to fully reconstruct the likelihood distribution of the parameters of the NN model. If the number of parameters is not huge - that is if we are working with a limited number of neurons in a single layer - then an exploration of the multi-dimensional likelihood distribution is still feasible. Moreover, if we are able to map the full likelihood of the model parameters we can also see where the best model found by the NN sits in the space of the likelihood. . To do this we can use a Monte Carlo Markov Chain (MCMC) analysis. This is actually what I would normally do when facing an optimization problem in physics, as often one can have a pretty good guess on a suitable functional form to use for the fitting function and, more importantly, this method naturally allows to study the uncertainties on the model found. . We&#39;re using the library emcee (paper) to run the MCMC analysis and corner (paper) to plot the posterior distribution. . import emcee import corner # the functional definition of the NN model def lmod(x, pars): &quot;&quot;&quot; A linear combination of nu sigmoidals composed with linear functions &quot;&quot;&quot; nu = int((len(pars)-1)/3) # number of neurons, with 2 biases res = pars[-1] for i in range(nu): res += sig(pars[0+i*3]*x+pars[1+i*3])*pars[2+i*3] return res # log-likelihood def lnlike(pars, x, y): &quot;&quot;&quot; This is equivalent to MSELoss(reduction=&#39;sum&#39;) &quot;&quot;&quot; y_lmod = lmod(x,pars) return -((y-y_lmod).pow(2).sum() / len(y)).item() # log-prior on the parameters def lnprior(pars): &quot;&quot;&quot; A multi-dimensional Gaussian prior with null mean, fixed std and null correlation &quot;&quot;&quot; std = 30 lp = 0. for p in pars: lp += -0.5*(p)**2/std**2-np.log(np.sqrt(2*np.pi)*std) return lp # log-probability = log-likelihood + log-prior def lnprob(pars, x, y): lk, lp = lnlike(pars, x, y), lnprior(pars) if not np.isfinite(lp) or not np.isfinite(lk): return -np.inf return lp + lk . . Let&#39;s run the MCMC analysis for a $N=2$ NN (see here for a bit more context on MCMCs with emcee) . nunits = 2 dim = 3*nunits+1 . %%time nu, nw, nstep = 2, 4*dim, 10000 # initial conditions of each chain pos = [[0]*dim + 1e-4*np.random.randn(dim) for j in range(nw)] # launch the MCMC sampler = emcee.EnsembleSampler(nw, dim, lnprob, args=(x.squeeze(), y.squeeze())) sampler.run_mcmc(pos, nstep); # collate the chains of each walker and remove the first 500 steps - the burn-in phase samples = sampler.chain[:,500:,:].reshape((-1, dim)) . /Users/lposti/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp &#34;&#34;&#34;Entry point for launching an IPython kernel. . CPU times: user 1min 23s, sys: 988 ms, total: 1min 24s Wall time: 1min 38s . Finally, let&#39;s plot the posterior probability distribution of the 7 parameters of a NN model with $N=2$ neurons and let&#39;s also mark the location of the model we obtained above with nn.Sequential . corner.corner(samples, bins=30, smooth=1.5, smooth1d=1.5, truths=mod2_sigmoid); . Find the maximum probability model, i.e. the model with highest log-prob . idmax = np.unravel_index(sampler.lnprobability.argmax(), sampler.lnprobability.shape) # maximum probability max_prob = sampler.chain[idmax[0],idmax[1],:] . print (&#39;NN model:&#39;) print (string_func_mod(mod2_sigmoid), &#39; LOSS:%1.2f&#39; % lnlike(mod2_sigmoid, x, y)) print () print (&#39;MCMC model&#39;) print (string_func_mod(max_prob), &#39; LOSS:%1.2f&#39; % lnlike(max_prob, x, y)) . NN model: 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 LOSS:-0.73 MCMC model 28 sig(-1.2*x -4.3) + 31 sig(1.0*x -3.9) + -0.2 LOSS:-0.31 . and we can also plot them side by side in comparison to the data . fig,ax = plt.subplots(figsize=(9,4.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) ax.plot(x,y,&#39;ko&#39;) ax.set_ylim((-1,32)) commons(ax[0]) ax[0].set_title(&quot;NN best model&quot;, fontsize=16) ax[0].plot(x, lmod(x, mod2_sigmoid), &#39;r.&#39;) ax[0].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), mod2_sigmoid), &#39;b--&#39;) commons(ax[1]) ax[1].set_title(&quot;MCMC max prob&quot;, fontsize=16) ax[1].plot(x, lmod(x, max_prob), &#39;r.&#39;) ax[1].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), max_prob), &#39;b--&#39;) fig,ax = plt.subplots(figsize=(9,2.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$ Delta y$&#39;) ax.set_ylim((-0.1,3)) commons(ax[0]) ax[0].plot(x, (y-lmod(x, mod2_sigmoid)).abs(), &#39;k-&#39;, lw=0.5) ax[0].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, mod2_sigmoid)).abs().mean(), transform=ax[0].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); commons(ax[1]) ax[1].plot(x, (y-lmod(x, max_prob)).abs(), &#39;k-&#39;, lw=0.5) ax[1].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, max_prob)).abs().mean(), transform=ax[1].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); . Part 3: Dealing with more complex non-linearities . Let&#39;s repeat the analysis of Part 1 but for a more complex non-linear function, for instance a sin function with an oscillating non-linear behaviour. Let&#39;s now ask ourselves how many neurons would you need to fit this function. . As before, let&#39;s start by generating some data . size = 1000 x = torch.linspace(-10, 10, size) y = torch.sin(x) + 0.2 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131f3c4a8&gt;] . Now we can use the same ModelsTestingActivations class that we wrote above, just passing the new xs and ys to the run method. Let&#39;s use a Sigmoid activation function and let&#39;s have a look at the performance of the NN models for increasing number of neurons $N$ . mods_sin_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=4e-2, num_epochs=2000) mods_sin_sigmoid.run(x, y) . mods_sin_sigmoid.plots(xextrap=20) . From these plots we can clearly notice a few important things: . the number of neurons limits the number turnovers (i.e. changes of sign of the derivative) of the function that can be fitted | an NN model with $N$ neurons is generally limited to approximate decently only function that change their increasing/decreasing tendency $N$ times | in this particular example, the sin function turnsover 6 times in the interval $[-10,10]$, thus an NN with at least $N=6$ neurons is needed to capture all the times the data turnover | also in this case, the extrapolation of the NN models outside of the domain of the data yield poor predictions. This means that the NN has learned to reproduce the data, but has not learned the underlying functional behaivour | .",
            "url": "https://lposti.github.io/MLPages/neural_network/basics/bayesian/mcmc/jupyter/2022/03/18/neurons-non-linearities.html",
            "relUrl": "/neural_network/basics/bayesian/mcmc/jupyter/2022/03/18/neurons-non-linearities.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lposti.github.io/MLPages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lposti.github.io/MLPages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Lorenzo Posti, currently a postdoctoral researcher in Astrophysics at the Observatory of Strasbourg (France). Before that, I&#39;ve been working on research in Astrophysics in Groningen (Netherlands), Bologna (Italy), Oxford (UK) and Baltimore (USA). . You can find my updated list of publications on ADS. I used to maintain a personal website with some details on my research, however I stopped doing that for various reasons. An archived and stripped-down version can be found here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lposti.github.io/MLPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lposti.github.io/MLPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}