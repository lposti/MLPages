{
  
    
        "post0": {
            "title": "`vcdisk`: a python package for galaxy rotation curves",
            "content": "What is vcdisk? . vcdisk is a handy toolbox which implements several essential routines for anyone working on galactic dynamics and, in particular, with galaxy rotation curves. vcdisk provides fast and efficient solutions of Poisson&#39;s equation (using results from Casertano 1983, Cuddeford 1993, Noordermeer 2008 and Binney &amp; Tremaine 2008) in the context of thick galaxy disks and flattened oblate bulges of any observed surface density profile. . It is a new minimial python package that I wrote and it can be found at https://github.com/lposti/vcdisk. This notebook reproduces the introductory &quot;How to use vcdisk&quot; tutorial which can be found in the documentation. . Why do we need vcdisk? . The most typical use case for vcdisk is in rotation curve modelling. Specifically, this module allows to compute the contribution to the circular velocity of the observed baryonic components of a disk galaxy, e.g. the stellar disk, the gas disk, the stellar bulge. . If we have some kinematical observations that allowed us to measure the rotational velocities of some material in circular orbits in the disk of a spiral galaxy (e.g. cold gas), the observed rotation curve can be decomposed into several dynamically important components in the galaxy where $V_{ rm DM}$ is the contribution from dark matter, $V_{ star, rm disk}$ from the stellar disk, $V_{ rm gas, disk}$ from the gas disk etc. While the left-hand side of this equation usually comes from observations, and the contribution of DM is the unknown that is often what we want to constrain, for all the other terms there is vcdisk! . vcdisk calculates the contribution to the gravitational field of an axisymmetric component whose radial surface density distribution is known. For instance, the surface density profiles of stellar ($ Sigma_{ star, rm disk}$) and gaseous disks ($ Sigma_{ star, rm disk}$) can be directly obtained from galaxy images in appropriate wavebands and these can then be used as input for vcdisk.vcdisk to calculate $V_{ rm disk}(R, Sigma_{ rm disk}(R))$. . The vcdisk module provides appropriate calculations for both disky (vcdisk.vcdisk) and spheroidal (vcdisk.vcbulge) galaxy components. . Example 1: analytic surface density . Import vcdisk . import vcdisk import numpy as np import matplotlib.pylab as plt # suppressing warnings in the scipy.integrate.quad calls in vcbulge import warnings from scipy.integrate import IntegrationWarning warnings.filterwarnings(&quot;ignore&quot;, category=IntegrationWarning) . Exponential thick disk . Let&#39;s start with the case of a thick disk with a classical exponential surface density. . md, rd = 1e10, 2.0 # mass, scalelength of the disk r = np.linspace(0.1, 30.0, 100) # radii samples def expdisk_sb(r, md, rd): # exponential disk surface density return md / (2*np.pi*rd**2) * np.exp(-r/rd) sb_exp = expdisk_sb(r, md, rd) # run vcdisk vdisk = vcdisk.vcdisk(r, sb_exp) . def plot_sb_vdisk(ax, r, sb, vdisk, label=None): ax[0].plot(r, np.log10(sb), lw=2) ax[0].set_xlabel(&quot;radius / kpc&quot;); ax[0].set_ylabel(&quot;log surface density / M_sun kpc&quot;+&quot;$^{2}$&quot;); ax[1].plot(r, vdisk, lw=2, label=label) ax[1].set_xlabel(&quot;radius / kpc&quot;); ax[1].set_ylabel(&quot;velocity / km s&quot;+&quot;$^{-1}$&quot;); if label is not None: ax[1].legend() fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sb_exp, vdisk) . We can explore how does $V_{ rm disk}$ change when changing the scaleheight of the disk $z_0$ for instance: . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sb_exp, vdisk, label=&#39;z0=0.3 kpc&#39;) plot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, z0=0.1), label=&#39;z0=0.1 kpc&#39;) plot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, z0=0.8), label=&#39;z0=0.8 kpc&#39;) . Sersic profile . Let&#39;s compare to other popular analytic surface density profiles. Probably the most used in Astronomy is the Sersic (1968) profile, which is often used to describe all sorts of stellar components, including disks and bulges. We can use the class sersic in the vcdisk package to get Sersic profiles . sers_n1 = vcdisk.sersic(md, 1.678*rd, 1.0) sers_n2 = vcdisk.sersic(md, 1.678*rd, 2.0) sers_n05 = vcdisk.sersic(md, 1.678*rd, 0.5) sers_n4 = vcdisk.sersic(md, 1.678*rd, 4.0) . Here, we have taken the reference model sers_n1 which is equivalent to the exponential disk above, since $R_e simeq 1.678 R_{ rm d}$ for $n=1$ exponential profiles. Let&#39;s now compare the circular velocities of disks with Sersic surface brightnesses with different index $n$, while keeping the total luminosity fixed. . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sers_n1(r), vcdisk.vcdisk(r, sers_n1(r)), label=&#39;Sersic: n=1&#39;) plot_sb_vdisk(ax, r, sers_n05(r), vcdisk.vcdisk(r, sers_n05(r)), label=&#39;Sersic: n=0.5&#39;) plot_sb_vdisk(ax, r, sers_n2(r), vcdisk.vcdisk(r, sers_n2(r)), label=&#39;Sersic: n=2&#39;) plot_sb_vdisk(ax, r, sers_n4(r), vcdisk.vcdisk(r, sers_n4(r)), label=&#39;Sersic: n=4&#39;) ax[0].set_ylim(2,10); ax[1].set_ylim(-1, 105); . Sersic bulges . We have just computed the circular velocities of thick galaxy disks whose surface brightness profile is of the Sersic form with different $n$. However, we may also be interested in the circular velocity of a spheroidal component, like a stellar bulge, whose observed surface brightness is well approximated by a Sersic law. . If for disky components we use vcdisk, for spheroidal components we use vcbulge! . Let&#39;s now compute the circular velocity on the mid-plane of a nearly spherical Sersic bulges with different $n$. . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sers_n1(r), vcdisk.vcbulge_sersic(r, md, 1.678*rd, 1.0), label=&#39;Sersic: n=1&#39;) plot_sb_vdisk(ax, r, sers_n05(r), vcdisk.vcbulge_sersic(r, md, 1.678*rd, 0.5), label=&#39;Sersic: n=0.5&#39;) plot_sb_vdisk(ax, r, sers_n2(r), vcdisk.vcbulge_sersic(r, md, 1.678*rd, 2.0), label=&#39;Sersic: n=2&#39;) plot_sb_vdisk(ax, r, sers_n4(r), vcdisk.vcbulge_sersic(r, md, 1.678*rd, 4.0), label=&#39;Sersic: n=4&#39;) ax[0].set_ylim(2,10); ax[1].set_ylim(-1, 105); . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/vcdisk.py:925: RuntimeWarning: invalid value encountered in double_scalars return -self.Ie*self.bn/(self.n*self.re) * np.exp(-self.bn*((R/self.re)**(1/self.n)-1)) * (R/self.re)**(1/self.n-1.0) . Flattened Sersic bulges . We can compute the circular velocity also for flattened oblate bulges, thus for components whose 3D density is stratified as $ rho= rho(m)$ where $m^2=R^2+z^2/q^2$ with axis ratio $0&lt;q leq 1$. . Let&#39;s take the same baseline model as in Fig. 2 (top central panel) of Noordermeer (2008): . mb, re = 5e9, 1.0 # mass, effective radius of the bulge rb = np.logspace(-2, 1, 100) # radii samples sers_bulge = vcdisk.sersic(mb, re, 1.0) sb_sers_bulge = sers_bulge(rb) . and we can plot $V_{ rm bulge}$ for different axis ratios $q$ at a fixed mass, effective radius and Sersic index. For comparison, the black dotted line is for a point mass with the same mass as the bulge. . %%time plt.plot(rb, np.sqrt(4.301e-6*5e9/rb),&#39;k:&#39;) plt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge), c=&#39;C0&#39;, label=&#39;q=0.99&#39;) plt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.8), c=&#39;C1&#39;, label=&#39;q=0.8&#39;) plt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.6), c=&#39;C2&#39;, label=&#39;q=0.6&#39;) plt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.4), c=&#39;C3&#39;, label=&#39;q=0.4&#39;) plt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.2), c=&#39;C4&#39;, label=&#39;q=0.2&#39;) plt.legend() plt.ylim(0,160); plt.xlim(0,10); plt.xlabel(&quot;radius / kpc&quot;); plt.ylabel(&quot;velocity / km s&quot;+&quot;$^{-1}$&quot;); . CPU times: user 52 s, sys: 541 ms, total: 52.5 s Wall time: 54.5 s . Text(0, 0.5, &#39;velocity / km s$^{-1}$&#39;) . User-defined vertical density profile . vcdisk allows us to specify our own vertical density profile, if we so choose. This can be easily done through the rhoz argument. . def rhoz_gauss(z, m, std): return np.exp(-0.5*(z-m)**2/std**2) / np.sqrt(2*np.pi*std**2) def plot_rhoz(ax, z, rhoz, label=None): ax.plot(z, rhoz, lw=2, label=label) ax.legend() ax.set_xlabel(&quot;radius / kpc&quot;); ax.set_ylabel(&quot;vertical density / M_sun kpc&quot;+&quot;$^{2}$&quot;); fig, ax = plt.subplots(figsize=(6,4)) z = np.linspace(0,3) plot_rhoz(ax, z, np.exp(-z/0.3) / (2*0.3), label=&#39;exp&#39;) plot_rhoz(ax, z, rhoz_gauss(z, 0., 0.8), label=&#39;gauss&#39;) . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sb_exp, vdisk) plot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, rhoz=rhoz_gauss, rhoz_args={&#39;m&#39;:0.0, &#39;std&#39;:0.8})) . Flaring disk . We can also have a flaring disk, i.e. one where the vertical density depends also on radius $ rho_z= rho_z(z,R)$. We can work out this case as well with vcdisk by specifying how $ rho_z$ depends on $R$. . def rhoz_exp_flaring(z, R, z00, Rs): z0R = z00+np.arcsinh(R**2/Rs**2) return np.exp(-z/z0R) / (2*z0R) fig, ax = plt.subplots(figsize=(6,4)) z = np.linspace(0,3) plot_rhoz(ax, z, rhoz_exp_flaring(z, 0.0, 0.1, 0.8), label=&#39;R=0&#39;) plot_rhoz(ax, z, rhoz_exp_flaring(z, 0.5, 0.1, 0.8), label=&#39;R=0.5&#39;) plot_rhoz(ax, z, rhoz_exp_flaring(z, 1.0, 0.1, 0.8), label=&#39;R=1&#39;) plot_rhoz(ax, z, rhoz_exp_flaring(z, 1.5, 0.1, 0.8), label=&#39;R=1.5&#39;) plt.ylim(None,2); . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, r, sb_exp, vdisk) plot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, rhoz=rhoz_exp_flaring, rhoz_args={&#39;z00&#39;:0.1, &#39;Rs&#39;:0.8}, flaring=True)) . Example 2: observed surface brightness . NGC2403: a late (Sc) bulgeless disk galaxy . Let&#39;s say that, instead of having just simple galaxy images from which we can derive an analytic approximation for the galaxy surface brightness, we have a detailed galaxy image from which we can measure the intensity in elliptical annuli. This is for instance the case of nearby galaxies such as NGC 2403, that I use here as an example. . I use data taken from the SPARC database for this galaxy, which I include in this package as the NGC2403_rotmod.dat file. Here are reported measurements of the intensity at 3.6$ mu$m taken with the SPITZER space telescope in elliptical annuli for this target. . rad, _, _, _, _, _, sb_n2403, _ = np.genfromtxt(&#39;NGC2403_rotmod.dat&#39;, unpack=True) # convert SB to Msun / kpc^2 sb_n2403 *= 1e6 . Now we can very easily use vcdisk to calculate $V_{ star, rm disk}$ for this galaxy, and we can evena compare it to some analytic profiles. . vd_n2403 = vcdisk.vcdisk(rad, sb_n2403, z0=0.4)# z0=0.4 kpc from Fraternali et al. (2002): https://ui.adsabs.harvard.edu/abs/2002AJ....123.3124F sb_exp_n2403 = expdisk_sb(rad, 1.004e10, 1.39) # numbers taken from http://astroweb.cwru.edu/SPARC/ fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, rad, sb_exp_n2403, vcdisk.vcdisk(rad, sb_exp_n2403), label=&#39;exp-disk&#39;) plot_sb_vdisk(ax, rad, sb_n2403, vd_n2403, label=&#39;observed&#39;) . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10 . The $V_{ star, rm disk}$ curve obtained with vcdisk directly from the ellipse photometry of NGC 2403 is much more structured between $1-5 rm ,kpc$, which is where the surface brightness has dips and peaks. Capturing this rich structure is essential to rotation curve modelling. . UGC 2953: an early (Sa) spiral galaxy . Let&#39;s take a different case now of an early spiral galaxy, i.e. a disk galaxy with redder colors and a prominent central bulge: UGC 2953. . I use again the data taken from the SPARC database and I include them here as the UGC02953_rotmod.dat file. There are measurements of the near-infrared intensities of both the disk and bulge components separated. . rad, _, _, _, _, _, sbd_u2953, sbb_u2953 = np.genfromtxt(&#39;UGC02953_rotmod.dat&#39;, unpack=True) # convert SB to Msun / kpc^2 sbd_u2953 *= 1e6 sbb_u2953 *= 1e6 . With the two different surface brightness arrays, we can use vcdisk and vcbulge to get the corresponding circular velocities. . vd_u2953 = vcdisk.vcdisk(rad, sbd_u2953, z0=0.6) vb_u2953 = vcdisk.vcbulge(rad, sbb_u2953) . And we can finally plot them together. . fig,ax = plt.subplots(figsize=(12,4), ncols=2) plot_sb_vdisk(ax, rad, sbd_u2953, vd_u2953, label=&#39;disk&#39;) plot_sb_vdisk(ax, rad, sbb_u2953, vb_u2953, label=&#39;bulge&#39;) plot_sb_vdisk(ax, rad, sbd_u2953+sbb_u2953, np.sqrt(vd_u2953**2+vb_u2953**2), label=&#39;total&#39;) . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10 /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10 /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10 .",
            "url": "https://lposti.github.io/MLPages/jupyter/vcdisk/2022/12/08/vcdisk.html",
            "relUrl": "/jupyter/vcdisk/2022/12/08/vcdisk.html",
            "date": " • Dec 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "A simple neural network to predict galaxy rotation curves from photometry",
            "content": "Predicting rotation curves just from photometric observations . An interesting, and still open, question in modern Astronomy is how the properties of a galaxy, such as its luminosity, size, and stellar/gas content, shape the gravitational field in which the galaxy is immersed. In the case of disk galaxies, orbits are close to circular, thus measuring the rotational velocity of stars or gas in the disk gives a direct probe of the total gravitational potential. For the galaxies for which we are able to obtain such rotation curves with optical or radio telescopes, we can try to establish a link between the shape of the circular velocity curve, i.e. of the total gravitational potential, and with the distribution of luminous baryons, stars and gas. . It is very well-known that the gravitational field generated by the luminous baryons is not enough to account for the observed rotation curves -- the so-called missing mass problem. This could be either because there is an additional invisible matter component to be considered in the potential budget (dark matter) or because the laws of gravity as we know them need revision in the context of disk galaxies. Whathever the reason, the missing mass problem implies that by simply measuring the surface brightness profile of a disk galaxy there is no simple, direct way to transform this into a circular velocity profile. . In this post I will try to establish a connection between the distribution of luminous mass of disk galaxies, that can be derived with cheap photometric observations, and the shape of the rotation curve, which traces the gravitational potential. This could be useful in order to predict the shape of the gravitational field with just images of the galaxy. . To do this I will train a very simple feedforward neural network (NN) made with just one hidden linear layer, activations, and batch normalization. This will be trained with a sample of 145 galaxies with measured rotation curves in the SPARC database. . The baseline model to which I will compare the performance of the NN is the framework of Modified Newtonian Dynamics (MOND), since its empirical law that relates the baryonic acceleration to the total gravitational acceleration is observed to provide excellent approximations of the observed rotation curves (e.g. Begeman et al. 1991). . The SPARC dataset . I use the rich rotation curve catalog called SPARC, which was originally compiled by Lelli et al. (2016) and it is perfectly suited for this kind of study. . import numpy as np import pandas as pd import matplotlib.pylab as plt import torch, math import torch.nn as nn from torch import tensor %matplotlib inline %config Completer.use_jedi = False rng = np.random.default_rng() . . Here I copied the table describing the sample and the table with all the velocity profiles and surface brightnesses. . df_sp_master = pd.read_csv(&#39;sparc_mastertable.txt&#39;, delim_whitespace=True, header=None, names=[&#39;Galaxy&#39;,&#39;T&#39;,&#39;D&#39;,&#39;e_D&#39;,&#39;f_D&#39;,&#39;Inc&#39;,&#39;e_Inc&#39;,&#39;L[3.6]&#39;,&#39;e_L[3.6]&#39;,&#39;Reff&#39;,&#39;SBeff&#39;, &#39;Rdisk&#39;,&#39;SBdisk0&#39;,&#39;MHI&#39;,&#39;RHI&#39;,&#39;Vflat&#39;,&#39;e_Vflat&#39;,&#39;Q&#39;,&#39;Ref.&#39;]) # profiles table df_sp_mm = pd.read_csv(&#39;sparc_mmtab.txt&#39;, delim_whitespace=True) . The SPARC database provides Vgas, Vdisk, and Vbul for each radius R, which are the contribution of respectively the gas, disk stellar component, and bulge stellar component to the total circular velocity curve of the galaxy. This are obtained directly from observations of the gas/disk/bulge surface density profiles through Poisson&#39;s equation (see Casertano 1983). . From the contributions to the circular velocities of the various components, we can obtain the total baryonic circular velocity which is . $$ V_{ rm bar}^2 = V_{ rm gas}^2 + (M/L)_{ rm disk}V_{ rm disk}^2 + (M/L)_{ rm bul}V_{ rm bul}^2, $$where we set $(M/L)_{ rm disk}=0.5$ and $(M/L)_{ rm bul}=0.7$ (at 3.6$ mu$m) as suggested by Lelli et al. (2016). . df_sp_mm[&#39;Vbar&#39;] = np.sqrt(df_sp_mm[&#39;Vgas&#39;]**2+0.5*df_sp_mm[&#39;Vdisk&#39;]**2+0.7*df_sp_mm[&#39;Vbul&#39;]**2) . An appropriate baseline model: MOND . The best way that we have to predict rotation curves from surface density profiles is by using the empirical MOND law. Modified Newtonian Dynamics (MOND) was introduced by Milgrom (1983) and it consists of a modification of Newtonian gravity which is empirically observed to fit galaxy rotation curves without the need for dark matter. . While MOND has been developed to be a full gravitational theory, here I am going to use the phenomenological law of gravity relevant for galaxy disks (see e.g. Eq. 3 here). The acceleration in MOND is related to the Newtonian acceleration as: $$ g_{ rm MOND} = g_{ rm N} nu(g_{ rm N}/a_0), qquad { rm or} qquad g_{ rm N} = g_{ rm MOND} mu(g_{ rm MOND}/a_0), $$ where $g_{ rm N}$ is the Newtonian acceleration, $a_0$ is a constant acceleration and $ nu$ is the so-called interpolating function that specifies the MOND theory. Here I am using $$ nu(y) = frac{1+(1+4y^{-1})^{1/2}}{2}, $$ whose inverse is $ mu(x)=x/(1+x)$, i.e. the so-called simple interpolating function in MOND jargon (Famaey &amp; Binney 2005, Famaey &amp; McGaugh 2012), and a constant acceleration scale $a_0 = 1.2 times 10^{-10} rm m ,s^{-2}$ (see e.g. McGaugh et al. 2016). . The acceleration due to baryons alone is just $g_{ rm bar} = V_{ rm bar}^2/R$. Then, since $V_{ rm MOND} = sqrt{g_{ rm MOND}R}$ and $g_{ rm MOND} = g_{ rm bar} nu(g_{ rm bar}/a_0)$, I finally get that $$ V_{ rm MOND} = sqrt{R ,g_{ rm bar} nu(g_{ rm bar}/a_0)}=V_{ rm bar} sqrt{ nu(g_{ rm bar}/a_0)} $$ . df_sp_mm[&#39;Gbar&#39;] = df_sp_mm[&#39;Vbar&#39;]**2 / (df_sp_mm[&#39;R&#39;]*3.086e16) * 1e3 # m s^-2 # interpolating nu-function (Eq. 3, http://www.scholarpedia.org/article/The_MOND_paradigm_of_modified_dynamics) # corresponding to the &quot;simple&quot; mu-function of Famaey &amp; Binney nu_mond = lambda y: 0.5+0.5*np.sqrt(1+4/y) df_sp_mm[&#39;Vmond&#39;] = df_sp_mm[&#39;Vbar&#39;] * np.sqrt(nu_mond(df_sp_mm[&#39;Gbar&#39;]/1.2e-10)) . Now we have a baseline model that we can use as an expectation on the rotation curve of a disk galaxy from its surface brightness . A neural network model . We now want to set up a deep neural network (NN) architecture that gets as an input arrays of $R$ and $V_{ rm bar}(R)$ and outputs arrays of the total circular velocity of a disk galaxy, $V_{ rm obs}(R)$. . Data manipulation . Let&#39;s start by splitting the sample into training and validation sets. There are 175 galaxies in SPARC, each with a rotation curve array with variable size. Let&#39;s split the catalog in 145 random galaxies for training and 30 random galaxis for validation, even though they will have very different numbers of curve datapoints. . valid_gals = rng.choice(174, size=30, replace=False) # 30/175 = 17% of galaxies in validation set train_idx = np.where(~np.isin(df_sp_mm.Galaxy, df_sp_master[&#39;Galaxy&#39;][valid_gals]))[0] valid_idx = np.where(np.isin(df_sp_mm.Galaxy, df_sp_master[&#39;Galaxy&#39;][valid_gals]))[0] # shuffle training dataset rng.shuffle(train_idx) splits = (list(train_idx), list(valid_idx)) . Then I normalize the $X=[R, V_{ rm bar}]$ and $Y=[V_{ rm obs}]$ arrays, i.e. the input and output of the NN, and I convert them to tensors. . df_nn = df_sp_mm[[&#39;R&#39;, &#39;Vbar&#39;]] # columns used in the NN df_nn_train, df_nn_valid = df_nn.iloc[train_idx], df_nn.iloc[valid_idx] y_nn_train, y_nn_valid = df_sp_mm[&#39;Vobs&#39;].iloc[train_idx], df_sp_mm[&#39;Vobs&#39;].iloc[valid_idx] # normalize X mean_df_nn, std_df_nn = df_nn_train.mean(), df_nn_train.std() df_nn_train_norm = (df_nn_train-mean_df_nn)/std_df_nn df_nn_valid_norm = (df_nn_valid-mean_df_nn)/std_df_nn # normalize y mean_y_nn, std_y_nn = y_nn_train.mean(), y_nn_train.std() y_nn_train_norm = (y_nn_train-mean_y_nn)/std_y_nn y_nn_valid_norm = (y_nn_valid-mean_y_nn)/std_y_nn # convert to tensors x_train = tensor(df_nn_train_norm.values).float() y_train = tensor(y_nn_train_norm.values).float() x_valid = tensor(df_nn_valid_norm.values).float() y_valid = tensor(y_nn_valid_norm.values).float() . Normalize and tensorize also the MOND predictions, so that we will be able to compare the losses with the NN model. . ymond_train = tensor(((df_sp_mm[&#39;Vmond&#39;].iloc[train_idx]-mean_y_nn)/std_y_nn).values).float() ymond_valid = tensor(((df_sp_mm[&#39;Vmond&#39;].iloc[valid_idx]-mean_y_nn)/std_y_nn).values).float() . Minimal Dataset and Dataloader . I bundle the training and validation sets into a simple dataset class derived from torch.utils.data.Dataset. This is meant to be used with torch.utils.data.DataLoader for batch training. . Defining the datasets . class RotCurveDset(torch.utils.data.Dataset): def __init__(self, x, y): self.x = x self.y = y def __len__(self): return len(self.x) def __getitem__(self, idx): return self.x[idx], self.y[idx] dset_train = RotCurveDset(x_train, y_train) dset_valid = RotCurveDset(x_valid, y_valid) . and defining the dataloaders, with a rather large batch size (640) compared to the total number of the training datapoints (~2700) . bs = 64*10 dload_train = torch.utils.data.DataLoader(dset_train, batch_size=bs) dload_valid = torch.utils.data.DataLoader(dset_valid, batch_size=bs) . The torch.nn model . I set up an extremely simple architecture with just one hidden layer, with 64 units, a leaky ReLU activation, and a BatchNorm layer. . nl1 = 64 model = nn.Sequential(nn.Linear(2, nl1), nn.LeakyReLU(), nn.BatchNorm1d(nl1), # nn.Linear(nl1, nl1), nn.LeakyReLU(), #nn.Dropout1d(p=0.6), nn.Linear(nl1, 1)) model . I use a standard mean squared error loss function and the Adam optimizer algorithm, with a moderate weight decay to prevent overfitting. . loss_fn = nn.MSELoss(reduction=&#39;mean&#39;) optim = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4) . Training loop . We now train the model. At each epoch we store the training loss and validation loss and after 1000 epochs we compare the training losses and validation losses to the MOND case. . %%time epochs = 1000 train_losses, valid_losses = [], [] for i in range(epochs): # # train phase # model.train() for batch, (x,y) in enumerate(dload_train): # model and loss y_pred = model(x) loss = loss_fn(y_pred.squeeze(), y) # backprop optim.zero_grad() loss.backward() optim.step() # # test phase # model.eval() with torch.no_grad(): loss = loss_fn(model(x_train).squeeze(), y_train) val_loss = loss_fn(model(x_valid).squeeze(), y_valid) train_losses.append(loss) valid_losses.append(val_loss) if i % 100 == 0: print (&quot;Epoch: %2d [Train Loss: %1.5f // Valid Loss: %1.5f]&quot; % (i, loss, val_loss)) print () print (&#39;&#39;) print (&#39;Comparison with MOND: [Train Loss: %1.5f // Valid Loss: %1.5f]&#39; % (loss_fn(ymond_train, y_train), loss_fn(ymond_valid, y_valid))) print (&#39;&#39;) print () . Epoch: 0 [Train Loss: 1.05912 // Valid Loss: 1.13404] Epoch: 100 [Train Loss: 0.09393 // Valid Loss: 0.07471] Epoch: 200 [Train Loss: 0.07501 // Valid Loss: 0.05540] Epoch: 300 [Train Loss: 0.07064 // Valid Loss: 0.05421] Epoch: 400 [Train Loss: 0.06949 // Valid Loss: 0.05205] Epoch: 500 [Train Loss: 0.06878 // Valid Loss: 0.05173] Epoch: 600 [Train Loss: 0.06816 // Valid Loss: 0.05136] Epoch: 700 [Train Loss: 0.06742 // Valid Loss: 0.05121] Epoch: 800 [Train Loss: 0.06701 // Valid Loss: 0.05139] Epoch: 900 [Train Loss: 0.06668 // Valid Loss: 0.05140] Comparison with MOND: [Train Loss: 0.07238 // Valid Loss: 0.04494] CPU times: user 54 s, sys: 1.24 s, total: 55.3 s Wall time: 42 s . We can see that while the NN model is able to have a training loss significantly smaller than the MOND case, the validation loss is similar or even smaller in the MOND case. This exemplifies the difficulty in generalizing the behaviour learned in the training phase. . Let&#39;s now look at the plot of the training and validation losses as a function of epoch. . plt.loglog(train_losses, label=&#39;train&#39;) plt.loglog(valid_losses, label=&#39;valid&#39;) plt.legend(); plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;); . Here we can see that for this particular train/valid split, the learning rate and the weight decay hyper-parameters, as well as the number of units and linear layers of the NN, are well tuned to have gradually decreasing train and validation losses. Increasing the model&#39;s complexity or adding additional layers (e.g. Dropout) does not seem to help in this case, thus I preferred to stick with a simple 1-layer feedforward NN. . Comparing rotation curve predictions . Now that we have trained the NN model, we can apply it to the validation set in order to make proper predictions for the rotation curves of disk galaxies just from their surface density profile. . y_pred = model(x_valid).squeeze(-1).detach() . And finally we can plot the rotation curves of the 30 galaxies in the validation set in separate panels. Below I overplot the real data (black points), the MOND empirical expectation (blue dotted curve), and the NN prediction (orange solid curve) . fig, ax = plt.subplots(figsize=(18,20), ncols=5, nrows=6, gridspec_kw={&#39;hspace&#39;:0.3}) for k,g in enumerate(df_sp_mm[&#39;Galaxy&#39;].iloc[valid_idx].unique()): i,j = int(k/5), k%5 ax[i,j].errorbar(df_sp_mm[&#39;R&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], df_sp_mm[&#39;Vobs&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], yerr=df_sp_mm[&#39;e_Vobs&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], c=&#39;k&#39;, fmt=&#39;.&#39;, lw=0.5, label=&#39;data&#39;) ax[i,j].plot(df_sp_mm[&#39;R&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], df_sp_mm[&#39;Vmond&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], c=&#39;C9&#39;, ls=&#39;:&#39;, label=&#39;MOND&#39;) ax[i,j].plot(df_sp_mm[&#39;R&#39;][df_sp_mm[&#39;Galaxy&#39;]==g], std_y_nn*y_pred[(df_sp_mm[&#39;Galaxy&#39;].iloc[valid_idx]==g).values] + mean_y_nn, c=&#39;C1&#39;, lw=1, label=&#39;NN&#39;) ax[i,j].set_xlim((0,None)) ax[i,j].set_ylim((0,None)) ax[i,j].set_title(g) if i==5: ax[i,j].set_xlabel(&#39;radius / kpc&#39;); if j==0: ax[i,j].set_ylabel(&#39;velocity / km s&#39;+&#39;$^{-1}$&#39;); ax[0,0].legend(); . All in all, the performance of the NN model are quite satisfactory as it gives nice predictions for $V_{ rm obs}$ for the majority, though not all, galaxies. . Strikingly, the predictions of the NN model are extremely close to the MOND rotation curves! This means that a flexible NN when asked to predict rotation curves from surface densities reduces to something that closely resemble the MOND phenomenology. This suggests that, at least in the SPARC dataset, there is nothing more to learn on the rotation curve shapes that is not encapsulated in the MOND empirical law. . To see this even better, let&#39;s plot the baryonic acceleration versus the total gravitational acceleration oberved/predicted in the rotation curves. This is the so-called radial acceleration relation plot (McGaugh et al. 2016). . The MOND prediction is pretty clear on this diagram, as $g_{ rm MOND} = g_{ rm bar} nu(g_{ rm bar}/a_0)$. On the other hand, to generate this for the NN model I take a toy rotation curve that is flat at 200 km/s and that spans a huge radial radial extension, from 0.1 kpc to 1 Mpc. This is done just to cover smoothly both the low- and high-acceleration regimes in which the SPARC galaxies are found. . def gmond(vbar, r): return (vbar**2 * nu_mond(vbar**2/(r*3.086e16)*1e3/1.2e-10)) / (r*3.086e16)*1e3 def get_normed_rv_tensor(r, v): return ((torch.vstack((tensor(r),tensor(v))).T - tensor(mean_df_nn.values))/tensor(std_df_nn.values)).float() r = np.logspace(-1., 3., 200) v = np.full_like(_r, 200.) vmod = (model(get_normed_rv_tensor(r, v)).squeeze(-1).detach()*std_y_nn+mean_y_nn).numpy() fig,ax = plt.subplots(figsize=(7,7)) ax.scatter(df_sp_mm[&#39;Gbar&#39;], df_sp_mm[&#39;Vobs&#39;]**2 / (df_sp_mm[&#39;R&#39;]*3.086e16) * 1e3, s=2, c=&#39;grey&#39;, alpha=0.5, label=&#39;data&#39;) ax.loglog([1e-12,1e-8],[1e-12,1e-8], &#39;k-&#39;, lw=0.5, label=&#39;1:1&#39;) ax.loglog(v**2/(r*3.086e16)*1e3, gmond(v, r), &#39;:&#39;, c=&#39;C9&#39;, lw=6, label=&#39;MOND&#39;) ax.loglog(v**2/(r*3.086e16)*1e3, vmod**2/(r*3.086e16)*1e3, &#39;-&#39;, c=&#39;C1&#39;, lw=2, label=&#39;NN&#39;) ax.tick_params(labelsize=14) ax.legend(fontsize=14) ax.set_xlabel(r&quot;$g_{ rm bar}/ rm m ,s^{-2}$&quot;, fontsize=20); ax.set_ylabel(r&quot;$g_{ rm obs}/ rm m ,s^{-2}$&quot;, fontsize=20); . There is clearly a very good agreement between both models and the data and between the NN and MOND models themselves. Both the data and the two models follow a smooth departure from the 1:1 line at around the charcteristic acceleration scale $a_0=1.2 times 10^{-10} rm m ,s^{-2}$. . The fact that the flexible NN model follows closely the MOND expectations in this diagram is, again, suggesting that the full SPARC dataset can be very well described with just the MOND empirical law plus some scatter. .",
            "url": "https://lposti.github.io/MLPages/neural_network/jupyter/2022/11/16/rotcurves_NN_MOND.html",
            "relUrl": "/neural_network/jupyter/2022/11/16/rotcurves_NN_MOND.html",
            "date": " • Nov 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
            "content": "Attribution &amp; License . If you use parts of the code found in this notebook please cite the paper Posti (2022), Res. Notes AAS, 6, 233. . Copyright (c) 2022, Lorenzo Posti. The code is distributed under BSD-style license and it can be copied and used. . Introduction . Galaxy rotation curves are usually modelled by assuming that each datapoint in the curve is independent from the others. However, this is naturally just a first order approximation, since observational effects such due to geometrical projection and resolution, as well as physical effects such as non-circular motions, can make the velocities measured in two adjacent annnuli significantly correlated. . In this notebook I use the rotation curve of NGC 2403 as a test case to show how to include Gaussian Processes (GPs) in rotation curve decomposition models, in order to account for underlying data correlations. More details can be found in the accompanying paper Posti (2022), Res. Notes AAS, 6, 233. . import numpy as np import matplotlib import matplotlib.pylab as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import jax import jax.numpy as jnp import jaxopt from functools import partial from tinygp import GaussianProcess, kernels import numpyro import arviz import corner jax.config.update(&quot;jax_enable_x64&quot;, True) import warnings warnings.filterwarnings(&#39;ignore&#39;) from matplotlib import rc rc(&#39;text&#39;, usetex=True) %config Completer.use_jedi = False %matplotlib inline . . Data . Here I introduce the data for the galaxy NGC 2403 and the functions needed to work with Eq. (1) in the paper. . Note that the code below works also for any other galaxy whose data are formatted in the same way, e.g. it works with no modifications needed for all galaxies in the SPARC catalog (Lelli et al. 2016). . Definitions of functions for curve decomposition . I start with some definitions to specify $V_{ rm DM}(R)$, the contribution of DM to the circular velocity in Eq. (1). I assume NFW profiles for the DM halos, which are specified by two parameters: halo mass $M_{ rm h}$ and concentration $c$. $M_{ rm h}$ is the virial mass defined with a critical overdensity of $ Delta_{ rm c}=200$. . G = 4.301e-9 # gravitational constant, in Mpc km^2 s^-2 Msun^-1 H = 70. # Hubble&#39;s constant, in km s^-1 Mpc^-1 Dc= 200. # critical overdensity . Note that below I use jax, and not numpy, to define these functions. This is needed in order to do model inference with numpyro. . def jax_fc(x): return jnp.log(1+x)-x/(1+x) # definitions of virial velocity and virial radius def jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def jax_Rvir(Mh): rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G)) return 1e3 * ((Mh / rho_hat)**(1./3.)) # V_DM(R) for an NFW halo def jax_vhalo(params, R): Mh, cc = 10**params[&#39;log_mh&#39;], 10**params[&#39;log_c&#39;] rv = jax_Rvir(Mh) return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) . Finally, below I implement the model rotation curve in Eq. (1). Two things are worth pointing out: . I use a linear interpolation for the baryonic part of the curve, since I assume that $V_{ rm gas}$ and $V_ star$ are measured at fixed radii, so that params[&#39;r&#39;], params[&#39;vg&#39;] etc. are expected to be arrays of the same size. | I decomposed $V_ star$ in the bulge and disk components, with two different mass-to-light ratios. While my test case NGC 2403 has no bulge, it is good to include it here for the sake of generality. The SPARC catalog indeed includes stellar circular velocities decomposed into bulge and disk. | . def jax_vmod(params, R): return jnp.sqrt(jax_vhalo(params, R)**2 + # DM jnp.interp(R, params[&#39;r&#39;], params[&#39;vg&#39;]**2 + # gas 10**params[&#39;log_mld&#39;]*params[&#39;vd&#39;]**2+10**params[&#39;log_mlb&#39;]*params[&#39;vb&#39;]**2 # stars )) . Rotation curve data for NGC 2403 . I take the rotation curve data of NGC 2403 from the SPARC catalog at face value. I include a copy of the file here for convenience. . r, vobs, e_vobs, vg, vd, vb, _, _ = np.genfromtxt(&#39;data/NGC2403_rotmod.dat&#39;, unpack=True) . Let&#39;s plot the data below together with the best-fit model obtained by Posti et al. (2019) for reference. . params = { # parameters of the best-fit in Posti et al. (2019) &#39;log_mh&#39; : 11.4, &#39;log_c&#39; : 1.14, &#39;log_mld&#39;: -0.377, &#39;log_mlb&#39;: -99., # this galaxy has no bulge # data arrrays &#39;r&#39; : r, &#39;vg&#39; : vg, &#39;vd&#39; : vd, &#39;vb&#39; : vb, } fig,ax = plt.subplots(figsize=(8,5)) ax.errorbar(r, vobs, yerr=e_vobs, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=r&#39;$ rm data$&#39;) ax.plot(r, vg, &#39;:&#39;, c=&#39;tab:cyan&#39;, label=r&#39;$ rm gas$&#39;) ax.plot(r, np.sqrt(10**params[&#39;log_mld&#39;]*vd**2+10**params[&#39;log_mlb&#39;]*vb**2), &#39;--&#39;, c=&#39;tab:olive&#39;, label=r&#39;$ rm stars$&#39;) ax.plot(r, jax_vhalo(params, r), &#39;-.&#39;, c=&#39;tab:purple&#39;, label=r&#39;$ rm DM$&#39;) ax.plot(r, jax_vmod(params, r), c=&#39;grey&#39;, lw=2, label=r&#39;$ rm fit$&#39;) ax.set_xlabel(r&quot;$ rm radius/kpc$&quot;, fontsize=18) ax.set_ylabel(r&quot;$ rm velocity/km ,s^{-1}$&quot;, fontsize=18) ax.set_title(r&quot;$ rm NGC 2403$&quot;, fontsize=20); ax.legend(loc=&#39;lower right&#39;, fontsize=14); ax.tick_params(labelsize=14); . Models with or without GPs . Let&#39;s now get to the modelling side of things. I set up two models here. The first one is analogous to Posti et al. (2019), as well as many other works in this context (e.g. Katz et al. 2017, Li et al. 2020, Mancera-Pina et al. 2022, di Teodoro et al. 2022), it has a $ chi^2$ likelihood and it implicitly assumes that the rotation curve datapoints are independent. The second one generalizes this model by using GPs to take into account data correlations. I recommend the recent review by Aigrain &amp; Foreman-Mackey (2022), in particular their first section, as an introduction to GPs. . I use the library tinygp to set up my GP regression problem and I use numpyro to sample the posterior distribution. In particular, I use and Hamiltonian Monte Carlo sampler called No U-Turn Sampler (NUTS). . Gaussian Processes . Let&#39;s generate GPs with an Exp-Squared kernel with two parameters, an amplitude $A_k$ and a scale $s_k$, i.e. $$ k(R_i, R_j) = A_k exp left[- frac{1}{2} left( frac{|R_i-R_j|}{s_k} right)^2 right] $$ This kernel is said to be stationary because it depends only on the distance between two points $|R_i-R_j|$. . def build_gp(params, x, yerr): kernel = 10**params[&#39;log_amp&#39;]*kernels.ExpSquared(10**params[&#39;log_scl&#39;], distance=kernels.distance.L1Distance()) return GaussianProcess(kernel, x, diag=yerr**2, mean=partial(jax_vmod, params) ) . numpyro models . I now set up the model&#39;s posterior distribution to be sampled by numpyro, thus it is a function containing pyro primitives. . The model starts by defining the priors on the physical parameters ($ theta_V$ in the paper). I use: . uninformative prior on $ log ,M_{ rm h}$ | Gaussian on $ log ,c$, with mean and width following the $c-M_{ rm h}$ relation found in cosmological simulations (Dutton &amp; Maccio&#39; 2014) | Gaussian on $ log ,(M/L)_{ rm D}$, centred on $(M/L)_{ rm D}=0.5$ and with standard deviation of 0.2 dex (compatible with stellar population synthesis models, e.g. Lelli et al. 2016) | Gaussian on $ log ,(M/L)_{ rm B}$, centred on $(M/L)_{ rm B}=0.7$ and with standard deviation of 0.2 dex (again, see Lelli et al. 2016). Note that this is not used in the case of NGC 2403 | . After the definition of the priors the function branches out: one branch with GPs and one without. I borrowed this structure from the transit example of Fig. 3 in Aigrain &amp; Foreman-Mackey (2022). . The branch with GP also implement additional priors for the two parameters of the kernel ($ theta_k$ in the paper), i.e. the amplitude and scale. . def model(t, y_err, y, params, use_gp=False): # priors params[&quot;log_mh&quot;]=numpyro.sample(&quot;log_mh&quot;,numpyro.distributions.Uniform(8.0, 14.0)) params[&quot;log_c&quot;] =numpyro.sample(&#39;log_c&#39;,numpyro.distributions.Normal(0.905-0.101*(params[&#39;log_mh&#39;]*0.7-12),0.11)) params[&quot;log_mld&quot;]=numpyro.sample(&#39;log_mld&#39;,numpyro.distributions.Normal(-0.3, 0.2)) params[&quot;log_mlb&quot;]=numpyro.sample(&#39;log_mlb&#39;,numpyro.distributions.Normal(-0.15, 0.2)) if use_gp: ################### # branch WITH GPs # ################### # define kernel parameters params[&quot;log_amp&quot;] = numpyro.sample(&quot;log_amp&quot;, numpyro.distributions.Uniform(-4.0, 5.0)) params[&quot;log_scl&quot;] = numpyro.sample(&quot;log_scl&quot;, numpyro.distributions.Uniform(-2.0, 3.0)) # generate the GP gp = build_gp(params, t, y_err) # sample the posterior numpyro.sample(&quot;y&quot;, gp.numpyro_dist(), obs=y) # calculate the predicted V_rot (i.e. the mean function) of the model mu = gp.mean_function(params[&quot;r_grid&quot;]) numpyro.deterministic(&quot;mu&quot;, mu) else: ###################### # branch WITHOUT GPs # ###################### # sample the posterior numpyro.sample(&quot;y&quot;, numpyro.distributions.Normal(jax_vmod(params, t), y_err), obs=y) # calculate properties of the model numpyro.deterministic(&quot;mu&quot;, jax_vmod(params, params[&quot;r_grid&quot;])) . Running the model without GP, i.e. assuming independent datapoints . I start by sampling the posterior of the model akin to that of Posti et al. (2019), i.e. assuming that the points in the curve are independent. . I&#39;m using arviz to analyse the posterior sampled by NUTS. In particular, keep an eye on r_hat which is the Gelman-Rubin statistics: for our purposes, we can use ${ rm r_{hat}} simeq 1$ as an indicator that the marginalized posterior on a particular parameter is well determined. . grid_size = 1000 r_grid = jnp.linspace(r.min(), r.max(), grid_size) # radial grid on which to predict V_rot(R) params = {&quot;vg&quot; : vg, &quot;vd&quot; : vd, &quot;vb&quot; : vb, &quot;r&quot; : r, &quot;r_grid&quot;: r_grid} . num_warmup=1000 num_samples=3000 num_chains=3 accept_prob = 0.9 sampler_wn = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=accept_prob, ), num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains, progress_bar=True, ) %time sampler_wn.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params) inf_data_wn = arviz.from_numpyro(sampler_wn) arviz.summary(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;]) . sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 277.33it/s, 15 steps of size 2.45e-01. acc. prob=0.92] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 281.80it/s, 15 steps of size 2.07e-01. acc. prob=0.95] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:13&lt;00:00, 288.30it/s, 15 steps of size 1.95e-01. acc. prob=0.95] . CPU times: user 43.9 s, sys: 807 ms, total: 44.7 s Wall time: 45.7 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.325 | 0.014 | 11.298 | 11.351 | 0.000 | 0.000 | 3623.0 | 4045.0 | 1.0 | . log_c 1.234 | 0.019 | 1.198 | 1.268 | 0.000 | 0.000 | 3382.0 | 3801.0 | 1.0 | . log_mld -0.542 | 0.045 | -0.632 | -0.463 | 0.001 | 0.001 | 3267.0 | 3282.0 | 1.0 | . Let&#39;s plot the chains to make sure each parameter is converged . azLabeller = arviz.labels.MapLabeller(var_name_map={&quot;log_mh&quot; : r&quot;$ log ,M_{ rm h}$&quot;, &quot;log_c&quot; : r&quot;$ log ,c$&quot;, &quot;log_mld&quot;: r&quot;$ log ,M/L$&quot;, &quot;log_amp&quot;: r&quot;$A_k$&quot;, &quot;log_scl&quot;: r&quot;$s_k$&quot;}) arviz.plot_trace(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], figsize=(12,9), labeller = azLabeller); . Running the model with GP, i.e. taking into account data correlations . Now, let&#39;s have a look at what happens when GPs come into play. I run exactly the same procedure as before to sample the model&#39;s posterior, but this time I select the branch with use_gp=True. Since now I have two more free parameters, the scale and amplitude of the kernel, I also initialize these two in init_strategy. . sampler = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=accept_prob, ), num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains, progress_bar=True, ) %time sampler.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params, use_gp=True) inf_data = arviz.from_numpyro(sampler) arviz.summary(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;]) . sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:29&lt;00:00, 136.37it/s, 15 steps of size 3.46e-01. acc. prob=0.96] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:27&lt;00:00, 146.69it/s, 7 steps of size 3.67e-01. acc. prob=0.96] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:25&lt;00:00, 159.04it/s, 7 steps of size 4.01e-01. acc. prob=0.94] . CPU times: user 2min 27s, sys: 2.85 s, total: 2min 30s Wall time: 1min 38s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.491 | 0.076 | 11.353 | 11.642 | 0.001 | 0.001 | 5466.0 | 4897.0 | 1.0 | . log_c 1.079 | 0.065 | 0.956 | 1.201 | 0.001 | 0.001 | 5602.0 | 4905.0 | 1.0 | . log_mld -0.326 | 0.075 | -0.470 | -0.191 | 0.001 | 0.001 | 5531.0 | 4435.0 | 1.0 | . log_amp 1.104 | 0.270 | 0.624 | 1.602 | 0.005 | 0.004 | 3793.0 | 3277.0 | 1.0 | . log_scl -0.018 | 0.087 | -0.188 | 0.131 | 0.001 | 0.001 | 3788.0 | 3652.0 | 1.0 | . And let&#39;s plot again the chains to make sure that every parameter has converged . arviz.plot_trace(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;], figsize=(12,15), labeller = azLabeller); . Everything looks good for both models. . Comparing the predictions of the two models . Finally, I can compare the posterior distributions of the two models. I use corner to plot 1-D and 2-D projections of the posteriors. . ranges = [(11.2, 11.8), (0.8, 1.35), (-0.8,-0.0)] # NGC 2403 fig = corner.corner(inf_data_wn, bins=40, range=ranges, color=&quot;C0&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], smooth=1.0, smooth1d=1.0 ) fig = corner.corner(inf_data, bins=40, range=ranges, color=&quot;C1&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_mld&quot;], smooth=1.0, smooth1d=1.0, labels=[&quot;$ log ,M_h$&quot;, &quot;$ log ,c$&quot;, r&quot;$ log ,M/L$&quot;], label_kwargs={&quot;fontsize&quot;:20}, fig=fig) # make legend ax = fig.axes[1] key_nn = matplotlib.lines.Line2D([], [], color=&#39;C0&#39;, linestyle=&#39;-&#39;, label=r&#39;$ rm Model , ,{ bf without , ,GPs}: , ,independent , ,data$&#39;) key_gp = matplotlib.lines.Line2D([], [], color=&#39;C1&#39;, linestyle=&#39;-&#39;, label=r&#39;$ rm Model , ,{ bf with , ,GPs}: , ,correlated , ,data$&#39;) ax.legend(loc=&#39;upper left&#39;, handles=[key_nn, key_gp], fontsize=16); . Then I can compare the predicted curve decompositions of the two models. Here I get the predictions of the model, excluding the warmup phase during sampling. . final_shape = (num_chains*(num_samples-num_warmup),grid_size) # shape of the predictions array after removing warmup pred_wn = sampler_wn.get_samples(group_by_chain=True)[&#39;mu&#39;][:,num_warmup:,:].reshape(final_shape) pred = sampler.get_samples(group_by_chain=True)[&#39;mu&#39;][:,num_warmup:,:].reshape(final_shape) . fig,ax = plt.subplots(figsize=(7,8), nrows=2, gridspec_kw={&#39;hspace&#39;:0}) def commons(ax, i, lmh, lc, lmld, lmlb, leg=True): # rotation curve ax[i].errorbar(r, vobs, yerr=e_vobs, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=r&#39;$ rm data$&#39;, markersize=4) ax[i].plot(r, vg, &#39;:&#39;, c=&#39;tab:cyan&#39;, label=r&#39;$ rm gas$&#39;) ax[i].plot(r, np.sqrt(10**lmld*vd**2+10**lmlb*vb**2), &#39;--&#39;, c=&#39;tab:olive&#39;, label=r&#39;$ rm stars$&#39;) ax[i].plot(r, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r), &#39;-.&#39;, c=&#39;tab:purple&#39;, label=r&#39;$ rm DM$&#39;) if leg: ax[i].legend(loc=&#39;lower right&#39;, fontsize=14) ax[i].set_xlabel(r&quot;$ rm radius/kpc$&quot;, fontsize=18) ax[i].set_ylabel(r&quot;$ rm velocity/km ,s^{-1}$&quot;, fontsize=18) ax[i].tick_params(direction=&#39;inout&#39;, top=True, labelsize=14) ax[0].fill_between(r_grid, np.percentile(pred_wn,2.1,axis=0), np.percentile(pred_wn,97.9,axis=0), facecolor=&#39;C0&#39;, alpha=0.3) ax[1].fill_between(r_grid, np.percentile(pred,2.1,axis=0), np.percentile(pred,97.9,axis=0), facecolor=&#39;C1&#39;, alpha=0.3) ax[0].plot(r_grid, np.median(pred_wn, axis=0), &#39;C0&#39;) ax[1].plot(r_grid, np.median(pred, axis=0), &#39;C1&#39;) commons(ax, 0, 11.325, 1.234, -0.542, -99.) commons(ax, 1, 11.491, 1.079, -0.326, -99., leg=False) . Correlation matrix . Given that the parameters of the kernel $A_k$ and $s_k$ are well constrained by the model, we can have a look at the correlation matrix of the model with GPs. . %%time def get_kmat(params, x, yerr,): gp = build_gp(params, x, yerr) xm1, xm2 = jnp.meshgrid(x,x) zm = np.zeros_like(xm1.flatten()) for i in range(len(xm1.flatten())): zm[i]=gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i]) return zm.reshape((len(x), len(x))) zm = get_kmat({&quot;log_amp&quot;:1.104, &quot;log_scl&quot;:-0.018, &#39;log_mh&#39;:11.325, &#39;log_c&#39;:1.234, &#39;log_mld&#39;:-0.542, &#39;log_mlb&#39;:-99., &#39;vg&#39;:vg, &#39;vd&#39;:vd, &#39;vb&#39;:vb, &#39;r&#39;:r}, r, e_vobs) . CPU times: user 1min 35s, sys: 2.17 s, total: 1min 37s Wall time: 2min . def plt_mat(ax, zm): im=ax.matshow(zm) ax.set_xlabel(&quot;$i$&quot;, fontsize=20) ax.set_ylabel(&quot;$j$&quot;, fontsize=20) ax.tick_params(labelsize=14) ax.xaxis.set_label_position(&#39;top&#39;) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.05) cb = plt.colorbar(im, cax=cax) cb.set_label(r&quot;$k(R_i, R_j)$&quot;, fontsize=22) cb.ax.tick_params(labelsize=16) . fig,ax = plt.subplots() plt_mat(ax, zm) . The peculiar 2-block shape of this matrix is due to the combined H$ alpha$-HI nature of the rotation curve. The optical H$ alpha$ curve samples the inner regions of the galaxy, up to $ sim 5$ kpc, with a finer spatial sampling of 0.1 kpc with respect to the HI rotation curve, which instead has a spacing of 0.5 kpc and is from 5 kpc outwards. .",
            "url": "https://lposti.github.io/MLPages/gaussian_processes/2022/11/02/gp_rotcurves.html",
            "relUrl": "/gaussian_processes/2022/11/02/gp_rotcurves.html",
            "date": " • Nov 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Gaussian Processes: modelling correlated noise in a dataset",
            "content": "How I got interested in Gaussian Processes . A few weeks ago I saw a paper passing on the arXiv by Aigrain &amp; Foreman-Mackey which is a review on Gaussian Processes (GPs) from an astrophysical perspective - it is actually targeted for astronomical time-series. I had heard a lot of things about GPs before, but I never really had the time nor will to sit down an actually understand what they are about. That changed when I glanced over the review by Aigrain &amp; Foreman-Mackey, as I quickly realised a couple of things that caught my interest. . In particular, the thing that struck me the most is their Figure 3. This appears at the end of the first Section of the review, which is mostly dedicated to two motivating examples from astronomical time-series analysis (a very well-crafted first Section I must say!). This shows the fit of a mock exoplanet transit lightcurve by two models: the both share the same physics, which is also used to generate the mock dataset, but one takes into account the correlated noise in the data, while the other doesn&#39;t and therefore works under the assumption that the datapoints are all uncorrelated and independent. . GPs are used to realistically represent the correlation in the time-series data typically observed for exoplanet transits and are an ingredient used to generate the mock dataset. So the final outcome of this controlled experiment is that only the model that accounts for correlated data, modelled with a GP, is able to recover reasonably well the true physical parameters, while the other model without a GP infers a severely biased result. . It is interesting to notice that in this example the GP introduces another layer of uncertainty in the model (i.e. how are the data correlated) over which we have to marginalise in order to arrive at the final prediction for the physical parameters. This means that the simple model that treats the data as independent is very confidently inferring a biased result (with relatively high accuracy), as opposed to the model with a GP which is instead less accurate, but unbiased. . Gaussian Processes (GPs) in rotation curve modelling . In this notebook I&#39;m going to explore how we can use GPs in modelling rotation curves of galaxies. Often - pretty much always - circular velocity measurements in galaxies are treated as independent and parametrized rotation curve models are fitted to such datasets without worrying too much if that is a reasonable assumption. Given that most HI rotation curves are derived with a tilted-ring model, I am unconfortable with assuming that each datapoint in a rotation curve is actually independent from all the others, since adjacent rings can easily enter the line-of-sight of a given radius. . For this reason I am going to generate a mock rotation curve dataset where the points are actually correlated. This will be done using a GP. Then I will be fitting this dataset with two models: one assuming that the data are independent, and another taking into account the correlated noise. . import numpy as np import matplotlib import matplotlib.pylab as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import jax import jax.numpy as jnp import jaxopt from functools import partial from tinygp import GaussianProcess, kernels import numpyro import arviz import corner jax.config.update(&quot;jax_enable_x64&quot;, True) import warnings warnings.filterwarnings(&#39;ignore&#39;) %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . Definitions and rotation curve sampling . Define the mathematical functions for the galaxy rotation curves . G, H, Dc = 4.301e-9, 70, 200. # accessory functions def jax_fc(x): return jnp.log(1+x)-x/(1+x) def jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def jax_Rvir(Mh): rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G)) return 1e3 * ((Mh / rho_hat)**(1./3.)) # actual rotation curve def jax_vhalo(params, R): Mh, cc = 10**params[&#39;log_mh&#39;], 10**params[&#39;log_c&#39;] rv = jax_Rvir(Mh) return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) . Lat&#39;s now plot a randomly sampled rotation curve with a typical error on each datapoint of 8 km/s. As a first step, we will assume that each datapoint is independent and thus we will sample from a Gaussian noise distribution for each measurement. . size = 40 lmh, lc, verr = 12, 0.95, 10.0 # generating independent datapoints with Gaussian errors x = np.linspace(0.1, 50, size) y = jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x) + rng.normal(loc=0, scale=verr, size=size) fig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) # rotation curve ax[0].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0].errorbar(x, y, yerr=verr, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5, label=&#39;data&#39;) ax[0].legend(loc=&#39;lower right&#39;) ax[0].set_ylabel(&#39;velocity&#39;); # residuals ax[1].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1].errorbar(x, y-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=verr, fmt=&#39;.&#39;, c=&#39;k&#39;, lw=0.5) ax[1].set_xlabel(&#39;radius&#39;) ax[1].set_ylabel(&#39;residuals&#39;) ax[1].set_ylim(-60,60); . In the simple case plotted above we have generated the points with the implicit assumption that each datapoint was independent from all the others. This is why mathematically we simply added a Gaussian noise term to the median curve when defining y. In the residuals plot, the fact that each datapoint is independent becomes apparent since there is no clear trend in the residuals as a function of radius. . However, in practice this is rarely the case with astronomical observations, since typically instrumental characteristics of the telescope and physical processes make the measurement of a single datapoint to have a non-negligible dependence on some other datapoints. Most of the times when modelling astrophysical data we do not know precisely if and which measurements are correlated with which others, so it is in our best interest to employ a modelling technique that allows for correlated datapoints, instead of assuming they are independent. This is where GPs come into play. . Generating rotation curve data with correlated noise using GPs . Let&#39;s now use GPs to generate a new set of datapoints, but this time they will be correlated to one another. To specify this correlation we need to define a kernel or a covariance function which, in the simplest case that we are using here, is a function only of the physical distance of each point (absolute or L1 distance). . Kernels that depend only of the distance of points are called stationary. A very common kernel function used in GPs is the so-called radial basis function (RBF) or exponential-squared, since $k(x_i, x_j) propto exp left[- frac{1}{2}(d_{ij}/s)^2 right]$, where $d_{ij}=|x_i-x_j|$ is the distance of the two datapoints, while $s in mathbb{R}$ is a scale parameter. . Build the GP with tinygp . We define the GP as follows using the library tinygp. . def build_gp(params, x, yerr): kernel = 10**params[&#39;log_amp&#39;]*kernels.ExpSquared(10**params[&#39;log_scl&#39;], distance=kernels.distance.L1Distance()) return GaussianProcess(kernel, x, diag=yerr**2, mean=partial(jax_vhalo, params) ) . This GP has 2 adjustable parameters: an amplitude log_amp and a scale log_scl (both defined in log). We build the GP by passing it the kernel function, the set of datapoints (just x, not the velocity measurements), the measured uncertainty of the measurements, and the mean function that needs to be added to the noise generated by the GP. . What the library is doing is just building a full covariance matrix on the dataset x using the kernel function provided. The value that we pass on the diag argument will be considered as an additional variance to be added to the covariance matrix. . Let&#39;s now initialize a GP with some amplitude and scale parameters and let&#39;s sample random datapoints from its covariance matrix. . params = {&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc, &#39;log_amp&#39;:jnp.log10(300.0), &#39;log_scl&#39;:jnp.log10(5.0)} # initialize the GP and sample from it gp = build_gp(params, x, verr) # vm = gp.sample(jax.random.PRNGKey(11)) vm = gp.sample(jax.random.PRNGKey(33)) e_vm = np.sqrt(gp.variance) . Here gp.sample gets a random realization of y-measurements on the x-array. We can define their standard errorbars by just taking the variances (i.e. diagonal of the covariance matrix). . The covariance matrix . Just to help out with visualising the GP, let&#39;s plot the covariance matrix for this problem. . def plt_mat(ax, params, x, yerr): gp = build_gp(params, x, yerr) xm1, xm2 = jnp.meshgrid(x,x) zm = np.zeros_like(xm1.flatten()) for i in range(len(xm1.flatten())): zm[i]=(gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i])) im=ax.matshow(zm.reshape((len(x), len(x))), extent=(x.min(), x.max(), x.max(), x.min())) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.05) plt.colorbar(im, cax=cax) . fig,ax = plt.subplots() plt_mat(ax, params, x, verr) . Here each point is coloured according to the covariance $k(x_i,x_j)$. It is highest along the diagonal, where $k(x_i,x_i)=300$ km$^2$/s$^2$, implying a standard uncertainty on each data point of $ sigma= sqrt{300+10^2}=20$ km/s, where the term $10^2$ comes from adding the measured uncertainties to the covariance matrix (the diag argument in GaussianProcess). Given the kernel function and the scale of $5$ that we used in this example, we can see that each datapoint has a significant correlation with all the points closer than ~10. . Sampling the rotation curve with correlated noise . Finally, let&#39;s plot the sampled rotation curve and its residuals and let&#39;s compare them with the uncorrelated case above. . fig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) # rotation curve ax[0].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0].errorbar(x, vm, yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5, label=&#39;data&#39;) ax[0].legend(loc=&#39;lower right&#39;) ax[0].set_ylabel(&#39;velocity&#39;); # residuals ax[1].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1].errorbar(x, vm-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5) ax[1].set_xlabel(&#39;radius&#39;) ax[1].set_ylabel(&#39;residuals&#39;) ax[1].set_ylim(-60,60); . We do see quite a lot of structure in the residuals plot this time! This is in stark contrast to the picture we had when generating datapoints independently. This time each measurements feels the influence of the other measurements closer than ~10 in radius, thus the rotation curve starts having significant trends above and below the mean. . When fitting the rotation curve these trend can be misinterpreted as signal, instead of just correlated noise, and this can potentially bias our inference on the curve parameters quite significantly. We see below an example of this. . Fitting the rotation curve with or without Gaussian Processes . Let&#39;s now consider the rotation curve generated with the GP above, i.e. the blue set of points, and let&#39;s build a model to fit it. The model is the same jax_vhalo function that we used to generate the data, which has 2 free parameters: a mass log_mh and a concentration log_c. . We run the fit in a Bayesian framework and in particular with an MCMC sampler using a standard $ chi^2$ log-likelihood on the observed datapoints. We impose a uniform prior on log_mh and normal prior on log_c, whose mean follows the well-known mass-concentration relation of dark matter halos in $ Lambda$CDM (Dutton &amp; Maccio&#39; 2014). . We use the library numpyro to define the model and to run the MCMC sampling. In particular, numpyro uses a state-of-the-art Hamiltonian No U-Turn Sampler (NUTS) to derive the posterior of the parameters. . We run the fit two times: the first time, we treat the datapoints as independent and we have a &quot;standard&quot; Bayesian inference on the parameters; the second time, we allow the data to be correlated and we model their correlation with a GP with an exp-squared kernel that has two additional free parameters, an amplitude log_amp and a scale log_scl. We impose an uniformative uniform prior on the two parameters of the kernel. . Model without GP . r_grid = jnp.linspace(0.1, 50.0, 1000) def model(t, y_err, y, use_gp=False): # priors log_mh = numpyro.sample(&#39;log_mh&#39;, numpyro.distributions.Uniform(8.0, 14.0)) log_c = numpyro.sample(&#39;log_c&#39;, numpyro.distributions.Normal(0.905-0.101*(log_mh-12.0), 0.15)) # parameters of the underlying physical model params = {&quot;log_mh&quot;: log_mh, &quot;log_c&quot; : log_c} if use_gp: # branch WITH GPs # # define kernel parameters params[&quot;log_amp&quot;] = numpyro.sample(&quot;log_amp&quot;, numpyro.distributions.Uniform(-4.0, 5.0)) params[&quot;log_scl&quot;] = numpyro.sample(&quot;log_scl&quot;, numpyro.distributions.Uniform(-2.0, 3.0)) # generate the GP gp = build_gp(params, t, y_err) # sample the posterior numpyro.sample(&quot;y&quot;, gp.numpyro_dist(), obs=y) # calculate properties of the model mu = gp.mean_function(r_grid) numpyro.deterministic(&quot;mu&quot;, mu) numpyro.deterministic(&quot;gp&quot;, gp.condition(y, r_grid, include_mean=False).gp.loc) else: # branch WITHOUT GPs # # sample the posterior numpyro.sample(&quot;y&quot;, numpyro.distributions.Normal(jax_vhalo(params, t), y_err), obs=y) # calculate properties of the model numpyro.deterministic(&quot;mu&quot;, jax_vhalo(params, r_grid)) . Sampling the posterior with numpyro . The model function above has the numpyro primitives like numpyro.sample that are used by the NUTS MCMC sampler to construct the posterior. Below we run the model the first time selecting the use_gp=False branch, i.e. assuming that the data are independent. . We launch 2 chains for 3000 steps (of which 1/3 of warmup) of the NUTS sampler, starting from a specific value of the parameters that is not too far from the truth (to convieniently speed up convergence). We then use the arviz package to evaluate some statistics on the posterior samples. . sampler_wn = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=0.9, init_strategy=numpyro.infer.init_to_value(values={&#39;log_mh&#39;:11.0, &#39;log_c&#39;:1.0}), ), num_warmup=1000, num_samples=3000, num_chains=2, progress_bar=True, ) %time sampler_wn.run(jax.random.PRNGKey(11), x, e_vm, vm) inf_data_wn = arviz.from_numpyro(sampler_wn) arviz.summary(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;]) . sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11&lt;00:00, 357.51it/s, 3 steps of size 8.43e-01. acc. prob=0.91] sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11&lt;00:00, 357.45it/s, 3 steps of size 7.96e-01. acc. prob=0.93] . CPU times: user 18.3 s, sys: 1.07 s, total: 19.4 s Wall time: 23.4 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.897 | 0.060 | 11.786 | 12.009 | 0.001 | 0.001 | 4581.0 | 3535.0 | 1.0 | . log_c 1.087 | 0.058 | 0.983 | 1.200 | 0.001 | 0.001 | 4758.0 | 3911.0 | 1.0 | . Explore the MCMC samples with arviz . The posterior has been successfully sampled and we can now have a look at the marginalized distributions of the two physical parameters, mass and concentration. . arviz.plot_density(inf_data_wn, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], hdi_prob=0.99, colors=&#39;k&#39;, shade=0.1); . Model with GP . Let&#39;s now repeat the fitting procedure, but this time for the use_gp=True branch of model, i.e. allowing for the data to be correlated. . sampler = numpyro.infer.MCMC( numpyro.infer.NUTS( model, dense_mass=True, target_accept_prob=0.9, init_strategy=numpyro.infer.init_to_value(values={&#39;log_mh&#39;:11.0, &#39;log_c&#39;:1.0, &#39;log_amp&#39;:1.0, &#39;log_scl&#39;:0.5 }), ), num_warmup=1000, num_samples=3000, num_chains=2, progress_bar=True, ) %time sampler.run(jax.random.PRNGKey(11), x, e_vm, vm, use_gp=True) inf_data = arviz.from_numpyro(sampler) arviz.summary(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;, &quot;log_amp&quot;, &quot;log_scl&quot;]) . sample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:21&lt;00:00, 183.03it/s, 7 steps of size 2.41e-01. acc. prob=0.97] sample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14&lt;00:00, 273.18it/s, 31 steps of size 1.56e-01. acc. prob=0.97] . CPU times: user 1min, sys: 2.38 s, total: 1min 2s Wall time: 44.9 s . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . log_mh 11.974 | 0.261 | 11.525 | 12.476 | 0.006 | 0.004 | 2440.0 | 1578.0 | 1.0 | . log_c 0.989 | 0.142 | 0.716 | 1.258 | 0.003 | 0.002 | 2553.0 | 2809.0 | 1.0 | . log_amp 2.723 | 0.647 | 1.929 | 3.712 | 0.039 | 0.027 | 942.0 | 580.0 | 1.0 | . log_scl 0.840 | 0.233 | 0.517 | 1.143 | 0.008 | 0.008 | 1543.0 | 1419.0 | 1.0 | . In this case, we have two additional free parameters that are the amplitude and scale of the GP kernel. These can be considered as nuisance parameters in the present case, since we are only interested in the distributions of the two physical parameters marginalized over everything else. . The chain statistics summarized above look great, but let&#39;s inspect the plot of the autocorrelation time to be extra sure that all the chains are converged and well-behaved. . arviz.plot_autocorr(inf_data, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], max_lag=200); . From this plot we can see that the autocorrelation of both parameters for all chains tends to die out for sufficiently large lags. This confirms that the MCMC samples that we have derived are actually independent and can be reliably used to infer the posterior. . Let&#39;s now compare the marginalized distributions of log_mh and log_c between the two modeling runs. . arviz.plot_density([inf_data_wn, inf_data], var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], data_labels=[&quot;independent data&quot;, &quot;correlated data&quot;], hdi_prob=0.99, colors=[&#39;k&#39;,&#39;C0&#39;], shade=0.1); . We clearly see that if we working under the assumption of uncorrelated data the resulting posteriors are thinner and somewhat biased. On the other hand, by allowing the data to be correlated in the fit, as modelled by a GP, the resulting posterior are significantly wider and more uncertain, but are significantly less biased. . Comparing the 2 models with a corner plot . Let&#39;s have a look at the corner plot of the marginalised posterior distribution in the mass-concentration space. . ranges = [(11.4, 12.6), (0.5, 1.4)] # PRNG 33 fig = corner.corner(inf_data_wn, bins=40, range=ranges, color=&quot;k&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], smooth=1.0, smooth1d=1.0 ) fig = corner.corner(inf_data, bins=40, range=ranges, color=&quot;C0&quot;, var_names=[&quot;log_mh&quot;, &quot;log_c&quot;], smooth=1.0, smooth1d=1.0, labels=[&quot;$ log ,M_h$&quot;, &quot;$ log ,c$&quot;], truths=[params[&#39;log_mh&#39;], params[&#39;log_c&#39;]], truth_color=&#39;tab:pink&#39;, fig=fig) # make legend ax = fig.axes[1] key_tr = matplotlib.lines.Line2D([], [], color=&#39;tab:pink&#39;, linestyle=&#39;-&#39;, marker=&#39;s&#39;, label=&#39;truth&#39;) key_nn = matplotlib.lines.Line2D([], [], color=&#39;k&#39;, linestyle=&#39;-&#39;, label=&#39;independent data&#39;) key_gp = matplotlib.lines.Line2D([], [], color=&#39;C0&#39;, linestyle=&#39;-&#39;, label=&#39;correlated data&#39;) ax.legend(loc=&#39;upper right&#39;, handles=[key_tr, key_nn, key_gp]); . This figure shows more clearly how the first run of the fit, assuming independent data, infers a clearly biased (lower) mass and (higher) concentration. On the other hand, when including a GP in the model to account for correlations in the data the posterior on $M_h$ and $c$ becomes perfectly compatible with the true value within 1-$ sigma$. . Predicted rotation curves of the 2 models . Finally, let&#39;s plot the predicted rotation curves of the two models in comparison with the data. . pred_wn = sampler_wn.get_samples(group_by_chain=True)[&#39;mu&#39;][:,1000:,:].reshape((4000,1000)) pred = sampler.get_samples(group_by_chain=True)[&#39;mu&#39;][:,1000:,:].reshape((4000,1000)) pred_cd = (sampler.get_samples(group_by_chain=True)[&#39;mu&#39;]+ sampler.get_samples(group_by_chain=True)[&#39;gp&#39;])[:,1000:,:].reshape((4000,1000)) # get random subset inds = np.random.randint(0, 2000, 20) . fig,ax = plt.subplots(figsize=(16,6), ncols=3, nrows=2, gridspec_kw={&#39;height_ratios&#39;:(0.65,0.35)}) def commons(ax, i): # rotation curve ax[0,i].plot(x, jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), &#39;--&#39;, c=&#39;tab:pink&#39;, label=&#39;true mean&#39;) ax[0,i].errorbar(x, vm, yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5, label=&#39;data&#39;) ax[0,i].legend(loc=&#39;lower right&#39;) ax[0,i].set_ylabel(&#39;velocity&#39;); # residuals ax[1,i].axhline(y=0, ls=&#39;--&#39;, c=&#39;tab:pink&#39;) ax[1,i].errorbar(x, vm-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, x), yerr=e_vm, fmt=&#39;.&#39;, c=&#39;C0&#39;, lw=0.5) ax[1,i].set_xlabel(&#39;radius&#39;) ax[1,i].set_ylabel(&#39;residuals&#39;) ax[1,i].set_ylim(-60,60); commons(ax,0) commons(ax,1) commons(ax,2) ax[0,0].plot(r_grid, pred_wn[inds].T, &#39;k&#39;, alpha=0.1) ax[0,1].plot(r_grid, pred[inds].T, &#39;C0&#39;, alpha=0.2) ax[0,2].plot(r_grid, pred_cd[inds].T, &#39;C2&#39;, alpha=0.1) ax[1,0].plot(r_grid, (pred_wn[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;k&#39;, alpha=0.1) ax[1,1].plot(r_grid, (pred[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;C0&#39;, alpha=0.2) ax[1,2].plot(r_grid, (pred_cd[inds]-jax_vhalo({&#39;log_mh&#39;:lmh, &#39;log_c&#39;:lc}, r_grid)).T, &#39;C2&#39;, alpha=0.1); ax[0,0].set_title(&quot;Model assuming independent data&quot;); ax[0,1].set_title(&quot;Model with GP: correlated data&quot;); ax[0,2].set_title(&quot;Model with GP &quot;+r&quot;$ rm bf conditioned$&quot;+&quot; to data&quot;); . In the left-hand panels we compare 20 random samples of the predicted rotation curve for the first iteration of the model, i.e. the one treating each datapoint as independent. We see that the model rotation curves (in black) tend to overshoot the mean at small radii and tend to fall below it at large radii - this becomes particularly clear in the residuals plot. The reason for this discrepancy is that this model finds a biased result, predicting a higher concentration and a lower mass than the true values. . The middle panels similarly compare samples of the predictions of the second model, i.e. the one that uses GPs to model correlations among successive datapoints. We see that while the prediction of the rotation curve (in blue) becomes much less accurate, it is now unbiased. In fact, the true mean rotation curve that we used to generate the dataset is very well encompassed by the random samples of this model. . The right panels demonstrate why such a large variety of rotation curve shapes in the blue model is consistent with the dataset. In fact, each blue curve is itself a GP and when conditioning) it to the measurements we obtain the green curves in the right panels, which are all consistent with the data (reduced $ chi^2$ less than unity). .",
            "url": "https://lposti.github.io/MLPages/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes.html",
            "relUrl": "/gaussian_processes/bayesian/jupyter/2022/10/17/gaussian-processes.html",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Variational Autoencoder: learning an underlying distribution and generating new data",
            "content": "Variational AutoEncoder (VAE): an algorithm to work with distributions . This notebook deals with generating an Autoencoder model to learn the underlying distribution of the data. To do this we have to modify the autoencoder such that the encoder does not learn a compressed representation of the input data, but rather it will learn the parameters of the distribution of the data in the latent (compressed) space. . So the idea is to start from an observed sample of the distribution of the data $P({ bf X})$ and to pass this to the encoder which will reduce its dimensionality, i.e. $P({ bf X}) mapsto P&#39;({ bf X}_{ rm c})$ where ${ bf X} in mathrm{R}^m$ and ${ bf X}_{ rm c} in mathrm{R}^n$ with $n&lt;m$. In other words, in a VAE the encoder step does not represent the input data ${ bf X}$ with a code ${ bf X}_{ rm c}$, but rather the initial data distribution $P({ bf X})$ with a compressed distribution $P&#39;({ bf X}_{ rm c})$, which we usually need to approximate in some analytic form, e.g. a multi-variate normal $P&#39;({ bf X}_{ rm c}) sim mathcal{N}( mu, Sigma)$. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random import corner %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . # halo concentration--mass relation def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . . Let&#39;s start again by generating the distribution of physical parameters and calculating the rotation curve of a galaxy with those parameters. This part is taken from the blog post on Autoencoders so I just refer to that for details. . nsamp = 2000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . cm=curveMod(ms,rd,mh,cc) . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . This plot shows a random realization of nsamp rotation curves from our physical model. These curves are the dataset the we are going to use to train our Variational Autoencoder. Let&#39;s start by normalizing the data and defining the training and validation sets. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . /Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/torch/csrc/utils/tensor_new.cpp:204.) &#34;&#34;&#34; . The Variational Autoencoder . We now build the class, deriving from nn.Module, for our Variational AutoEncoder (VAE). In particular, we define the layers in the __init__ method and we define an encoder and a decoder method, just as we did for the Autoencoder. However, this class is quite a lot richer than the one we used for and Autoencoder and we will go through each method below. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Sequential(nn.Linear(in_features=ninp, out_features=32), nn.ReLU()) self.encodeLayer2 = nn.Sequential(nn.Linear(in_features=32, out_features=16), nn.ReLU()) self.encodeOut = nn.Linear(in_features=16, out_features=8) self.decodeLayer1 = nn.Sequential(nn.Linear(in_features=4, out_features=16), nn.ReLU()) self.decodeLayer2 = nn.Sequential(nn.Linear(in_features=16, out_features=32), nn.ReLU()) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar = torch.split(self.encodeOut(self.encodeLayer2(self.encodeLayer1(x))),4,dim=1) return mean, logvar def decoder(self, encoded): return self.decodeOut(self.decodeLayer2(self.decodeLayer1(encoded))) def reparametrize(self, mean, logvar): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) return eps * torch.exp(logvar * 0.5) + mean # exp(0.5logvar) = std # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed # https://arxiv.org/pdf/1312.6114.pdf # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes?noredirect=1&amp;lq=1 def _ELBO(self, x, decoded, mean, logvar): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim = 1) return (KLdiv - logpx_z).mean() def forward(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, logvar) return decoded def getELBO_loss(self, x): mean, logvar = self.encoder(x) z = self.reparametrize(mean, logvar) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, logvar) . Ok, so there&#39;s a lot to break down here! . First of all, we notice that the overall structure of the encoder/decoder network is relatively similar to the autoencoder we saw before, with the important difference that the encoder now returns 8 parameters instead of 4. These are the parameters of the multi-variate normal distribution with which we represent the 4-dimensional latent space, so mean and variance for each of the four physical properties that generate the rotation curves. Thus, the encoder step does not output a code, but means and variances for each physical property. | the decoder step is instead totally similar to a simple autoencoder and in fact it requires a code as an input. In order to generate a code from the means and variances that come out of the encoder phase without breaking the backprop flow of the algorithm, Kingma &amp; Welling (2013) proposed to use a reparametrization trick, which consists of throwing a new sample from a standard normal and then shifting this to have the same mean and variance as given by the encoder. | the forward method of this class follows these steps: the encoder gives means and variances of the latent space, the reparametrization trick is used to generate a code, which is finally decoded by the decoder. | the appropriate loss function for a variational autoencoder is the Evidence Lower BOund (ELBO). In fact, minimising the -ELBO means maximising a lower bound on the evidence or likelihood of the model. The evidence, or reconstruction loss, is logpx_z which is just an MSE loss on the data and the decoded output of the autoencoder. This term encourages the reconstruction of the dataset and tends to prefer separated encodings for each element of the dataset. The other term, KLdiv, is the Kullback-Leibler divergence of the proposed distribution in the latent space with the likelihood. This term has the opposite effect of promoting overlapping encodings for separate observations. For this reason, maximising ELBO guarantees to achieve a nice compromise between representing the original data and the ability to generalize by generating realistic new data. | . With these changes our neural network is now capable of constructing an approximation for the distribution of the four physical parameters in the latent space. We can now run the usual optimization algorithm and start training this model. . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=1e-2) for epoch in range(2000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%100==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.46e+04 valid L:1.76e+04 100 train L:1.29e+03 valid L:3.67e+02 200 train L:5.40e+02 valid L:1.73e+02 300 train L:4.72e+02 valid L:1.39e+02 400 train L:1.42e+02 valid L:6.75e+01 500 train L:1.19e+02 valid L:5.85e+01 600 train L:9.75e+01 valid L:5.35e+01 700 train L:1.05e+02 valid L:5.57e+01 800 train L:7.98e+01 valid L:4.68e+01 900 train L:1.91e+02 valid L:7.53e+01 1000 train L:7.49e+01 valid L:4.26e+01 1100 train L:8.18e+01 valid L:4.40e+01 1200 train L:6.40e+01 valid L:3.92e+01 1300 train L:6.89e+01 valid L:3.86e+01 1400 train L:6.15e+01 valid L:3.69e+01 1500 train L:6.42e+01 valid L:3.66e+01 1600 train L:5.76e+01 valid L:3.55e+01 1700 train L:5.99e+01 valid L:3.60e+01 1800 train L:1.28e+02 valid L:5.11e+01 1900 train L:4.93e+01 valid L:3.26e+01 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The plot above shows the distribution of rotation curves that the VAE has learned. We can see that there is a quite large variety of rotation curve shapes that can be represented by this model. Notice that the region in the radius-velicity space covered by the VAE is indeed quite similar to that of the training set (see plot above). . This shows that the VAE has indeed learned an effective distribution in the latent space which generates the rotation curve dataset we started from. . Exploring the latent space distribution . We can now have a look at what the VAE has learned about the latent space. To do so, we can take the means and variances derived by the encoder on the training set and we can use them to generate samples on the latent space. Basically for each $x_i$ in the training set we get a $ mu_i$ and a $ sigma^2_i$ and we draw 100 samples from $ mathcal{N}( mu_i, sigma^2_i)$. . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . We thus have an array of training_set_size x N_samples = 1600 x 100 = 160000 samples generated from the distribution inferred by the VAE on the latent space. Let&#39;s now plot them (I&#39;m using corner to do so) . rr = ((-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . Here in black we plotted the resulting samples of the latent space as described above, while in orange we overplot just the means $ mu_i$ for each element in the training set. It turns out that the distribution of the means (in orange) is virtually identical to that derived from sampling each multi-variate Gaussian in the latent space (in black). . This implies that the variances $ sigma^2_i$ that are the output of the encoder are generally quite small and that the VAE effectively reconstructs the distribution of the latent space by superimposing many thin Gaussians, all with slightly different mean and small variance. In fact, one could interpret the orange distribution as a superposition of Dirac deltas centered at each mean derived by the encoder on the training set, i.e. $ sum_i delta_i(x- mu_i)$. . Let&#39;s now plot together the physical parameters that we used to generate each rotation curve in the training set, together with the means derived by the encoder step on each element of that set. This allows us to explore whether there are any correlations between the original 4 physical parameters and the 4 dimensions of the latent space constructed by the VAE. . To do this we can stack together the tensor of physical parameters and that of the means. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] mdtrain, mhtrain = mdshuff[:int(nsamp*(1.0-fval))], mhshuff[:int(nsamp*(1.0-fval))] rdtrain, cctrain = rdshuff[:int(nsamp*(1.0-fval))], ccshuff[:int(nsamp*(1.0-fval))] # physical parameters corresponding to each element of the training set partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) # stacking the tensor of phsyical parameters with that of the means derived by the encoder dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr2 = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr2, smooth=0.75, smooth1d=0.75, labels=ll); . Ok, now this corner plot has a lot of information...let&#39;s breakdown the most important ones: . the first 4 blocks of this corner plot are relative to the physical parameters (ms,mh,rd,cc) and show the marginal distribution of each of these parameters and how they are correlated with each other. | the last 4 blocks, instead, are relative to the 4 latent parameters (L1,L2,L3,L4), and again show the marginal distribution of each and their mutual correlations - this is equivalent to the corner plot just above (in that case this was plotted in orange colour) | the 4x4 sub-block on the lower-left corner is possibly the most interesting one as it is the &quot;mixed&quot; block that highlights the relation between the physical and latent parameters. Each of these 16 panels show the correlation of one of the physical parameters with one of the latent ones. We can see that there are several significant correlations (e.g. ms-L1, mh-L2, rd-L3), meaning that these two sets are not independent. | . Generating new realistic data . Now that we have a working approximation of the distribution in the latent space it is easy to use the VAE to generate new rotation curves. We will showcase now a quite simplistic way to do it, which is by assuming that we can represent the latent space with a 4-dimensional Gaussian, even though we have seen in the plots above that the actual distribution is more complex. . We consider the means $ mu_i$ that the encoder derives for the full training set and we take the mean and standard deviation of these. We use these two parameters to define a normal distribution . size=500 dd = torch.distributions.normal.Normal(msm.mean(dim=0), msm.std(dim=0)) new_code = dd.sample(torch.Size([size])) . Let&#39;s make a comparison by plotting the marginalised distributions of the 4 latent parameters with that of the normal distribution that we are assuming . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(rr[0][0],rr[0][1],20), np.linspace(rr[1][0], rr[1][1],20), np.linspace(rr[2][0],rr[2][1],20), np.linspace(rr[3][0], rr[3][1],20)] for i in range(4): ax[i].hist(msm[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;$ mathcal{N}$&#39;+&#39;-approx&#39;); if i==0: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) ax[i].set_xlabel(&#39;L%1d&#39;%(i+1)) . Quite a big difference! . However, this is not an issue since we are just using a normal distribution since it is convenient to sample and for us it is just a means to generate plausible code to be interpreted by the decoder. As a matter of fact, by doing this we are able to generate quite a new variety of possible galaxy rotation curves. . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(vae.decoder(new_code),xmean,xstd)[:100]: ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With a bit on work on the reparametrization trick, we can relax the assumption that the distribution in the latent space is of a multi-variate, but uncorrelated, normal. In practice, instead of having just 4 means and 4 variances as output of the encoder step, we also add 6 covariances, so that we can define the full non-zero covariance matrix of the multi-variate normal in the latent space. . Unfortunately this require quite a bit more math on the reparametrization trick, which is not anymore just $ epsilon* sigma+ mu$, but where the full covariance matrix is used. This calculation requires, among other things, to derive the square root of a matrix, for which we employ the SVD decomposition: if $A = U ,{ rm diag}(s) ,V^{ rm T}$, where $A, U, V in mathbb{R}^{n, n}$, $s in mathbb{R}^n$, and diag$(s)$ is a diagonal matrix with the elements of $s$ as diagonal, then $ sqrt{A} = U ,{ rm diag} left( sqrt{s} right) ,V^{ rm T}$. . class VariationalAutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=14) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) self.ELBO_loss = None def encoder(self, x): mean, logvar, covs = torch.split(self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))), [4, 4, 6], dim=1) return mean, logvar, covs def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def reparametrize(self, mean, m_cov): eps = tensor(rng.normal(size=mean.shape), dtype=torch.float) # return eps * var.sqrt() + mean # find matrix square root with SVD decomposition # https://math.stackexchange.com/questions/3820169/a-is-a-symmetric-positive-definite-matrix-it-has-square-root-using-svd?noredirect=1&amp;lq=1 U,S,V = torch.svd(m_cov) # A = U diag(S) V.T dS = torch.stack([torch.diag(S[i,:]) for i in range(S.shape[0])]) # sqrt(A) = U diag(sqrt(S)) V.T cov_sqrt = torch.einsum(&#39;bij,bkj-&gt;bik&#39;,torch.einsum(&#39;bij,bjk-&gt;bik&#39;,U,dS.sqrt()),V) return torch.einsum(&#39;bij,bi-&gt;bj&#39;, cov_sqrt, eps) + mean def _ELBO(self, x, decoded, mean, m_cov, var): mseloss = nn.MSELoss(reduction=&#39;sum&#39;) logpx_z = -mseloss(x, decoded) KLdiv = -0.5 * (torch.log(m_cov.det()) + 4 - torch.sum(mean**2 + var, dim = 1)) return torch.mean((KLdiv - logpx_z)[~(KLdiv - logpx_z).isnan()]) # torch.nanmean def _get_m_cov(self, logvar, covs): # covariance matrix m_cov = torch.zeros(logvar.shape[0], 4, 4) m_cov[:,[0,1,2,3],[0,1,2,3]] = logvar.exp() m_cov[:,[0,0,0,1,1,2],[1,2,3,2,3,3]] = covs m_cov[:,[1,2,3,2,3,3],[0,0,0,1,1,2]] = covs # var = torch.einsum(&#39;bii-&gt;bi&#39;, m_cov) return m_cov, logvar.exp() def forward(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) self.ELBO_loss = self._ELBO(x, decoded, mean, m_cov, var) return decoded def getELBO_loss(self, x): mean, logvar, covs = self.encoder(x) m_cov, var = self._get_m_cov(logvar, covs) z = self.reparametrize(mean, m_cov) decoded = self.decoder(z) return self._ELBO(x, decoded, mean, m_cov, var) . vae = VariationalAutoEncoder(len(cm.rad)) # Adam and ELBO Loss optimizer = torch.optim.Adam(vae.parameters(), lr=0.4e-2) for epoch in range(1000): ymod = vae.forward(xtrain) loss = vae.ELBO_loss loss.backward() optimizer.step() optimizer.zero_grad() # print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % vae.getELBO_loss(xvalid)) . 0 train L:8.26e+04 valid L:2.21e+04 50 train L:6.80e+03 valid L:1.42e+03 100 train L:1.47e+03 valid L:4.22e+02 150 train L:9.56e+02 valid L:2.77e+02 200 train L:7.22e+02 valid L:1.98e+02 250 train L:5.99e+02 valid L:2.26e+02 300 train L:4.56e+02 valid L:1.53e+02 350 train L:4.18e+02 valid L:1.85e+02 400 train L:3.63e+02 valid L:1.66e+02 450 train L:3.53e+02 valid L:1.57e+02 500 train L:3.33e+02 valid L:1.39e+02 550 train L:4.32e+02 valid L:1.75e+02 600 train L:3.07e+02 valid L:1.39e+02 650 train L:3.10e+02 valid L:1.10e+02 700 train L:2.77e+02 valid L:1.09e+02 750 train L:9.70e+02 valid L:3.07e+02 800 train L:6.58e+02 valid L:3.33e+02 850 train L:4.97e+02 valid L:2.41e+02 900 train L:4.53e+02 valid L:1.82e+02 950 train L:6.56e+02 valid L:3.57e+02 . for v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy()) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . msm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach() # the above are the means and variances obtained by the encoder ns = 100 sss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0] for i in range(1, msm.shape[0]): sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i])) print (sss.shape) . torch.Size([160000, 4]) . rr = ((-25,30),(-40,15),(-30,10),(-20,6)) fig = corner.corner(sss.numpy(), range=rr, hist_kwargs={&quot;density&quot;:True}); fig = corner.corner(msm.numpy(), range=rr, color=&#39;C1&#39;, fig=fig, hist_kwargs={&quot;density&quot;:True}); . dd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy() rr = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-25,30),(-40,15),(-30,10),(-20,6)) ll = [&#39;Mstar&#39;, &#39;Mhalo&#39;, &#39;Rd&#39;, &#39;c&#39;, &#39;L1&#39;, &#39;L2&#39;, &#39;L3&#39;, &#39;L4&#39;] corner.corner(dd, range=rr, smooth=0.75, smooth1d=0.75, labels=ll); .",
            "url": "https://lposti.github.io/MLPages/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves.html",
            "relUrl": "/neural_network/autoencoder/variational%20autoencoder/basics/jupyter/2022/10/07/variational-autoencoder-rotcurves.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Autoencoder represents a multi-dimensional smooth function",
            "content": "Autoencoder: an algorithm to learn a representation of the dataset . In this piece I&#39;m interested in a neural network that is not designed to predict an outcome given a dataset, but rather in an algorithm that is capable of learning an underlying - more compressed - representation of the dataset at hand. This can be used for a number of excitng applications such as data compression, latent representation or recovering the true data distribution. An autoencoder, which is a special encoder/decoder network, is an efficient algorithm that can achieve this. It is divided in two parts: . encoder: the algorithm learns a simple (lower dimensional) representation of the data (code) | decoder: starting from an instance of such representation, code, the algorithm develops it returning the data in the orignal (higher dimensional) form. | . The Autoencoder is then a map $ x longmapsto Psi longmapsto x$, where the first part is the encoder and the second is the decoder; thus it is effectively a map of $x$ onto itself. This implies 1) that we can use the data itself $x$ in the loss function and 2) that the algorithm is learning a representation of the dataset itself. . import numpy as np import matplotlib.pylab as plt from scipy.special import i0, i1, k0, k1 from torch import tensor from torch import nn from torch.nn import functional as F import torch, math import random %config Completer.use_jedi = False %matplotlib inline rng = np.random.default_rng() . . A smooth multi-dimensional function: rotation curve model . To test an autoencoder network we&#39;ll use a simple rotation curve model that is the superposition of two massive components, a disc and an halo. Both components have circular velocity curves that depend on two parameters, a mass and a physical scale, such that the total model has 4 parameters, 2 for each component. . This is a nice case to test how an autoencoder learns an underlying lower dimensional representation of a dataset, since each rotation curve that we will supply to it is actually derived sampling a smooth function of just 4 parameters. . G, H, Dc = 4.301e-9, 70, 200. def fc(x): return np.log(1+x)-x/(1+x) def Vvir(Mh): return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.)) def Rvir(Mh): rho_c = 3. * (H)**2 / (8. * np.pi * G) rho_hat = 4. / 3. * np.pi * Dc * rho_c return 1e3 * np.power(Mh / rho_hat, 1./3.) . . def c(Mh, w_scatter=False, H=70.): if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh))) return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12)) # disc mass--size relation def getRd_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate mass-size relation &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md))) return 10**((np.log10(Md)-10.7)*0.3+0.5) # disc mass--halo mass relation def getMh_fromMd(Md, w_scatter=False): &#39;&#39;&#39; approximate SHMR &#39;&#39;&#39; if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md))) return 10**((np.log10(Md)-10.7)*0.75+12.0) . Sampling uniformly in disc mass, between $10^9$ and $10^{12}$ solar masses, and generating random samples of disc size, halo mass, and halo concentration following the above scaling relations. . nsamp = 1000 ms = 10**rng.uniform(9, 12, nsamp) rd = getRd_fromMd(ms, w_scatter=True) mh = getMh_fromMd(ms, w_scatter=True) cc = c(mh, w_scatter=True) . Above we have generated our latent representation of the dataset, that is each galaxy model is represented by a quadruple (ms,rd,mh,cc). Below we construct a class to generate the rotation curve of the corresponding galaxy model. . class curveMod(): def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)): self.G, self.H, self.Dc = 4.301e-9, 70, 200. # physical constants self.Md, self.Rd = Md, Rd self.Mh, self.cc = Mh, cc self.rad = rad if hasattr(self.Md, &#39;__len__&#39;): self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))] self.vdm = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))] self.vc = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))] else: self.vdisc = self._vdisc(self.rad, self.Md, self.Rd) self.vdm = self._vhalo(self.rad, self.Mh, self.cc) self.vc = np.sqrt(self.vdisc**2+self.vdm**2) def _fc(self, x): return np.log(1+x)-x/(1+x) def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.)) def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.) def _vhalo(self, R, Mh, cc): # circular velocity of the halo component (NFW model) rv = self._Rvir(Mh) return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) def _vdisc(self, R, Md, Rd): # circular velocity of the disc component (exponential disc) y = R/2./Rd return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y)))) . We can now initialize this class with the samples (ms,rd,mh,cc) above, which will store inside the class the rotation curves of all the models - defined on the same radial scale (np.logscale(-1, np.log10(50), 50)) by default. . cm=curveMod(ms,rd,mh,cc) . Let&#39;s plot all the rotation curves together: . for v in cm.vc: plt.plot(cm.rad, v) plt.xlabel(&#39;radius&#39;) plt.ylabel(&#39;velocity&#39;); . The Autoencoder network . We can now build our Autoncoder class deriving from nn.Module. In this example, each rotation curve is a collection of 50 values (see the radial grid in curveMod) and we want to compress it down to a code of just 4 numbers. Notice that we start from the simplified case in which we assume to already know that the ideal latent representation has 4 parameters. . The encoder network has 3 layers, going from the initial $n=50$ to $32$, then to $16$, and finally to $4$. The decoder is symmetric, going from $n=4$ to $16$, then to $32$, and finally to $50$. . class AutoEncoder(nn.Module): def __init__(self, ninp, **kwargs): super().__init__() self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32) self.encodeLayer2 = nn.Linear(in_features=32, out_features=16) self.encodeOut = nn.Linear(in_features=16, out_features=4) self.decodeLayer1 = nn.Linear(in_features=4, out_features=16) self.decodeLayer2 = nn.Linear(in_features=16, out_features=32) self.decodeOut = nn.Linear(in_features=32, out_features=ninp) def encoder(self, x): return self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))) def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded))))) def forward(self, x): encoded = self.encoder(x) decoded = self.decoder(encoded) return decoded . AutoEncoder(len(cm.rad)) . AutoEncoder( (encodeLayer1): Linear(in_features=50, out_features=32, bias=True) (encodeLayer2): Linear(in_features=32, out_features=16, bias=True) (encodeOut): Linear(in_features=16, out_features=4, bias=True) (decodeLayer1): Linear(in_features=4, out_features=16, bias=True) (decodeLayer2): Linear(in_features=16, out_features=32, bias=True) (decodeOut): Linear(in_features=32, out_features=50, bias=True) ) . Data normalization and training/validation split . We now shuffle, normalize, and split the rotation curve dataset into training and validation with a 20% validation split. . def datanorm(x): return (x-x.mean())/x.std(), x.mean(), x.std() def datascale(x, m, s): return x*s+m idshuff = torch.randperm(nsamp) xdata = tensor(cm.vc, dtype=torch.float)[idshuff,:] xdata, xmean, xstd = datanorm(xdata) fval = 0.20 xtrain = xdata[:int(nsamp*(1.0-fval))] xvalid = xdata[int(nsamp*(1.0-fval)):] . Training loop . We now initialize the training loop, defining a simple MSE loss function and a standard Adam optimizer, and we start training . ae = AutoEncoder(len(cm.rad)) # Adam and MSE Loss loss_func = nn.MSELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.Adam(ae.parameters(), lr=0.01) for epoch in range(1001): ymod = ae.forward(xtrain) loss = loss_func(xtrain, ymod) loss.backward() optimizer.step() optimizer.zero_grad() if epoch%50==0: print (epoch, &quot;train L:%1.2e&quot; % loss, &quot; valid L:%1.2e&quot; % loss_func(xvalid, ae.forward(xvalid))) . 0 train L:1.04e+00 valid L:9.36e-01 50 train L:2.77e-02 valid L:2.33e-02 100 train L:5.27e-03 valid L:5.16e-03 150 train L:3.13e-03 valid L:3.33e-03 200 train L:2.59e-03 valid L:2.80e-03 250 train L:2.26e-03 valid L:2.12e-03 300 train L:1.59e-03 valid L:1.63e-03 350 train L:1.04e-03 valid L:1.14e-03 400 train L:9.29e-04 valid L:9.83e-04 450 train L:7.87e-04 valid L:8.51e-04 500 train L:9.03e-04 valid L:1.10e-03 550 train L:6.68e-04 valid L:7.73e-04 600 train L:6.68e-04 valid L:7.54e-04 650 train L:7.83e-04 valid L:7.54e-04 700 train L:6.77e-04 valid L:7.19e-04 750 train L:1.35e-03 valid L:1.92e-03 800 train L:4.56e-04 valid L:5.63e-04 850 train L:4.23e-04 valid L:5.46e-04 900 train L:8.74e-04 valid L:1.07e-03 950 train L:3.84e-04 valid L:4.79e-04 1000 train L:4.45e-04 valid L:4.95e-04 . After 1000 epochs of trainig we get down to a low and stable MSE on the validation set. We can now compare the actual rotation curves in the validation set with those decoded by the model, finding an impressively good match! . fig,ax = plt.subplots(figsize=(12,4), ncols=2) for v in datascale(xvalid,xmean,xstd): ax[0].plot(cm.rad, v) for v in datascale(ae.forward(xvalid),xmean,xstd): ax[1].plot(cm.rad, v.detach().numpy()) ax[0].set_xlabel(&#39;radius&#39;); ax[1].set_xlabel(&#39;radius&#39;) ax[0].set_ylabel(&#39;velocity&#39;); . How does the code correlate with the original 4 physical parameters? . Finally we can explore a bit the properties of the 4 values encoded by the autoencoder and try to understand how do they relate to the original 4 physical parameters. Initially we generated each rotation curve starting from a 4-ple in (ms, mh, rd, cc), where these numbers are generated from some well known scaling relations. We plot the distribution of the initial 4 parameters here: . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp = [ms, mh, rd, cc] for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(np.log10(pp[j]), np.log10(pp[i]), s=2) if j==i: ax[i,j].hist(np.log10(pp[i]), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[1,0].set_ylabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[2,0].set_ylabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); ax[3,0].set_ylabel(&#39;log10(cc)&#39;); . Now we can ask ourselves if similar correlations are observed in the 4 parameters coded by the autoencoder. In principle these are some other 4 numbers that fully specify a single rotation curve model, but that do not necessarily have anything to do with the original (ms, mh, rd, cc). . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;i: ax[i,j].scatter(pp_ae[:,j], pp_ae[:,i], s=2) if j==i: ax[i,j].hist(pp_ae[:,i].detach().numpy(), bins=15, lw=2, histtype=&#39;step&#39;); if j&gt;i: ax[i,j].set_axis_off() . We can see that the coded parameters are indeed strongly correlated among themselves, however it is difficult to draw parallelisms with the behaviour we see in the original 4 parameters. An important difference to notice in these plots is that here we do not use a logarithmic scale for the 4 coded parameters, since they also take negative values unlike (ms, mh, rd, cc). . Let&#39;s now have a look at how the 4 coded parameters are correlated with the original ones. This is interesting since while the 4-dimensional latent space found by the autoencoder is not necessarily the original space of (ms, mh, rd, cc), the 4 new parameters might be well correlated with the 4 original physical quantites. . mdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff] rdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff] ith = int(nsamp*(1.0-fval)) mdtrain, mhtrain = mdshuff[:ith], mhshuff[:ith] rdtrain, cctrain = rdshuff[:ith], ccshuff[:ith] mdvalid, mhvalid = mdshuff[ith:], mhshuff[ith:] rdvalid, ccvalid = rdshuff[ith:], ccshuff[ith:] partrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T) parvalid = (np.vstack([mdvalid, mhvalid, rdvalid, ccvalid]).T) . . Plotting the mutual correalations in the training set we do see that the 4 coded parameters are not at all randomly related to the original physical quantities. . fig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4) pp_ae = ae.encoder(xtrain).detach() for i in range(4): for j in range(4): if j&lt;=i: ax[i,j].scatter(np.log10(partrain[:,j]), pp_ae[:,i], s=2) if j&gt;i: ax[i,j].set_axis_off() ax[3,0].set_xlabel(&#39;log10(ms)&#39;); ax[3,1].set_xlabel(&#39;log10(mh)&#39;); ax[3,2].set_xlabel(&#39;log10(rd)&#39;); ax[3,3].set_xlabel(&#39;log10(cc)&#39;); . Generating new data with the autoencoder . To finish, let&#39;s have a look at how we can use the autoencoder to generate new fake data that resembles our original dataset. We do so by random sampling from the distribution of coded values that we obtained during training. In this way we generate a new plausible code which we then decode to construct a new rotation curve. . We start by generating new code from the distribution obtained during training - to do this we use numpy.random.choice . size=500 new_pp_ae = [] for i in range(4): new_pp_ae.append(tensor(np.random.choice(pp_ae[:,i].numpy(), size))) new_code = torch.stack(new_pp_ae).T . Let&#39;s plot the original and new distributions of coded parameters: . fig,ax = plt.subplots(figsize=(12,3), ncols=4) bins=[np.linspace(-10,10,50), np.linspace(-1,20,50), np.linspace(-2.5,2.5,50), np.linspace(-5,5,50)] for i in range(4): ax[i].hist(pp_ae[:,i].numpy(), bins=bins[i], density=True, label=&#39;data&#39;); ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype=&#39;step&#39;, lw=2, density=True, label=&#39;new code&#39;); if i==1: ax[i].legend(loc=&#39;upper right&#39;, frameon=False) . Since they look very much alike we can now decode the new code that we just generated and we are ready to plot the new rotation curves . fig,ax = plt.subplots(figsize=(6,4)) for v in datascale(ae.decoder(new_code),xmean,xstd): ax.plot(cm.rad, v.detach().numpy()) ax.set_xlabel(&#39;radius&#39;) ax.set_ylabel(&#39;velocity&#39;); . With this method we have effectively generated new rotation curves that are realistic and are not part of the training dataset. This illustrates the power of autoencoders, however to do this even better we can adapt our autoencoder to learn the underlying distribution in the code space - this is what Variatioal Autoencoders (VAEs) are for! .",
            "url": "https://lposti.github.io/MLPages/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves.html",
            "relUrl": "/neural_network/autoencoder/basics/jupyter/2022/06/10/autoencoder-rotcurves.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Ground-up construction of a simple neural network",
            "content": "Linear layers and activation functions from scratch . When approaching the study of a new subject I find it extremely useful to get my hands dirty and play around with the stuff I&#39;m learning, in order to cement the knowledge that I&#39;m passively acquiring reading or listening to a lecture. In the case of deep learning, before starting to use massively the superb python libraries available, e.g. pytorch or fast.ai, I think it&#39;s critical to build a simple NN from scratch. . The bits required are just linear operations, e.g. matrix multiplications, functional composition and the chain rule to get the derivatives during back-propagation. All of this sounds not terrible at all, so we just need a bit of organization to glue all the pieces together. . We take inspiration from the pytorch library and we start by building an abstract Module class. . import numpy as np from torch import tensor from torch import nn import torch, math import random %config Completer.use_jedi = False rng = np.random.default_rng() . . class Module(): &quot;&quot;&quot; abstract class: on call it saves the input and output, and it returns the output &quot;&quot;&quot; def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . When called, Module stores the input and the output items and just returns the output which is defined by the method forward, which needs to be overridden by the derived class. Another method, backward, will have to return the derivative of the function, thus implementing the necessary step for back-propagation. . Let&#39;s now use the class Module to implement a sigmoid activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) class Sigmoid(Module): def forward(self, inp): return sig(inp) def bwd(self, out, inp): inp.g = sig(inp) * (1-sig(inp)) * out.g . Here the class Sigmoid inherits from Module and we just need to specify the forward method, which is just the value of the sigmoid function, and the bwd method, which is what is called by backward. We use bwd to implement the derivative of the sigmoid $$ sigma&#39;(x) = sigma(x) left[1- sigma(x) right], $$ which we store in the .g attribute, that stands for gradient, of the input. This storing the gradient of the class in the .g attribute of the input combined with the last multiplication by out.g that we do in the bwd method is basically the chain rule. The gradient in each layer of an NN is, according to the chain rule, the derivative of the layer times the derivative of the input. Once computed, we store this in the gradient of inp, which is exactly the same variable as out of the previous layer, thus we can reference its gradient with out.g when climbing back the hierarchy of layers. . Similarly, a linear layer $W{ bf x} + b$, where $w$ is a matrix, ${ bf x}$ is a vector and $b$ is a scalar, can be written as: . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) . As before, forward implements the linear layer (@ is the matrix multiplication operator in pytorch) and bwd implements the gradient. The derivative of a matrix multiplication $W{ bf x}$ is just a matrix multiplication by the transpose of the matrix, $W^T$. Since the linear layer has the weights $w$ and bias $b$ parameters that we want to learn, then we need to calculate the gradient of the output of the layer with respect to the weights and the bias. This is what is implemented in self.w.g and self.b.g. . Finally we can define the loss as a class derived from Module as: . class Mse(Module): def forward (self, inp, target): return (inp.squeeze(-1) - target).pow(2).mean() def bwd(self, out, inp, target): inp.g = 2*(inp.squeeze(-1)-target).unsqueeze(-1) / target.shape[0] . This is a mean squared error loss function, $L({ bf y},{ bf y}_{ rm target}) = sum_i (y_i-y_{i, rm target})^2$, where the forward and bwd methods have the same meaning as above. Notice that here the bwd method just stores the inp.g attribute and does not have a multiplication by out.g, because this is the final layer of our NN. . Finally we can bundle everything together in a Model class which takes as input a list of layers and implements a forward method, where maps the input into each layer sequentially, and a backward method, where it goes through the gradient of each layer in reversed order. . class Model(): def __init__(self, layers): self.layers = layers self.loss = Mse() def __call__(self, x, target): return self.forward(x, target) def forward(self, x, target): for l in self.layers: x = l(x) return self.loss(x, target) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Let&#39;s now take some fake data and let&#39;s randomly initialize the weights and biases (unsing standard Xavier initialization so that the output of the layers are still a null mean and unit variance) . n,m = 200,1 x = torch.randn(n,m) y = x.pow(2) nh = 100 # standard xavier init w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . We can now define a model as a sequence of linear and activation layers and we can make a forward pass to calculate the loss... . model = Model([Lin(w1,b1), Sigmoid(), Lin(w2,b2)]) loss = model(x, y) . ...and also a backward pass to calculate the gradients . model.backward() . The architecture above is basically equivalent to an nn.Sequential model . nn.Sequential(nn.Linear(m,nh), nn.Sigmoid(), nn.Linear(nh,1)) . Sequential( (0): Linear(in_features=1, out_features=100, bias=True) (1): Sigmoid() (2): Linear(in_features=100, out_features=1, bias=True) ) .",
            "url": "https://lposti.github.io/MLPages/neural_network/basics/jupyter/2022/03/22/NN-from-scratch.html",
            "relUrl": "/neural_network/basics/jupyter/2022/03/22/NN-from-scratch.html",
            "date": " • Mar 22, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Understanding how basic linear NNs handle non-linearities",
            "content": "How simple neurons handle non-linearities . The Universal approximation Theorem . While studying deep learning, coming from a mathematical physics background, I struggled a lot in understanding how a neural network (NN) can fit non-linear functions. Most of the visual explanations or tutorials I could find on NNs did not give me a satisfactory explanation as to how a composition of linear functions - i.e. an NN - can handle non-linear behaviour. So I looked for more formal derivations, as I knew that the mathematical foundations of deep learning had to be sound. . Eventually I came across one of the most important papers in this field: Cybenko (1989, doi:10.1007/BF02551274). Reading this paper opened my eyes and gave me a completely different perspective on the problem. I realized that the key to the universal approximation theorem is that the composition of a linear function and a sigmoidal (so-called activation) function yields a series of functions which is dense in the space of continuous functions. . In other words, any continuous function can be written as a finite sum of terms given by the composition of a linear and a sigmoidal function, i.e. $$ sum_{i=0}^N alpha_i , sigma({ bf w}_i cdot{ bf x} + b_i), $$ with $ sigma: mathrm{R} to mathrm{R}$ being a sigmoidal activation function, ${ bf x} in mathrm{R}^n$ and ${ bf w}_i in mathrm{R}^n$, $ alpha_i, ,b_i in mathrm{R}$ $ forall i$. Cybenko (1989) showed that the set of functions above spans the whole space of continuous functions in $ mathrm{R}^n$, effectively making this set kind of a basis for $ mathrm{R}^n$, except that the functions are not linearly independent. . Elements of this set of function as in the equation above are usually called units or neurons. . Where does the non-linear behaviour come from . Since a neuron is a composition of a linear function with an activation function, the key to approximate non-linear functions is in the sigmoidal activation function. Formally a sigmoidal is a function $ sigma: mathrm{R} to mathrm{R}$ such that $ lim_{x to+ infty} sigma(x)=1$ and $ lim_{x to- infty} sigma(x)=0$. The Heaviside function is an example of one of the simplest sigmoidal functions; however that is not continuous near $x=0$, thus in practice smooth approximations of it are often used. A popular one is: $$ sigma(x)= frac{1}{1+e^{-x}} $$ . But how can a linear combination of lines composed with a step function approximate any non-linear behaviour? I knew from Cybenko&#39;s results that this had to be the case, so I set out and tried to understand and see this better. . Part 1: How many neurons are needed to approximate a given non-linearity? . I asked myself how many neurons (i.e. elements of Cybenko&#39;s quasi-basis) would I need to approximate a simple second-order non-linearity, i.e. the function $x mapsto x^2$. . import torch import numpy as np import matplotlib.pyplot as plt import torch.nn as nn device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; device = torch.device(device) %matplotlib inline %config Completer.use_jedi = False . . Let&#39;s generate the data of a nice parabolic curve with some small random noise and let&#39;s plot them . size = 500 x = torch.linspace(-5, 5, size) y = x.pow(2) + 0.5 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131837ba8&gt;] . In order to build some intuition of how we may approximate this data with a linear combination of neurons let&#39;s experiment a bit and overplot a few simple combinations of neurons by hand. . First, we need to define our sigmoidal activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) . fig,ax = plt.subplots(figsize=(15,4), ncols=3) def commons(ax): ax.plot(x,y,&#39;k.&#39;, zorder=0) ax.legend(loc=&#39;upper center&#39;) ax.set_ylim((None, 30)) ax[0].plot(x,20*sig(x), lw=3, label=r&quot;$ rm 20 sigma(x)$&quot;) ax[0].plot(x,20*sig(-x), lw=3, label=r&quot;$ rm 20 sigma(-x)$&quot;) ax[0].plot(x,20*(sig(x)+sig(-x)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x)+ sigma(-x)]$&quot;) commons(ax[0]) ax[1].plot(x,20*sig(x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*sig(-x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*(sig(x-3)+sig(-x-3)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x-3)+ sigma(-x-3)]$&quot;) commons(ax[1]) ax[2].plot(x,25*(sig(1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax[2].plot(x,25*(sig(-1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax[2].plot(x,25*(sig(1.2*x-4)+sig(-1.2*x-4)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax[2]) . These examples help in building up some intuition on how we can use these sigmoidal building blocks to approximate relatively simple non linear behaviours. In this specific case of a convex second-order non-linearity, the sum of two scaled, shifted and mirrored step functions seems to yield a decent representation of the data close to the origin. . Dependence on the shape of the activation function . Naturally, this is strongly dependent on the particular shape of the sigmoidal function that we chose, i.e. $ sigma: x mapsto (1+e^{-x})^{-1}$. If we had chosen a Heaviside function instead, the sum of the two neurons above would not have yielded a similarly good approximation of the data. . fig,ax=plt.subplots() ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(-1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)+np.heaviside(-1.2*x-4,0.5)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax) . . NN fitting of $x^2$ function . Now that we have some intuition on the linear combinations of neurons let&#39;s try to answer the question posed at the beginning of Part 1, that is how many neurons are needed to approximate $x^2$. . To answer this we will build a series of simple torch models made up of just one (hidden) layer of neurons, i.e. . nn.Sequential(nn.Linear(1, N_units), nn.Sigmoid(), nn.Linear(N_units, 1)) . We start by defining a learning rate and a number of epochs; then we loop through the 5 numbers of neurons explored, [1,2,4,8,16], we set up the nn.Sequential model and we start the full training loop on the data. We put all of this into a convenient class with a run method and a plots method, which we use to visualize the output. . class ModelsTestingActivations(): def __init__(self, activation_fn=nn.Sigmoid(), loss_fn=nn.MSELoss(reduction=&#39;sum&#39;), units=[1,2,4,8,16], learning_rate=3e-2, num_epochs=1000): self.activ_fn, self.loss_fn = activation_fn, loss_fn self.units, self.lr, self.num_epochs = units, learning_rate, num_epochs # outputs self.models, self.preds = [], [] def make_model(self, u): return nn.Sequential(nn.Linear(in_features=1, out_features=u, bias=True), self.activ_fn, nn.Linear(in_features=u, out_features=1) ) def plots(self, residuals=True, xextrap=10): if not hasattr(self, &#39;x&#39;): print (&#39;Have you run the model yet?&#39;) return fig,ax = plt.subplots(figsize=(18,3.2), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$y$&#39;) ax[i].plot(self.x,self.y,&#39;k.&#39;) ax[i].plot(self.x,self.preds[i],&#39;r.&#39;) ax[i].plot(np.linspace(-xextrap,xextrap), self.models[i](torch.linspace(-xextrap,xextrap,50).unsqueeze(1)).detach(), &#39;b--&#39;) ax[i].text(0.05,0.05,r&quot;N=%d&quot; % self.units[i], transform=ax[i].transAxes, fontsize=14) # residuals if not residuals: return fig,ax = plt.subplots(figsize=(18,1.6), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$ Delta y$&#39;) ax[i].plot(self.x,(self.y-self.preds[i]).abs(), &#39;k-&#39;, lw=0.5) ax[i].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (self.y-self.preds[i]).abs().mean(), transform=ax[i].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;) def run(self, x, y): self.x, self.y = x, y for i,u in enumerate(self.units): # define model model = self.make_model(u) self.models.append(model) # define optimizer optimizer = torch.optim.Adam(model.parameters(), lr=self.lr) # fitting loop for epoch in range(self.num_epochs): # reinitialize gradient of the model weights optimizer.zero_grad() # prediction &amp; loss y_pred = model(self.x.unsqueeze(-1)) loss = self.loss_fn(y_pred, self.y.unsqueeze(-1)) # backpropagation loss.backward() # weight update optimizer.step() self.preds.append(y_pred.squeeze().detach()) . . It is interesting to see how the results of this series of models change depending on which activation function is used, thus we can make our custom class to have as input the shape of the activation function. . We start with the classic sigmoidal . mods_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_sigmoid.run(x, y) . mods_sigmoid.plots() . These plots show how the NN model (red) compares to the actual data used for training (black) as a function of the number (N) of neurons used in the linear layer. Moving from the panels left to right the number of neurons increases from $N=1$ to $N=6$. The dashed blue line is the prediction of the NN model, which we also extrapolated in the range $x in[-10,10]$ outside of the domain of the data, that are confined within $[-5,5]$. The rows of panels below show the residuals $ Delta y=|y-y_{ rm NN}|$, i.e. abs(black-red), while $ langle Delta y rangle$ is the mean of the residuals. . There are a few interesting things that we can notice: . when using only one neuron the NN is able only to capture the mean of the data $ langle y rangle approx 8.613$ | with N=2 neurons, the linear combinations of the two activations is already able to represent the convex 2nd-order non-linearity of the data, reducing the mean residual by an order of magnitude with respect to the model just predicting the mean (i.e. that with N=1). | obviously, increasing N results in a better approximation to the training data, for a fixed learning rate and number of training epochs, up to a residual 4x better with N=6 than with N=2. | the extrapolations of the NN models outside of the domain of the data do not follow the $x^2$ curve at all, showing that the NN models have successfully learned to reproduce the data, but have not learned completely the behaviour of the underlying curve. This exemplifies that NN models are often poor predictors outside of the domain of the training data | . We can have a closer look at the parameters obtained by the NN model with 2 neurons and see how do they compare to the simple fit by eye that we did above. To do this we can grab the named_parameters of the model with $N=2$ and print them out . for name, param in mods_sigmoid.models[1].named_parameters(): if param.requires_grad: print (name, param.data) . 0.weight tensor([[-1.4160], [ 1.4624]]) 0.bias tensor([-4.9017, -4.9822]) 2.weight tensor([[23.6837, 22.9339]]) 2.bias tensor([0.9818]) . Let&#39;s print out explicitly the function found with $N=2$ neurons . def string_func_mod(x): return (&quot;%1.0f sig(%1.1f*x %1.1f) + %1.0f sig(%1.1f*x %1.1f) + %1.1f&quot; % (x[2], x[0], x[1], x[5], x[3], x[4], x[6])) print(string_func_mod(mod2_sigmoid)) . . 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 . Really similar to the parameters we obained by eye! (results may vary a bit due to randomness) . Fit with different activation functions . We can also repeat this exercise with different activation functions, e.g. with a softsign . mods_softsign = ModelsTestingActivations(activation_fn=nn.Softsign(), units=[1,2,3,4,6], learning_rate=9e-2, num_epochs=1000) mods_softsign.run(x, y) . mods_softsign.plots() . or with a ReLU (however, ReLU is not sigmoidal in the sense of Cybenko, so strictly speaking their universal approximation theorem does not apply. It has been shown that the hypothesis on $ sigma$ can be relaxed to it being non-constant, bounded and piecewise continuous, see e.g. Hornik 1991, Leshno et al. 1993) . mods_relu = ModelsTestingActivations(activation_fn=nn.ReLU(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_relu.run(x, y) . mods_relu.plots() . While the accuracy of the NN with ReLU, as measured by $ langle Delta y rangle$, is not significantly better than with Sigmoid for the sake of this experiment, the extrapolation out-of-domain is much better. This is because the NN model with ReLU tends to linear outside of the data domain, while the NN model with Sigmoid is approximately constant outside of the range of the training data. . Part 2: How does this compares with a Bayesian likelihood estimator . It would be interesting to fully reconstruct the likelihood distribution of the parameters of the NN model. If the number of parameters is not huge - that is if we are working with a limited number of neurons in a single layer - then an exploration of the multi-dimensional likelihood distribution is still feasible. Moreover, if we are able to map the full likelihood of the model parameters we can also see where the best model found by the NN sits in the space of the likelihood. . To do this we can use a Monte Carlo Markov Chain (MCMC) analysis. This is actually what I would normally do when facing an optimization problem in physics, as often one can have a pretty good guess on a suitable functional form to use for the fitting function and, more importantly, this method naturally allows to study the uncertainties on the model found. . We&#39;re using the library emcee (paper) to run the MCMC analysis and corner (paper) to plot the posterior distribution. . import emcee import corner # the functional definition of the NN model def lmod(x, pars): &quot;&quot;&quot; A linear combination of nu sigmoidals composed with linear functions &quot;&quot;&quot; nu = int((len(pars)-1)/3) # number of neurons, with 2 biases res = pars[-1] for i in range(nu): res += sig(pars[0+i*3]*x+pars[1+i*3])*pars[2+i*3] return res # log-likelihood def lnlike(pars, x, y): &quot;&quot;&quot; This is equivalent to MSELoss(reduction=&#39;sum&#39;) &quot;&quot;&quot; y_lmod = lmod(x,pars) return -((y-y_lmod).pow(2).sum() / len(y)).item() # log-prior on the parameters def lnprior(pars): &quot;&quot;&quot; A multi-dimensional Gaussian prior with null mean, fixed std and null correlation &quot;&quot;&quot; std = 30 lp = 0. for p in pars: lp += -0.5*(p)**2/std**2-np.log(np.sqrt(2*np.pi)*std) return lp # log-probability = log-likelihood + log-prior def lnprob(pars, x, y): lk, lp = lnlike(pars, x, y), lnprior(pars) if not np.isfinite(lp) or not np.isfinite(lk): return -np.inf return lp + lk . . Let&#39;s run the MCMC analysis for a $N=2$ NN (see here for a bit more context on MCMCs with emcee) . nunits = 2 dim = 3*nunits+1 . %%time nu, nw, nstep = 2, 4*dim, 10000 # initial conditions of each chain pos = [[0]*dim + 1e-4*np.random.randn(dim) for j in range(nw)] # launch the MCMC sampler = emcee.EnsembleSampler(nw, dim, lnprob, args=(x.squeeze(), y.squeeze())) sampler.run_mcmc(pos, nstep); # collate the chains of each walker and remove the first 500 steps - the burn-in phase samples = sampler.chain[:,500:,:].reshape((-1, dim)) . /Users/lposti/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp &#34;&#34;&#34;Entry point for launching an IPython kernel. . CPU times: user 1min 23s, sys: 988 ms, total: 1min 24s Wall time: 1min 38s . Finally, let&#39;s plot the posterior probability distribution of the 7 parameters of a NN model with $N=2$ neurons and let&#39;s also mark the location of the model we obtained above with nn.Sequential . corner.corner(samples, bins=30, smooth=1.5, smooth1d=1.5, truths=mod2_sigmoid); . Find the maximum probability model, i.e. the model with highest log-prob . idmax = np.unravel_index(sampler.lnprobability.argmax(), sampler.lnprobability.shape) # maximum probability max_prob = sampler.chain[idmax[0],idmax[1],:] . print (&#39;NN model:&#39;) print (string_func_mod(mod2_sigmoid), &#39; LOSS:%1.2f&#39; % lnlike(mod2_sigmoid, x, y)) print () print (&#39;MCMC model&#39;) print (string_func_mod(max_prob), &#39; LOSS:%1.2f&#39; % lnlike(max_prob, x, y)) . NN model: 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 LOSS:-0.73 MCMC model 28 sig(-1.2*x -4.3) + 31 sig(1.0*x -3.9) + -0.2 LOSS:-0.31 . and we can also plot them side by side in comparison to the data . fig,ax = plt.subplots(figsize=(9,4.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) ax.plot(x,y,&#39;ko&#39;) ax.set_ylim((-1,32)) commons(ax[0]) ax[0].set_title(&quot;NN best model&quot;, fontsize=16) ax[0].plot(x, lmod(x, mod2_sigmoid), &#39;r.&#39;) ax[0].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), mod2_sigmoid), &#39;b--&#39;) commons(ax[1]) ax[1].set_title(&quot;MCMC max prob&quot;, fontsize=16) ax[1].plot(x, lmod(x, max_prob), &#39;r.&#39;) ax[1].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), max_prob), &#39;b--&#39;) fig,ax = plt.subplots(figsize=(9,2.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$ Delta y$&#39;) ax.set_ylim((-0.1,3)) commons(ax[0]) ax[0].plot(x, (y-lmod(x, mod2_sigmoid)).abs(), &#39;k-&#39;, lw=0.5) ax[0].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, mod2_sigmoid)).abs().mean(), transform=ax[0].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); commons(ax[1]) ax[1].plot(x, (y-lmod(x, max_prob)).abs(), &#39;k-&#39;, lw=0.5) ax[1].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, max_prob)).abs().mean(), transform=ax[1].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); . Part 3: Dealing with more complex non-linearities . Let&#39;s repeat the analysis of Part 1 but for a more complex non-linear function, for instance a sin function with an oscillating non-linear behaviour. Let&#39;s now ask ourselves how many neurons would you need to fit this function. . As before, let&#39;s start by generating some data . size = 1000 x = torch.linspace(-10, 10, size) y = torch.sin(x) + 0.2 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131f3c4a8&gt;] . Now we can use the same ModelsTestingActivations class that we wrote above, just passing the new xs and ys to the run method. Let&#39;s use a Sigmoid activation function and let&#39;s have a look at the performance of the NN models for increasing number of neurons $N$ . mods_sin_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=4e-2, num_epochs=2000) mods_sin_sigmoid.run(x, y) . mods_sin_sigmoid.plots(xextrap=20) . From these plots we can clearly notice a few important things: . the number of neurons limits the number turnovers (i.e. changes of sign of the derivative) of the function that can be fitted | an NN model with $N$ neurons is generally limited to approximate decently only function that change their increasing/decreasing tendency $N$ times | in this particular example, the sin function turnsover 6 times in the interval $[-10,10]$, thus an NN with at least $N=6$ neurons is needed to capture all the times the data turnover | also in this case, the extrapolation of the NN models outside of the domain of the data yield poor predictions. This means that the NN has learned to reproduce the data, but has not learned the underlying functional behaivour | .",
            "url": "https://lposti.github.io/MLPages/neural_network/basics/bayesian/mcmc/jupyter/2022/03/18/neurons-non-linearities.html",
            "relUrl": "/neural_network/basics/bayesian/mcmc/jupyter/2022/03/18/neurons-non-linearities.html",
            "date": " • Mar 18, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Lorenzo Posti, currently a postdoctoral researcher at the Astronomical Observatory of Strasbourg (France). Before that, I&#39;ve been doing research in Astrophysics in Groningen (Netherlands), Bologna (Italy), Oxford (UK) and Baltimore (USA). . You can find my updated list of publications on ADS. I used to maintain a personal website with some details on my research, however I stopped doing that for various reasons. An archived and stripped-down version can be found here. . You can reach out to me at lorenzo.posti@gmail.com. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lposti.github.io/MLPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lposti.github.io/MLPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}