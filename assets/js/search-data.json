{
  
    
        "post0": {
            "title": "Understanding how basic linear NNs handle non-linearities",
            "content": "How simple neurons handle non-linearities . The Universal approximation Theorem . While studying deep learning, coming from a mathematical physics background, I struggled a lot in understanding how a neural network (NN) can fit non-linear functions. Most of the visual explanations or tutorials I could find on NNs did not give me a satisfactory explanation as to how a composition of linear functions - i.e. an NN - can handle non-linear behaviour. So I looked for more formal derivations, as I knew that the mathematical foundations of deep learning had to be sound. . Eventually I came across one of the most important papers in this field: Cybenko (1989, doi:10.1007/BF02551274). Reading this paper opened my eyes and gave me a completely different perspective on the problem. I realized that the key to the universal approximation theorem is that the composition of a linear function and a sigmoidal (so-called activation) function yields a series of functions which is dense in the space of continuous functions. . In other words, any continuous function can be written as a finite sum of terms given by the composition of a linear and a sigmoidal function, i.e. $$ sum_{i=0}^N alpha_i , sigma({ bf w}_i cdot{ bf x} + b_i), $$ with $ sigma: mathrm{R} to mathrm{R}$ being a sigmoidal activation function, ${ bf x} in mathrm{R}^n$ and ${ bf w}_i in mathrm{R}^n$, $ alpha_i, ,b_i in mathrm{R}$ $ forall i$. Cybenko (1989) showed that the set of functions above spans the whole space of continuous functions in $ mathrm{R}^n$, effectively making this set kind of a basis for $ mathrm{R}^n$, except that the functions are not linearly independent. . Elements of this set of function as in the equation above are usually called units or neurons. . Where does the non-linear behaviour come from . Since a neuron is a composition of a linear function with an activation function, the key to approximate non-linear functions is in the sigmoidal activation function. Formally a sigmoidal is a function $ sigma: mathrm{R} to mathrm{R}$ such that $ lim_{x to+ infty} sigma(x)=1$ and $ lim_{x to- infty} sigma(x)=0$. The Heaviside function is an example of one of the simplest sigmoidal functions; however that is not continuous near $x=0$, thus in practice smooth approximations of it are often used. A popular one is: $$ sigma(x)= frac{1}{1+e^{-x}} $$ . But how can a linear combination of lines composed with a step function approximate any non-linear behaviour? I knew from Cybenko&#39;s results that this had to be the case, so I set out and tried to understand and see this better. . Part 1: How many neurons are needed to approximate a given non-linearity? . I asked myself how many neurons (i.e. elements of Cybenko&#39;s quasi-basis) would I need to approximate a simple second-order non-linearity, i.e. the function $x mapsto x^2$. . import torch import numpy as np import matplotlib.pyplot as plt import torch.nn as nn device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; device = torch.device(device) %matplotlib inline %config Completer.use_jedi = False . . Let&#39;s generate the data of a nice parabolic curve with some small random noise and let&#39;s plot them . size = 500 x = torch.linspace(-5, 5, size) y = x.pow(2) + 0.5 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131837ba8&gt;] . In order to build some intuition of how we may approximate this data with a linear combination of neurons let&#39;s experiment a bit and overplot a few simple combinations of neurons by hand. . First, we need to define our sigmoidal activation function: . sig = lambda x: 1.0/(1.0+np.exp(-x)) . fig,ax = plt.subplots(figsize=(15,4), ncols=3) def commons(ax): ax.plot(x,y,&#39;k.&#39;, zorder=0) ax.legend(loc=&#39;upper center&#39;) ax.set_ylim((None, 30)) ax[0].plot(x,20*sig(x), lw=3, label=r&quot;$ rm 20 sigma(x)$&quot;) ax[0].plot(x,20*sig(-x), lw=3, label=r&quot;$ rm 20 sigma(-x)$&quot;) ax[0].plot(x,20*(sig(x)+sig(-x)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x)+ sigma(-x)]$&quot;) commons(ax[0]) ax[1].plot(x,20*sig(x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*sig(-x-3), lw=3, label=r&quot;$ rm 20 sigma(x-3)$&quot;) ax[1].plot(x,20*(sig(x-3)+sig(-x-3)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 20[ sigma(x-3)+ sigma(-x-3)]$&quot;) commons(ax[1]) ax[2].plot(x,25*(sig(1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax[2].plot(x,25*(sig(-1.2*x-4)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax[2].plot(x,25*(sig(1.2*x-4)+sig(-1.2*x-4)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax[2]) . These examples help in building up some intuition on how we can use these sigmoidal building blocks to approximate relatively simple non linear behaviours. In this specific case of a convex second-order non-linearity, the sum two scaled, shifted and mirrored step functions seems to yield a decent representation of the data close to the origin. . Dependence on the shape of the activation function . Naturally, this is strongly dependent on the particular shape of the sigmoidal function that we chose, i.e. $ sigma: x mapsto (1+e^{-x})^{-1}$. If we had chosen a Heaviside function instead, the sum of the two neurons above would not have yielded a similarly good approximation of the data. . fig,ax=plt.subplots() ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(-1.2*x-4,0.5)), lw=3, label=r&quot;$ rm 25 sigma(-1.2x-4)$&quot;) ax.plot(x,25*(np.heaviside(1.2*x-4,0.5)+np.heaviside(-1.2*x-4,0.5)), ls=&#39;--&#39;, c=&#39;tab:green&#39;, label=r&quot;$ rm 25[ sigma(1.2x-4)+ sigma(-1.2x-4)]$&quot;) commons(ax) . . NN fitting of $x^2$ function . Now that we have some intuition on the linear combinations of neurons let&#39;s try to answer the question posed at the beginning of Part 1, that is how many neurons are needed to approximate $x^2$. . To answer this we will build a series of simple torch models made up of just one (hidden) layer of neurons, i.e. . nn.Sequential(nn.Linear(1, N_units), nn.Sigmoid(), nn.Linear(N_units, 1)) . We start by defining a learning rate and a number of epochs; then we loop through the 5 numbers of neurons explored, [1,2,4,8,16], we set up the nn.Sequential model and we start the full training loop on the data. We put all of this into a convenient class with a run method and a plots method, which we use to visualize the output. . class ModelsTestingActivations(): def __init__(self, activation_fn=nn.Sigmoid(), loss_fn=nn.MSELoss(reduction=&#39;sum&#39;), units=[1,2,4,8,16], learning_rate=3e-2, num_epochs=1000): self.activ_fn, self.loss_fn = activation_fn, loss_fn self.units, self.lr, self.num_epochs = units, learning_rate, num_epochs # outputs self.models, self.preds = [], [] def make_model(self, u): return nn.Sequential(nn.Linear(in_features=1, out_features=u, bias=True), self.activ_fn, nn.Linear(in_features=u, out_features=1) ) def plots(self, residuals=True, xextrap=10): if not hasattr(self, &#39;x&#39;): print (&#39;Have you run the model yet?&#39;) return fig,ax = plt.subplots(figsize=(18,3.2), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$y$&#39;) ax[i].plot(self.x,self.y,&#39;k.&#39;) ax[i].plot(self.x,self.preds[i],&#39;r.&#39;) ax[i].plot(np.linspace(-xextrap,xextrap), self.models[i](torch.linspace(-xextrap,xextrap,50).unsqueeze(1)).detach(), &#39;b--&#39;) ax[i].text(0.05,0.05,r&quot;N=%d&quot; % self.units[i], transform=ax[i].transAxes, fontsize=14) # residuals if not residuals: return fig,ax = plt.subplots(figsize=(18,1.6), ncols=len(self.units)) for i in range(len(self.units)): ax[i].set_xlabel(r&#39;$x$&#39;) if i==0: ax[i].set_ylabel(r&#39;$ Delta y$&#39;) ax[i].plot(self.x,(self.y-self.preds[i]).abs(), &#39;k-&#39;, lw=0.5) ax[i].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (self.y-self.preds[i]).abs().mean(), transform=ax[i].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;) def run(self, x, y): self.x, self.y = x, y for i,u in enumerate(self.units): # define model model = self.make_model(u) self.models.append(model) # define optimizer optimizer = torch.optim.Adam(model.parameters(), lr=self.lr) # fitting loop for epoch in range(self.num_epochs): # reinitialize gradient of the model weights optimizer.zero_grad() # prediction &amp; loss y_pred = model(self.x.unsqueeze(-1)) loss = self.loss_fn(y_pred, self.y.unsqueeze(-1)) # backpropagation loss.backward() # weight update optimizer.step() self.preds.append(y_pred.squeeze().detach()) . . It is interesting to see how the results of this series of models change depending on which activation function is used, thus we can make our custom class to have as input the shape of the activation function. . We start with the classic sigmoidal . mods_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_sigmoid.run(x, y) . mods_sigmoid.plots() . These plots show how the NN model (red) compares to the actual data used for training (black) as a function of the number (N) of neurons used in the linear layer. Moving from the panels left to right the number of neurons increases from $N=1$ to $N=6$. The dashed blue line is the prediction of the NN model, which we also extrapolated in the range $x in[-10,10]$ outside of the domain of the data, that are confined within $[-5,5]$. The rows of panels below show the residuals $ Delta y=|y-y_{ rm NN}|$, i.e. abs(black-red), while $ langle Delta y rangle$ is the mean of the residuals. . There are a few interesting things that we can notice: . when using only one neuron the NN is able only to capture the mean of the data $ langle y rangle approx 8.613$ | with N=2 neurons, the linear combinations of the two activations is already able to represent the convex 2nd-order non-linearity of the data, reducing the mean residual by an order of magnitude with respect to the model just predicting the mean (i.e. that with N=1). | obviously, increasing N results in a better approximation to the training data, for a fixed learning rate and number of training epochs, up to a residual 4x better with N=6 than with N=2. | the extrapolations of the NN models outside of the domain of the data do not follow the $x^2$ curve at all, showing that the NN models have successfully learned to reproduce the data, but have not learned completely the behaviour of the underlying curve. This exemplifies that NN models are often poor predictors outside of the domain of the training data | . We can have a closer look at the parameters obtained by the NN model with 2 neurons and see how do they compare to the simple fit by eye that we did above. To do this we can grab the named_parameters of the model with $N=2$ and print them out . for name, param in mods_sigmoid.models[1].named_parameters(): if param.requires_grad: print (name, param.data) . 0.weight tensor([[-1.4160], [ 1.4624]]) 0.bias tensor([-4.9017, -4.9822]) 2.weight tensor([[23.6837, 22.9339]]) 2.bias tensor([0.9818]) . Let&#39;s print out explicitly the function found with $N=2$ neurons . def string_func_mod(x): return (&quot;%1.0f sig(%1.1f*x %1.1f) + %1.0f sig(%1.1f*x %1.1f) + %1.1f&quot; % (x[2], x[0], x[1], x[5], x[3], x[4], x[6])) print(string_func_mod(mod2_sigmoid)) . . 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 . Really similar to the parameters we obained by eye! (results may vary a bit due to randomness) . Fit with different activation functions . We can also repeat this exercise with different activation functions, e.g. with a softsign . mods_softsign = ModelsTestingActivations(activation_fn=nn.Softsign(), units=[1,2,3,4,6], learning_rate=9e-2, num_epochs=1000) mods_softsign.run(x, y) . mods_softsign.plots() . or with a ReLU (however, ReLU is not sigmoidal in the sense of Cybenko, so strictly speaking their universal approximation theorem does not apply. It has been shown that the hypothesis on $ sigma$ can be relaxed to it being non-constant, bounded and piecewise continuous, see e.g. Hornik 1991, Leshno et al. 1993) . mods_relu = ModelsTestingActivations(activation_fn=nn.ReLU(), units=[1,2,3,4,6], learning_rate=5e-2, num_epochs=2000) mods_relu.run(x, y) . mods_relu.plots() . While the accuracy of the NN with ReLU, as measured by $ langle Delta y rangle$, is not significantly better than with Sigmoid for the sake of this experiment, the extrapolation out-of-domain is much better. This is because the NN model with ReLU tends to linear outside of the data domain, while the NN model with Sigmoid is approximately constant outside of the range of the training data. . Part 2: How does this compares with a Bayesian likelihood estimator . It would be interesting to fully reconstruct the likelihood distribution of the parameters of the NN model. If the number of parameters is not huge - that is if we are working with a limited number of neurons in a single layer - then an exploration of the multi-dimensional likelihood distribution is still feasible. Moreover, if we are able to map the full likelihood of the model parameters we can also see where the best model found by the NN sits in the space of the likelihood. . To do this we can use a Monte Carlo Markov Chain (MCMC) analysis. This is actually what I would normally do when facing an optimization problem in physics, as often one can have a pretty good guess on a suitable functional form to use for the fitting function and, more importantly, this method naturally allows to study the uncertainties on the model found. . We&#39;re using the library emcee (paper) to run the MCMC analysis and corner (paper) to plot the posterior distribution. . import emcee import corner # the functional definition of the NN model def lmod(x, pars): &quot;&quot;&quot; A linear combination of nu sigmoidals composed with linear functions &quot;&quot;&quot; nu = int((len(pars)-1)/3) # number of neurons, with 2 biases res = pars[-1] for i in range(nu): res += sig(pars[0+i*3]*x+pars[1+i*3])*pars[2+i*3] return res # log-likelihood def lnlike(pars, x, y): &quot;&quot;&quot; This is equivalent to MSELoss(reduction=&#39;sum&#39;) &quot;&quot;&quot; y_lmod = lmod(x,pars) return -((y-y_lmod).pow(2).sum() / len(y)).item() # log-prior on the parameters def lnprior(pars): &quot;&quot;&quot; A multi-dimensional Gaussian prior with null mean, fixed std and null correlation &quot;&quot;&quot; std = 30 lp = 0. for p in pars: lp += -0.5*(p)**2/std**2-np.log(np.sqrt(2*np.pi)*std) return lp # log-probability = log-likelihood + log-prior def lnprob(pars, x, y): lk, lp = lnlike(pars, x, y), lnprior(pars) if not np.isfinite(lp) or not np.isfinite(lk): return -np.inf return lp + lk . . Let&#39;s run the MCMC analysis for a $N=2$ NN (see here for a bit more context on MCMCs with emcee) . nunits = 2 dim = 3*nunits+1 . %%time nu, nw, nstep = 2, 4*dim, 10000 # initial conditions of each chain pos = [[0]*dim + 1e-4*np.random.randn(dim) for j in range(nw)] # launch the MCMC sampler = emcee.EnsembleSampler(nw, dim, lnprob, args=(x.squeeze(), y.squeeze())) sampler.run_mcmc(pos, nstep); # collate the chains of each walker and remove the first 500 steps - the burn-in phase samples = sampler.chain[:,500:,:].reshape((-1, dim)) . /Users/lposti/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp &#34;&#34;&#34;Entry point for launching an IPython kernel. . CPU times: user 1min 23s, sys: 988 ms, total: 1min 24s Wall time: 1min 38s . Finally, let&#39;s plot the posterior probability distribution of the 7 parameters of a NN model with $N=2$ neurons and let&#39;s also mark the location of the model we obtained above with nn.Sequential . corner.corner(samples, bins=30, smooth=1.5, smooth1d=1.5, truths=mod2_sigmoid); . Find the maximum probability model, i.e. the model with highest log-prob . idmax = np.unravel_index(sampler.lnprobability.argmax(), sampler.lnprobability.shape) # maximum probability max_prob = sampler.chain[idmax[0],idmax[1],:] . print (&#39;NN model:&#39;) print (string_func_mod(mod2_sigmoid), &#39; LOSS:%1.2f&#39; % lnlike(mod2_sigmoid, x, y)) print () print (&#39;MCMC model&#39;) print (string_func_mod(max_prob), &#39; LOSS:%1.2f&#39; % lnlike(max_prob, x, y)) . NN model: 24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0 LOSS:-0.73 MCMC model 28 sig(-1.2*x -4.3) + 31 sig(1.0*x -3.9) + -0.2 LOSS:-0.31 . and we can also plot them side by side in comparison to the data . fig,ax = plt.subplots(figsize=(9,4.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$y$&#39;) ax.plot(x,y,&#39;ko&#39;) ax.set_ylim((-1,32)) commons(ax[0]) ax[0].set_title(&quot;NN best model&quot;, fontsize=16) ax[0].plot(x, lmod(x, mod2_sigmoid), &#39;r.&#39;) ax[0].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), mod2_sigmoid), &#39;b--&#39;) commons(ax[1]) ax[1].set_title(&quot;MCMC max prob&quot;, fontsize=16) ax[1].plot(x, lmod(x, max_prob), &#39;r.&#39;) ax[1].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), max_prob), &#39;b--&#39;) fig,ax = plt.subplots(figsize=(9,2.), ncols=2) def commons(ax): ax.set_xlabel(r&#39;$x$&#39;) ax.set_ylabel(r&#39;$ Delta y$&#39;) ax.set_ylim((-0.1,3)) commons(ax[0]) ax[0].plot(x, (y-lmod(x, mod2_sigmoid)).abs(), &#39;k-&#39;, lw=0.5) ax[0].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, mod2_sigmoid)).abs().mean(), transform=ax[0].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); commons(ax[1]) ax[1].plot(x, (y-lmod(x, max_prob)).abs(), &#39;k-&#39;, lw=0.5) ax[1].text(0.5,0.85, r&quot;$ rm langle Delta y rangle=%1.2f$&quot; % (y-lmod(x, max_prob)).abs().mean(), transform=ax[1].transAxes, fontsize=12, ha=&#39;center&#39;, va=&#39;center&#39;); . Part 3: Dealing with more complex non-linearities . Let&#39;s repeat the analysis of Part 1 but for a more complex non-linear function, for instance a sin function with an oscillating non-linear behaviour. Let&#39;s now ask ourselves how many neurons would you need to fit this function. . As before, let&#39;s start by generating some data . size = 1000 x = torch.linspace(-10, 10, size) y = torch.sin(x) + 0.2 * torch.rand_like(x) plt.plot(x,y,&#39;k.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7ff131f3c4a8&gt;] . Now we can use the same ModelsTestingActivations class that we wrote above, just passing the new xs and ys to the run method. Let&#39;s use a Sigmoid activation function and let&#39;s have a look at the performance of the NN models for increasing number of neurons $N$ . mods_sin_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=4e-2, num_epochs=2000) mods_sin_sigmoid.run(x, y) . mods_sin_sigmoid.plots(xextrap=20) . From these plots we can clearly notice a few important things: . the number of neurons limits the number turnovers (i.e. changes of sign of the derivative) of the function that can be fitted | an NN model with $N$ neurons is generally limited to approximate decently only function that change their increasing/decreasing tendency $N$ times | in this particular example, the sin function turnsover 6 times in the interval $[-10,10]$, thus an NN with at least $N=6$ neurons is needed to capture all the times the data turnover | also in this case, the extrapolation of the NN models outside of the domain of the data yield poor predictions. This means that the NN has learned to reproduce the data, but has not learned the underlying functional behaivour | .",
            "url": "https://lposti.github.io/MLPages/neural%20network/basics/bayesian/mcmc/jupyter/2022/03/17/neurons_non-linearities.html",
            "relUrl": "/neural%20network/basics/bayesian/mcmc/jupyter/2022/03/17/neurons_non-linearities.html",
            "date": " • Mar 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lposti.github.io/MLPages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lposti.github.io/MLPages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lposti.github.io/MLPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lposti.github.io/MLPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}