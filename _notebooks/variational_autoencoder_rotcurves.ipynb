{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1f97a0",
   "metadata": {},
   "source": [
    "# \"Variational Autoencoder: learning an underlying distribution and generating new data\"\n",
    "> \"Constructing an autoencoder that learns the underlying distribution of the input data, generated from a multi-dimensional smooth function `f=f(x_1,x_2,x_3,x_4)`. This can be used to generate new data, sampling from the learned distribution\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Lorenzo Posti\n",
    "- categories: [neural network, autoencoder, variational autoencoder, basics, jupyter]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49dbe40",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder (VAE): an algorithm to work with distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8822d32",
   "metadata": {},
   "source": [
    "This notebook deals with generating an `Autoencoder` model to learn the underlying distribution of the data. To do this we have to modify the autoencoder such that the `encoder` does not learn a compressed representation of the input data, but rather it will learn the parameters of the distribution of the data in the latent (compressed) space. \n",
    "\n",
    "So the idea is to start from an observed sample of the distribution of the data $P({\\bf X})$ and to pass this to the `encoder` which will reduce its dimensionality, i.e. $P({\\bf X})\\mapsto P({\\bf X}_{\\rm c})$ where ${\\bf X}\\in\\mathrm{R}^m$ and ${\\bf X}_{\\rm c}\\in\\mathrm{R}^n$ with $n<m$. In other words, in a VAE the `encoder` step does not represent the input data ${\\bf X}$ with a `code` ${\\bf X}_{\\rm c}$, but rather the initial data distribution $P({\\bf X})$ with a compressed distribution $P({\\bf X}_{\\rm c})$, which we usually need to approximate in some analytic form, e.g. a multi-variate normal $P({\\bf X}_{\\rm c})\\sim \\mathcal{N}(\\mu,\\Sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6306b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da66e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cbd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.special import i0, i1, k0, k1\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch, math\n",
    "import random\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb42e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a997855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d7e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, ninp, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32)\n",
    "        self.encodeLayer2 = nn.Linear(in_features=32,   out_features=16)\n",
    "        self.encodeOut    = nn.Linear(in_features=16,   out_features=8)\n",
    "        self.decodeLayer1 = nn.Linear(in_features=4,    out_features=16)\n",
    "        self.decodeLayer2 = nn.Linear(in_features=16,   out_features=32)\n",
    "        self.decodeOut    = nn.Linear(in_features=32,   out_features=ninp)\n",
    "        self.ELBO_loss = None\n",
    "        \n",
    "    def encoder(self, x):       \n",
    "        mean, logvar = torch.split(self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))),4,dim=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded)))))\n",
    "    \n",
    "    def reparametrize(self, mean, logvar):\n",
    "        eps = tensor(rng.normal(size=mean.shape), dtype=torch.float)\n",
    "        return eps * torch.exp(logvar * 0.5) + mean # exp(0.5logvar) = std\n",
    "    \n",
    "    # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "    # https://arxiv.org/pdf/1312.6114.pdf\n",
    "    # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes?noredirect=1&lq=1\n",
    "    def _ELBO(self, x, decoded, mean, logvar):\n",
    "        mseloss = nn.MSELoss(reduction='mean')\n",
    "        logpx_z = -mseloss(x, decoded)\n",
    "        KLdiv = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim = 1)\n",
    "        return (KLdiv - logpx_z).mean()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparametrize(mean, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        self.ELBO_loss = self._ELBO(x, decoded, mean, logvar)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def getELBO_loss(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparametrize(mean, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        return self._ELBO(x, decoded, mean, logvar)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabc54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e155b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c5652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b4df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3f1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
